:CONFIG:
#+hugo_base_dir: ../
#+seq_todo: TODO DRAFT DONE
#+options: creator:t
#+macro: updatetime {{{time(%B %e\, %Y)}}}
#+hugo_auto_set_lastmod: t
#+startup: hidestars
#+startup: overview
#+startup: logdone
# #+startup: inlineimages
# #+startup: latexpreview
:END:

#+title: Blog Source File
#+author: Prashant Tak

* TODO
1. Create Tufte Theme for hugo that ACTUALLY works based on tufte css and the current tale theme
2. Open external links in new tab by default, unless it's meta-links.
* People I Idolize
+ Robert Endre Tarjan
+ Lars Ingebrigtsen
+ Gennady Korotkevich
+ Srinivasa Ramanujun
+ Etho!
* Doubts
1. What is r?
2. dev-corpus refers to?
3. F1 score? Measure of accuracy
4. xpos?

* Readings
* Resources
* About
CLOSED: [2021-03-21 Mon 01:00]
:PROPERTIES:
:EXPORT_HUGO_SECTION: about
:EXPORT_FILE_NAME: about
:END:

Hi! I'm Prashant.

* Blog
:PROPERTIES:
:EXPORT_HUGO_SECTION: blog
:END:
** Random Notes :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: random-notes
:END:
*** Chirping Bird Wall

#+begin_quote
"What seems to be the officer, problem?" :tf:
#+end_quote

#+begin_quote
Added first chirp and performed first squash while b8kd.
#+end_quote

#+begin_quote
[[/mnt/Data/Downloads/Media/channie/myNFT.jpg]]
#+end_quote

#+begin_quote
step 1: eat and sleep better go outside and get sunlight. stand in the sidewalk outside of your shitty apartment, nobody will give a fuck. do not overcomplicate this, you and your mood are a product of the chemical compounds that you have chosen to fuel your body with. you've had 20-30 yrs to learn what does and doesnt agree with your body, stop worrying about lofty super-athlete shredded body goals, start worrying about eating what makes you feel fucking normal or better today and go from there. chances are you're consuming either way too much or not enough sugar, if you consume corn syrup it's the former.

step 2: lift things until you're absolutely confident facing all physical challenges you can see yourself being faced with. (warning if you're tall you better start with step 3 because you tend to become intimidating and this might be a detriment if you're socially retarded)

step 3: start and finish things. go do it, yeah, that thing you've been putting off. stop being comfortable, start challenging yourself. say hello to her, ask people how their weekend was, tell people about the thing you finally got off your ass and did.

if you can't manage to start any of this shit, either seek professional help or go do a serious dose of mushrooms in a comfortable environment. you need to forcefully light a fire under your ass and force yourself into action, either chemically or through extreme psychedelic self-analysis and introspection. if you've got serious problems it's not going to solve any of it. but it will be made very clear what needs to be cleaned up in your life for you to return to normal. you can't get wasted time back, but you cannot live in shame or guilt for wasting it, you will kill yourself.
#+end_quote

#+begin_quote
Relationships are like a control system, no matter the disturbances or change in affection, the output must remain consistent.
#+end_quote

Source: [[https://signalvnoise.com/posts/3124-give-it-five-minutes][Give it five minutes]]
#+begin_quote
Pushing back means you already think you know. Asking questions means you want to know. Ask more questions.
#+end_quote

Source: [[https://ava.substack.com/p/be-an-asker?s=w][Be an asker]]
#+begin_quote
You penalize yourself: that’s why you don’t ask. But if you’re able to stop punishing yourself for asserting your needs, something very important changes. You become capable of advocating for yourself.

Never get upset at someone for saying no. You don’t “deserve” an answer. The truth is, people who don’t know you don’t owe anything to you, and even people who know you really well are free to prioritize themselves (though you may want to rethink the relationship if they don’t seem to care about you, obviously).

Effective to ask lots rather than ask one person lots of times.

Best cure for rejection sensitivity is to get rejected a lot. Getting a yes eventually helps you learn how to get a yes and helps correct your mistaken self-narratives.

Don't be needy while asking and be fine with hearing a no, make people feel like it's safe to say no to you.

Asking is good, it saves you time and relationships.

Asking makes you better at answering.

Directness is a gateway to emotional honesty.
#+end_quote


*** Words to live by
#+begin_quote
we do these little things I guess to
fill our time and you set little goals
and I guess that's what makes life
exciting
-Nathan fielder
#+end_quote
*** [[https://www.nytimes.com/2015/01/09/style/no-37-big-wedding-or-small.html][36 Questions that lead to love]]
+ Idea is that mutual vulnerability fosters closeness.
**** Set 1
1. Given the choice of anyone in the world, whom would you want as a dinner guest?
2. Would you like to be famous? In what way?
3. Before making a telephone call, do you ever rehearse what you are going to say? Why?
4. What would constitute a “perfect” day for you?
5. When did you last sing to yourself? To someone else?
6. If you were able to live to the age of 90 and retain either the mind or body of a 30-year-old for the last 60 years of your life, which would you want?
7. Do you have a secret hunch about how you will die?
8. Name three things you and your partner appear to have in common.
9. For what in your life do you feel most grateful?
10. If you could change anything about the way you were raised, what would it be?
11. Take four minutes and tell your partner your life story in as much detail as possible.
12. If you could wake up tomorrow having gained any one quality or ability, what would it be?
**** Set 2
13. If a crystal ball could tell you the truth about yourself, your life, the future or anything else, what would you want to know?
14. Is there something that you’ve dreamed of doing for a long time? Why haven’t you done it?
15. What is the greatest accomplishment of your life?
16. What do you value most in a friendship?
17. What is your most treasured memory?
18. What is your most terrible memory?
19. If you knew that in one year you would die suddenly, would you change anything about the way you are now living? Why?
20. What does friendship mean to you?
21. What roles do love and affection play in your life?
22. Alternate sharing something you consider a positive characteristic of your partner. Share a total of five items.
23. How close and warm is your family? Do you feel your childhood was happier than most other people’s?
24. How do you feel about your relationship with your mother?
**** Set 3
25. Make three true “we” statements each. For instance, “We are both in this room feeling ... “
26. Complete this sentence: “I wish I had someone with whom I could share ... “
27. If you were going to become a close friend with your partner, please share what would be important for him or her to know.
28. Tell your partner what you like about them; be very honest this time, saying things that you might not say to someone you’ve just met.
29. Share with your partner an embarrassing moment in your life.
30. When did you last cry in front of another person? By yourself?
31. Tell your partner something that you like about them already.
32. What, if anything, is too serious to be joked about?
33. If you were to die this evening with no opportunity to communicate with anyone, what would you most regret not having told someone? Why haven’t you told them yet?
34. Your house, containing everything you own, catches fire. After saving your loved ones and pets, you have time to safely make a final dash to save any one item. What would it be? Why?
35. Of all the people in your family, whose death would you find most disturbing? Why?
36. Share a personal problem and ask your partner’s advice on how he or she might handle it. Also, ask your partner to reflect back to you how you seem to be feeling about the problem you have chosen.
** Worklog :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: worklog
:END:
*** 17 June 2022
+ [ ] Leave Everything, Learn Python
+ [-] Read Crafting Interpreters
+ [-] IUSACO
+ [ ] Programming Perl
+ [ ] DAS - From Scratch
*** 9 February 2022
+ [X] AnE Lab
+ [X] IIC Assignment
+ [X] DSA Catchup
+ [X] DSA Lab
+ [X] OOP 27 Jan Lecture

*** 3 February 2022
+ [X] Problem Formulation
*** 2 February 2022
+ [X] AnE Lab Submission (10 AM, 3rd Feb)
+ [X] SOP 2 Small Papers
+ [X] PowE Lab
*** 31 January 2022
+ [X] Chapter 2 T2 PowE
+ [X] Might be Test in AnE, so practice problems
+ [X] Prepare for IIC Tut too
** Morphosyntactic Tagging with BiLSTM Model
CLOSED: [2021-03-21 Mon 03:00]
:PROPERTIES:
:EXPORT_FILE_NAME: nnfl-paper
:EXPORT_AUTHOR: Bernd Bohnet, et al
:END:

#+begin_quote
"I had shingles, which is a painful disease."
#+end_quote

[[./assets/machine_learning.png][file:./assets/machine_learning.png]]

This post contains a complete overview of the titled paper and provides a basic outline of related concepts. This paper aims to investigate to what extent having initial sub-word and word context insensitive representations affect performance.

*** Abstract
1. RNN leads to advances in speech tagging accuracy [[https://www.aclweb.org/anthology/K18-2001.pdf][Zeman et al]]
2. Common thing among models, /rich initial word encodings/.
3. Encodings are composed of recurrent character-based representation with learned and pre-trained word embeddings[fn:10].
4. Problem with the encodings, context restriced to a single word hence only via subsequent recurrent layers the word information is processed.
5. The paper deals with models that use RNN with sentence-level context.
6. This provides results via synchronized training with a meta-model that learns to combine their states.
7. Results are provided on part-of-speech and morphological tagging[fn:1] with great performance on a number of languages.
*** Terms
1. Morphosyntactic = Morphology + Syntax and Morphology is study of words, how they are formed, and their relationship to other words in the same language.
2. [[https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577][RNN]]: [[https://arxiv.org/pdf/1211.5063.pdf][On difficulty of training RNNs]]
3. [[http://colah.github.io/posts/2015-08-Understanding-LSTMs/][LSTM]]: Long Short-Term Memory is a type of RNN that addresses the vanishing gradient problem through additional cells, input and output gates.
4. BiLSTM: It is a sequence processing model that consists of two LSTMs. They effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).
*** [[https://www.kdnuggets.com/2018/06/getting-started-natural-language-processing.html][Basics of NLP]] / Pre-requisites
**** Key Terms
1. *NLP*: Natural Language Processing concerns itself with interaction of technology with human languages.
2. *Tokenization*: An early step in the NLP process which splits longer strings of text into smaller pieces, or /tokens/.
3. *Normalization*: A series of tasks meant to put all text on a level playing field i.e. converting it to lowercase, removing punctuation, expanding contractions, converting numbers to their word equivalents, stripping white space, removing stop words and so on.
   + *Stemming*: Process of eliminating affixes (suffixes, prefixes, infixes, circumfixes) from a word to obtain its stem. For example, /running/ becomes /run/.
   + *Lemmatization*: It's related to stemming but is able to capture canonical forms based on the word's lemma (root form). For example, /better/ would turn into /good/.
4. *Corpus*: The latin word for /body/ refers to a collection of texts which may be formed of a single language of texts, or multiple. They are generally used for statistical linguistic analysis and hypothesis testing.
5. *Stop words*: Filter words which contribute little to the overall meaning of text since they are the very common words of the language. For example: /the/, /a/ etc.
6. *Parts-of-speech (POS) Tagging*: It consists of assigning a category tag to the tokenized parts of a sentence such as nouns, verbs, adjectives etc. The category of words is distinguished since they share similar grammatical properties.
7. *Statistical Language Modeling*: It's the process of building a model which takes /words/ as input and assign probabilities to the various sequences that can be formed using them.
8. *Bag of words*: It's a representation model used to simplify the contents of a selection of text by just reducing the words to their frequency.
9. *n-gram*: It focuses on preserving contagious sequences of N items from the text selection.
**** A framework for NLP
1. *Data Collection or Assembly*: Building the corpus
2. *Data Preprocessing*: Perform operations on the collected corpus which consists of tokenization, normalization, substitution (noise removal).
3. *Data Exploration & Visualization*: Includes visualizing word counts and distributions, generating wordclouds, performing distance measures.
4. *Model Building*: Choosing the language models (FSM, MM), classifiers and sequence models (RNNs, LSTMs).
5. *Model Evaluation*
**** Data Representation
1. We need to encode text in a way that can be controlled by us using a statistical classifier.
2. We go from a set of categorical features in text: words, letters, POS tags, word arrangement, order etc to a series of /vectors/.
3. *One-hot Encoding* (Sparse Vectors) :
   + Each word, or token corresponds to a vector element.
   + Result of one-hot encoding is a sparse matrix, that is, for a corpus containing a lot of tokens, representing a small subset of them would lead to a lot of zero vectors which would consume a large amount of memory.
   + One more drawback is that while it contains the information regarding the presence of a certain word, it lacks positional information so making sense of the tokens is not an option. For example, /Kate hates Alex/ is the same as /Alex hates Kate/.
   + Variants of one-hot encoding are /bag-of-words/, /n-gram/ and /TF-IDF/ representations.
4. *Dense Embedding Vectors*:
   + The information of the semantic relationship between tokens can be conveyed using manual or learned POS tagging that determines which tokens in a text perform what type of function. (noun, verb, adverb, etc)
   + This is useful for /named entity recognition/, i.e. our search is restricted to just the nouns.
   + But if one represents /features/[fn:2] as dense vectors i.e. with core features embedded into an embedding space of size /d/ dimensions, we can compress the number of dimensions used to represent a large corpus into a manageable amount.
   + Here, each feature no longer has its own dimension but is rather mapped to a vector.
**** [[http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf][Word Representation]]
**** [[https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79#:~:text=Subword%2Dmodels%3A%20Byte%20Pair%20Encodings%20and%20friends,-2.1%20Byte%20pair&text=Byte%20pair%20encoding%20(BPE)%20is,pairs%20into%20a%20new%20byte.&text=BPE%20is%20a%20word%20segmentation,(Unicode)%20characters%20in%20data.][Subword models]]
1. *Purely Character-level models*: In character-level modes, word embeddings[fn:3] can be composed of character embeddings which have several advantages. /Character-level/ models are needed because:
   + Languages like Chinese don't have /word segmentations/.
   + For languages that do have, they segment in different ways.
   + To handle large, open, informal vocabulary.
   + Character level model can generate embeddings for /unknown/ words.
   + Similar spellings share similar embeddings
2. *Subword-models*: TBD???
*** Morphology
It is a section of grammar whose main objects are *words* of languages, their /significant parts/ and /morphological signs/. Morphology studies:
+ Inflection
+ Derivation
+ POS
+ Grammatical values
**** Grammatical Value

*** Introduction
Morphosyntactic tagging accuracy has improved due to using BiLSTMs to create /sentence-level context sensitive encodings/[fn:4] of words which is done by creating an initial context insensitive word representation[fn:5] having three parts:
1. A dynamically trained word embedding
2. A fixed pre-trained word-embedding, induced from a large corpus
3. A sub-word character model, which is the final state of a RNN model that ingests one character at a time.
In such a model, sub-word character-based representations only interact via subsequent recurrent layers. To elaborate, context insensitive representations would normalize words that shouldn't be, but due to the subsequent BiLSTM layer, this would be overridden. This behaviour differs from traditional linear models.[fn:6]

This paper aims to investigate to what extent having initial subword and word context insensitive representations affect performance. It proposes a hybrid model based on three models- context sensitive initial character and word models and a meta-BiLSTM model which are all trained synchronously.

On testing this system on 2017 CoNLL data sets, largest gains were found for morphologically rich languages, such as in the Slavic family group. It was also benchmarked on English PTB(?) data, where it performed extremely well compared to the previous best system.
*** Related Work
1. An excellent example of an accurate linear model that uses both word and sub-word features.[fn:6] It uses context sensitive n-gram affix features.
2. First Modern NN for tagging which initially used only word embeddings[fn:7], was later extended to include suffix embeddings.[fn:8]
3. TBD TBD
4. This is the jumping point for current architectures for tagging models with RNNs.[fn:5]
5. Then [fn:4] showed that subword/word combination representation leads to state-of-the-art morphosyntactic tagging accuracy.
*** Models
**** Sentence-based Character Model
In this model, a BiLSTM is applied to all characters of a sentence to induce fully context sensitive initial word encodings. It uses sentences split into UTF8 characters as input, the spaces between the tokens are included and each character is mapped to a dynamically learned embedding. A forward LSTM reads the characters from left to right and a backward LSTM reads sentences from right to left.

#+CAPTION: Sentence-based Character Model: The representation for the token /shingles/ is the concatenation of the four shaded boxes.
[[./assets/nnfl1a.png][file:./assets/nnfl1a.png]]

For an /n/-character sentence, for each character embedding \((e_{1}^{char},...,e_{n}^{char})\), a BiLSTM is applied:
\[
f_{c,i}^{0},b_{c,i}^{0} = BiLSTM(r_{0},(e_{1}^{char},...,e_{n}^{char}))_{i}
\]
For multiple layers(/l/) that feed into each other through the concatenation of previous layer encodings, the last layer has both forward \((f_{c,l}^{l},...,f_{c,n}^{l})\) and backward \((b_{c,l}^{l},...,b_{c,n}^{l})\) output vectors for each character.

To create word encodings, relevant subsets of these context sensitive character encodings are combined which can then be used in a model that assigns morphosyntactic tags to each word directly or via subsequent layers. To accomplish this, the model concatenates upto four character output vectors: the {/forward, backward/} output of the {/first, last/} character in the token /T/ = \((F_{1st}(w), F_{last}(w), B_{1st}(w), B_{last}(w))\) which are represented by the four shaded box in /Fig. 1/.

Thus, the proposed model concatenates all four of these and passes it as input to an multilayer perceptron (MLP):
\[
g_{i} = concat(T)
\]
\[
m_{i}^{chars} = MLP(g_{i})
\]
A tag can then be predicted with a /linear classifier/ that takes as input \(m_{i}^{chars}\), applies a /softmax/ function and chooses for each word the tag with highest probability.
**** Word-based Character Model
To investigate whether a sentence sensitive character model (/Fig.1/) is better than a model where the context is restricted to the characters of a word, (/Fig.2/) which uses the final state of a unidirectional LSTM, combined with the attention mechanism of (ADD REF: cao rei) over all characters.

#+CAPTION: Word-based Character Model: The token is represented by concatenation of attention over the lightly shaded boxes with the final cell (dark box).
[[./assets/nnfl1b.png][file:./assets/nnfl1b.png]]

#+CAPTION: BiLSTM variant of Character-level word representation
[[./assets/nnfl1.png][file:./assets/nnfl1.png]]

**** Sentence-based Word Model
The inputs are the words of the sentence and for each of the words, we use pre-trained word embeddings \((p_{1}^{word},...,p_{n}^{word})\) summed with a dynamically learned word embedding for each word in the corpus \((e_{1}^{word},...,e_{n}^{word})\):
\[
in_{i}^{word} = e_{i}^{word}+p_{i}^{word}
\]
The summed embeddings \(in_{i}\) are passed as input to one or more BiLSTM layers whose output \(f_{w,i}^{l}, b_{w,i}^{l}\) is concatenated and used as the final encoding, which is then passed to an MLP:
\[
o_{i}^{word} = concat(f_{w,i}^{l}, b_{w,i}^{l})
\]
\[
m_{i}^{word} = MLP(o_{i}^{word})
\]
The output of this BiLSTM is essentially the Word-based Character Model before tag prediction, with the exception that the word-based character encodings are excluded.

#+CAPTION: Tagging Architecture of Word-based Character Model and Sentence-based Word Model
[[./assets/nnfl2a.png][file:./assets/nnfl2a.png]]

**** Meta-BiLSTM: Model Combination
If each of the character or word-based encodings are trained with their own loss and are combined using an additional meta-BiLSTM model, optimal performance is obtained. The meta-biLSTM model concatenates the output of context sensitive character and word-based encoding for each word and puts this through another BiLSTM to create an /additional/ combined context sensitive encoding. This is followed by a final MLP whose output is passed to a linear layer for tag prediction.
\[
cw_{i} = concat(m_{i}^{char}, m_{i}^{word})
\]
\[
f_{m,i}^{l}, b_{m,i}^{l} = BiLSTM(r_{0},(cw_{0},...,cw_{n}))_{i}
\]
\[
m_{i}^{comb} = MLP(concat(f_{m,i}^{l}, b_{m,i}^{l}))
\]

#+CAPTION: Tagging Architecture of Meta-BiLSTM. Data flows along the arrows and the optimizers minimize the loss of the classifiers independently and backpropogate along the bold arrows.
[[./assets/nnfl2b.png][file:./assets/nnfl2b.png]]
**** Training Schema
Loss of each model is minimized independently by separate optimizers with their own hyperparameters which makes this a multi-task learning model and hence a schedule must be defined in which individual models are updated. In the proposed algorithm, during each epoch, each of the models are updated in sequence using the entire training data.

[[./assets/nnflAlg.png][file:./assets/nnflAlg.png]]

In terms of model selection, after each epoch, the algorithm evaluates the tagging accuracy of the development set and keeps the parameters of the best model. Accuracy is measured using the meta-BiLSTM tagging layer, which requires a forward pass through all three models. Only the meta-BiLSTM layer is used for model selection and test-time prediction.

The training is synchronous as the meta-BiLSTM model is trained in tandem with the two encoding models, and not after they have converged. When the meta-BiLSTM was allowed to back-propagate through the whole network, performance degraded regardless of the number of loss functions used. Each language could in theory used separate hyperparameters but identical settings for each language works well for large corpora.
*** Experiments and Results
**** Experimental Setup
The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training. The dropout[fn:9] used on the embeddings is achieved by a single dropout mask and dropout is used on the input and the states of the LSTM.

#+NAME: Architecture
| Model | Parameter                     | Value |
|-------+-------------------------------+-------|
| C,W   | BiLSTM Layers                 |     3 |
| M     | BiLSTM Layers                 |     1 |
| CWM   | BiLSTM size                   |   400 |
| CWM   | Dropout LSTM                  |  0.33 |
| CWM   | Dropout MLP                   |  0.33 |
| W     | Dropout Embeddings            |  0.33 |
| C     | Dropout Embedding             |   0.5 |
| CWM   | Nonlinear Activation Fn (MLP) |   ELU |

TODO Add two remaining tables
**** Data Sets
**** POS Tagging Results
**** POS Tagging on WSJ
**** Morphological Tagging Results
*** Ablation Study (Takeaways)
+ *Impact of the training schema*: Separate optimization better than Joint optimization
+ *Impact of the Sentence-based Character Model*: Higher accuracy than word-based character context
+ *Impact of the Meta-BiLSTM Model Combination*: Combined model has significantly higher accuracy than individual models
+ *Concatenation Strategies for the Context-Sensitive Character Encodings*: Model bases a token encoding on both forward and backward character representations of both first and last character in token. (/Fig. 1/) ....
+ *Sensitivity to Hyperparameter Search*: With larger network sizes, capacity of the network increases, but it becomes prone to overfitting. Future variants of this model might benefit from higer regularization.
+ *Discussion*: TODO Proposed modifications
*** Conclusions
*** Readings and Resources
1. Pytorch: [[https://pytorch.org/tutorials/beginner/nn_tutorial.html][Beginner Guide]], [[https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG][Detailed Guides]], [[https://www.cs.toronto.edu//~lczhang/360/][Notebook form]]
2. Math: [[https://explained.ai/matrix-calculus/index.html][Matrix Calculus]], [[https://mml-book.com/][Book]]
3. Basics:
   + [[https://www.kaggle.com/learn/python][Python]]
   + [[https://realpython.com/jupyter-notebook-introduction/#getting-up-and-running-with-jupyter-notebook][Jupyter]]
   + [[http://cs231n.github.io/python-numpy-tutorial/#numpy][Numpy]], [[https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb][Numpy 2]]
   + [[https://mlcourse.ai/articles/topic1-exploratory-data-analysis-with-pandas/][Pandas]], [[https://www.kaggle.com/learn/pandas][Pandas 2]]
   + [[https://mlcourse.ai/articles/topic2-visual-data-analysis-in-python/][Matplotlib]], [[https://matplotlib.org/matplotblog/posts/an-inquiry-into-matplotlib-figures/][Matplotlib 2]]
   + [[https://mlcourse.ai/articles/topic2-part2-seaborn-plotly/][Seaborn]]
   + [[http://scipy-lectures.org/][Overview]]
4. Interactive Tutorials on [[https://www.deeplearning.ai/ai-notes/initialization/][Weight Initialization]], [[https://www.deeplearning.ai/ai-notes/optimization/][Different Optimizers]]
5. Rougier's Bits
   + [[https://github.com/rougier/matplotlib-tutorial][Matplotlib Tutorial]], [[https://github.com/matplotlib/cheatsheets][Matplotlib Cheatsheets]]
   + [[https://github.com/rougier/numpy-tutorial][Numpy Tutorial]], [[https://www.labri.fr/perso/nrougier/from-python-to-numpy/][From Python to Numpy]], [[https://github.com/rougier/numpy-100][100 Numpy Exercises]]
   + [[https://www.labri.fr/perso/nrougier/python-opengl/][Python & OpenGL for Scientific Visualization]], [[https://github.com/rougier/scientific-visualization-book][Scientific Visualization]]
6. NLP: [[https://github.com/microsoft/nlp-recipes][Best Practices]], [[https://nlpoverview.com/][DL Techniques for NLP]]
7. BiLSTM: [[https://arxiv.org/pdf/1807.00818v1.pdf][Improving POS tagging]]
8. [[https://github.com/google/meta_tagger][Implementation]] of the paper
*** Specific to Paper
1. [[https://universaldependencies.org/guidelines.html][Universal Dependencies]]
2. [[https://lena-voita.github.io/nlp_course.html][Great Tutorial for NLP]]
3. [[https://github.com/Sdernal/Morphology/blob/master/README.md][Morphology]]
*** Footnotes
[fn:1] Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically. As compared to Part-of-speech tagging, morphological tagging also considers morphological features, such as case, gender or the tense of verbs.
[fn:2] They are the different categorical characteristic of the given data. For example, it could be /grammatical/ classes or some /physical/ features. It is context and result dependent. Then for each token, a weight is assigned to it with respect to each feature.
[fn:3] A word embedding is a learned representation for text where words that have the same meaning have a similar representation.
[fn:4] [[https://www.aclweb.org/anthology/K17-3002.pdf][Graph based Neural Dependency Parser]]
[fn:5] [[https://arxiv.org/pdf/1604.05529.pdf][POS Tagging with BiLSTM]]
[fn:6] [[http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=40AFFD632AC50016FE3B435B5C3FD50F?doi=10.1.1.4.7273&rep=rep1&type=pdf][*Fast POS Tagging: SVM Approach]]
[fn:7] [[http://machinelearning.org/archive/icml2008/papers/391.pdf][Unified architecture for NLP]]
[fn:8] [[https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf][NLP(almost) from Scratch]]
[fn:9] Dropping out units (hidden and visible) in a neural network, helps prevent the network from overfitting.
[fn:10] [[https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe][Everything about Embeddings]] Embedding converts symbolic representations into meaningful
** Another nix post in the wall
CLOSED: [2022-06-02 Thu]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-intro
:END:
*** Starting
Are you using Nixos? This is not for you.
Do you want multi-user installation? This is not for you.
This is only useful if you want to use both flakes and home-manager.

+ Single-user installation (no sudo needed, easier to remove, good for testing purposes)
  #+begin_src sh
  sh <(curl -L https://nixos.org/nix/install) --no-daemon
  #+end_src
+ Source the new profile or login.
    =. ~/.nix-profile/etc/profile.d/nix.sh=
+ Since most of the nix "guides" are outdated, check what your current version supports =nix --help=, +at the time of writing this, there's no need to enable experimental features for flakes :)+ spoke too soon.
+ Upgrading nix:
  #+begin_src sh
  nix-channel --update; nix-env -iA nixpkgs.nix nixpkgs.cacert
  #+end_src
+ Check if =nixpkgs-unstable= channel (package sources basically) is installed or not by =nix-channel --list=
+ You can look at user-installed packages by =nix-env --query=
*** Home manager
Allows declarative configuration of user-specific (non global) packages and dotfiles.
**** Installation
  #+begin_src sh
  nix-channel --add https://github.com/nix-community/home-manager/archive/master.tar.gz home-manager
  nix-channel --update
  #+end_src

    In =.bash_profile=. (might not be needed for single-user systems, check back later)
  #+begin_src sh
  export NIX_PATH=${NIX_PATH:+:$NIX_PATH}$HOME/.nix-defexpr/channels:/nix/var/nix/profiles/per-user/root/channels
  #+end_src

    In your =.bash_profile=, add below and source the profile for your current session.
  #+begin_src sh
  source "$HOME/.nix-profile/etc/profile.d/hm-session-vars.sh"
  #+end_src
**** Configuration
Now check =~/.config/nixpkgs/home.nix=, if it exists then for the most part you've successfully installed (not sure about configuration) home-manager. Building a configuration produces a directory in the Nix store that contains all files and programs that should be available in your home directory and Nix user profile, respectively. Run =home-manager build= to successfully verify. Also periodically check =home-manager news= for updates regarding new changes to packages that are referred in your config. After [[https://nix-community.github.io/home-manager/index.html#sec-install-standalone][adding some packages]] (section 2.1), run =home-manger switch=.
*** Fleyks (/sorry/)
Flakes allow us to define inputs (you can think of them as dependencies) and outputs of packages in a declarative way and allow for dependency pinning using locks. As of writing this (June 2022) flakes are still experimental, so they must be enabled explicitly.

  #+begin_src sh
  nix-env -iA nixpkgs.nixFlakes
  #+end_src

This replaces nix 2.9.0 with 2.8.1? Look into why that's happening. For now we enable experimental features.

  #+begin_src sh
  mkdir -p ~/.config/nix
  echo 'experimental-features = nix-command flakes' >> ~/.config/nix/nix.conf
  #+end_src

The documentation is so stinky for flakes, like there are /n/ variants sayings $n^2$ different things, so for now I'm just winging it. Comment out the stateVersion from =home.nix= and in the same directory create a =flake.nix=. Replace jdoe with your username. Also the stateVersion can be changed accordingly to upgrade your =home-manager=.

  #+begin_src nix
  {
    description = "Home Manager configuration of Jane Doe";

    inputs = {
      # Specify the source of Home Manager and Nixpkgs
      home-manager.url = "github:nix-community/home-manager";
      nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
      home-manager.inputs.nixpkgs.follows = "nixpkgs";
    };

    outputs = { home-manager, ... }:
      let
        system = "x86_64-linux";
        username = "jdoe";
      in {
        homeConfigurations.${username} = home-manager.lib.homeManagerConfiguration {
          # Specify the path to your home configuration here
          configuration = import ./home.nix;

          inherit system username;
          homeDirectory = "/home/${username}";
          # Update the state version as needed.
          # See the changelog here:
          # https://nix-community.github.io/home-manager/release-notes.html#sec-release-21.05
          stateVersion = "22.05"; # TODO add current unstable home-manager version

          # Optionally use extraSpecialArgs
          # to pass through arguments to home.nix
        };
      };
  }
  #+end_src

Now it's time to flake-ify your =hm=. Here <flake-uri> would be =path:.config/nixpkgs= assuming your pwd is =~=.

  #+begin_src sh
  home-manager switch --flake '<flake-uri>#jdoe'
  #+end_src

The flake inputs are not upgraded automatically when switching. The analogy to the command =home-manager --update= ... is =nix flake update=. If updating more than one input is undesirable, the command =nix flake lock --update-input <input-name>= can be used.

** Old Headings
*** Why?
*** Benefits
*** Home-manager
*** Flakes
*** Overlays
*** Profiles
*** Multi User
*** Nix Shell
** Japanese Resources :noexport:
   :PROPERTIES:
   :EXPORT_FILE_NAME: japanese-guide
   :END:
*** JPod101
    Add a nice collection of their videos and cheatsheet materials since most sources don't really tell much about their actual content.
*** Hiragana
    + Why? All other basic textbooks have as requirement, romaji as a crutch that hurts later on
    + Tofugu Guide
    + Tae Kim Quiz
*** Katakana
    + Important thing to not delay learning it even when other sources say so.
    + TODO Add sources and more info
*** Kanji + Grammar + Vocabulary
**** Intro
     + Important step because after crossing the initial simple hurdle you're presented with a mammoth choice.
     + Genki - Mention ToKini
     + Grammar - Use genki before Tae Kim??
     + Kanji - Can I promote WaniKani Anki Deck???
     + Vocabulary
     + Readings - Tofugu article
**** Tango N5 deck
     | 彼   | Kare   | He   |
     | 彼女 | Kanojo | She  |
     | 名前 | Namae  | Name |
     |      |        |      |

*** Grammar Points
**** Verb Conjugation
     Start with misa's video then watch Andy's.
**** Particles (Do after conjugation!)
     + =は= (pronounced as wa): TOPIC MARKER- It marks the topic of a clause and creates focus. Translation: /as for, speaking of/. It can also be use as a CONTRASTING MARKER where =demo= acts as /but/ For example, I eat bread, *but* I *don't* eat butter. Here the topics bread and butter would be followed by the topic marker. =kore= means /this/ can can be just as a pointer for distinguishing an object
     + =ga=: SUBJECT PARTICLE- Since it denotes existence, it is generally used with =arimasu= and =imasu= (for animate things) meaning (/to have/ or /to exist/) There existence copulas can be negated by turning =su= to =sen=. It can be used as a DESIRE MARKER: `S + O + =ga= + Adjective of desire` for example =hoshii desu= which is /(I) want/ or `Stem of verb + =tai desu= ` which is /[I] want to [verb]/.
*** Pitch Accent
- Playlist
- My notes
- kotu.io

*** Bunpro
**** N5
***** Level 1
      1. da:
         + to be/is
         + Do not use with i-adj. (Cure Dolly video)
         + Noun+da
         + na-adj+da
         + Casual
      2. desu:
         + to be/is
         + Can be used with both na/i-adj
         + Noun/Adj + desu
         + Polite version of da
      3. wa(ha):
         + denotes the .... (use notes from text)
         + Sentence Topic + wa
         + Emphasizes what comes after it unlike ga.
         + ....
      4. mo:
         + also, too as well
         + Noun + mo
         + atashi mo. (me too)
         + Kore mo sensei desu. (He is also a teacher)
      5. kore:
         + this
         + kore wa penn desu.
         + kore mo hitsyoo desu. (This is also necessary)
      6. no:
         + indicates possession
         + Noun 1 + no + Noun 2
         + これも私のペンです。 (This is also my pen)
      7. ii:
         + adjective meaning "good"
         |          | Present | Past        |
         | Postive  | ii      | yokatta     |
         | Negative | yokunai | yokunakatta |
         + yoku also means "frequently/often" be careful
         + テストは、よくなかった。(Test was not good)
      8. ka:
         + question particle
         + Phrase + ka
         + 明日もいいですか。(ashita) (Is tomorrow also good?)
***** Level 2
      1. deshoo:
         + right? probably
         + Asking for confirmation
         + Noun/Verb/Adj + deshyoo
         + 明日も雨でしょう。 (ashita, ame) (It will also probably rain tomorrow)
      2. ga:
         + Subject marker identifier
         + Subject + ga
         + Denotes who or what performed the action, emphasizes what came before it
      3. sore:
         + that
         + それもいいです。(That is also good.)
      4. ru-verbs/Ichidan verbs:
         | Conjugation | Casual      | Polite           |
         | Present     | taberu      | tabemasu         |
         | Past        | tabeta      | tabmashita       |
         | Negative    | tabenai     | tabemasen        |
         | Neg. Past   | tabenakatta | tabemasendeshita |
      5. gaaru:
         + To be/ There is
         + Polite form: gaarimasu
         + Noun + gaaru
         + For non-living things
         + Ga is often omitted in casual speech
      6. koko:
         + here/ this place
         + (Near the speaker)
      7. to:
         + and/ with
         + Noun + to + Noun/Verb
         + When listing nouns, to: exhaustive, ya: non-exhaustive
      8. ~ ndesu.nodesu
         +

*** Anki Deck Sequence:
1. Hiragana Deck
2. Katakana Deck
3. Katakana Sentences
4. Kanji Deck: Either RTK order Allinone or WaniKani? or Kanji deck by [[https://sites.google.com/view/jo-mako/home][Jo Mako]]
5. Grammar Deck: Jo Mako
6. Vocabulary: Tango decks + JP1K + Kanji in context
** Creating a blog using ox-hugo
CLOSED: [2021-03-21 Mon 02:00]
:PROPERTIES:
:EXPORT_FILE_NAME: blog-creation
:END:

I was going to make a post explaining how I made this blog but it was rendered pretty useless by [[https://dev.to/usamasubhani/setup-a-blog-with-hugo-and-github-pages-562n][this.]] So yeah, I might archive this later.

1. Install hugo from your package manager.
2. Create a new site:
   #+begin_src sh
hugo new site blog
   #+end_src
3. Add a theme:
   #+begin_src sh
cd blog
git init
git submodule add <theme_url> themes/<name>
   #+end_src
4. Install ox-hugo in emacs
   #+begin_src emacs-lisp
;; goes in packages.el
(package! ox-hugo)

;; goes in config.el
(use-package ox-hugo
  :after ox)
   #+end_src
5. TODO Explain the process of content and properties, tags etc.
6. Export
7. Config.toml (theme, title, url, publishdir, etc)
8. Run server, check localhost.
9. Push
10. Go to GitHub repository Settings > GitHub pages. Select /docs in Source.
11. Voila!
** Updates & A letter to self :noexport:
:PROPERTIES:
:export_file_name: january-update
:END:
Hi there, it's been a while.
I doubt anyone's reading these but I felt like writing something regardless.

So I've started going ham into kanji study again, (hopefully this time it's more fruitful) and I think that now I'm getting the appeal of immersion and how beneficial it can be paired with the right amount of active recognition of what you're recalling from your anki reps.

Coming to the S.M.A.R.T. (Specific, Measurable, Attainable, Realistic, and Time Bound) goals regarding japanese, let's try to set some shall we? So that we have something to look forard to and back on.

+ In the month of february, you should ideally finish JLPT N5 vocabulary and kanji, also for kanji, it should be the RRTK450 deck.
+ Alongwith that, you should start repping JP1K if it's manageable.
+ For the final and optional goal, you should try to go through atleast the entire beginner's section of TaeKim.

Moving onto academic stuff, I messed up my sleeping schedule again and due to which I missed an easy test. This had been a problem that has persistently plagued me in the past and has done a lot of harm to my grades. So we should set some lifestyle goals.

+ Sleep around 11 PM, no more late night twitch streams or binging TV shows. You can do the same in between classes or watch highlights later. If you don't set your priorities now, you'll definitely regret it later.
+ One thing that I was very skeptical of in the past was taking baths daily, (even now I get lazy all the time), but what I've realised is that it really helps reset your mind and freshens up your body in inexplicable ways. So you should really try to follow through with this.
+ The major time consuming activites of your day are watching random stuff on streaming websites, to turn that into a positive you should really devote that time into immersion, that'll count towards your language learning hours and leisure time simultaneously.
+ The other major time sink is constantly checking the three social media platforms that you engage with, namely Twitter, Instagram and Discord. Now you shouldn't go all out and stop using these services altogether, you've tried that in the and have failed. A better alternative would be to set time limits/period each day during which you'll browse these networks. For now, a good starting point would be 30 mins each day after 9PM.

Now comes the real stuff, the reason why this post is needed and whenever you feel that you're straying from the goals mentioned here, what you should look back to.

There's an extremely urgent requirement for you to succeed in your college life which most of the shows that you watch fail to depict, that is having good grades. For as long as you've been in college, you've tried to portray yourself as this person that doesn't care about grades and in the process hampered your academic capabilites a lot. This type of attitude would've been fine if you were actually doing well but with that clearly not being the case, it's high time that you really change your attitude. The goals for this semester would be having consistent reviews of daily classes, just look at the situation as doing anki reps, if you miss a single day, it piles up so much that recovering is hard and the burden increases exponentially. Since the semester has just started you can easily make up for the lost time.

+ [A] Daily revisions of the day's classes
+ [B] Preparing for next day's classes
+ [C] Attempt textbook questions

Now it's time for miscellaneous things that you should really focus on in order to really up your skillset and builld up your credentials.
+ Finish SOP work by tuesday, you have that meeting on wednesday which /really, really/ needs to go well. Oh and one more thing that I would've completely missed, finish Power Electronics lab work.
+ Start going through the ITMO lectures, your goal should be to be done with them by the time February comes to an end.
+ One thing which you've been really lackadaisical about is having an internship which would reflect your work ethic and since you didn't really have the grades to sit for them during last season, you thought you'd /"crack"/ GSoC and add that as work experience, alas your sloth-like mentality has put you in a very difficult position when it comes to that too, since you've practically made no effort to keep in touch with the org or contribute towards their project while others have made huge strides, you need to play catch-up now.
+ Start going through their code base, look at issues, merged PRs and previous year successful contributions, try to model them and constantly stay in touch with the org members so that they're aware of your existence. Build up some kind of rapport with them so that they feel like you're in it for the long run.
+ Optional goals for this category would be to work a bit on corfu and mpc/music modules for doom.

This is all I could think of for now, and since your OOP lab's starting in any minute, so it's time to pen down, call it a day and follow through on what you've put out for the whole world to see.

You're responsible for your own success.

Go break a leg! :)
* Notes
:PROPERTIES:
:EXPORT_HUGO_SECTION: notes
:END:
** Elements of computing systems
CLOSED: [2022-06-10 Fri]
:PROPERTIES:
:export_file_name: elements-of-computing-systems
:END:
# vhdl source blocks for HDL
*** Introduction
These are the notes which I took while studying computer architecture from the textbook "/The Elements of Computing Systems/ by /Noam Nisan & Shimon Schocken/ ". They're very terse and have very opinionated content from the textbook so they should only be used in tandem with it for revision purposes.
[[./assets/ecs-1.png][file:./assets/ecs-1.png]]
The text program is parsed, its semantics are uncovered, it's represented in some low-level language that the computer can understand. This process is called /compilation/. Its result is another text file, containing machine-level code.

To make this abstract machine code concrete, it has to be realized by some /hardware architecture/ which is implemented by a /chipset/ - registers, memory, ALU, etc. These devices are made of logic gates which consist of /switching devices/ that are implemented by transistors.

/Church-Turing conjecture/: At their core, all computers are essentially equivalent.

[[./assets/ecs02.png][file:./assets/ecs02.png]]

*** Boolean Logic
[[./assets/ecs-03.png][file:./assets/ecs-03.png]]

Any boolean function can be realised by just the /nand/ gate. Let that sink in. A gate is a physical device that implements a simple boolean function. They are implemented as transistors etched in silicon, packaged as chips. The boolean function chip is designed and tested by using a /Hardware Description Language/ (HDL). This simulated realisation is then tested for corectness and other parameters such as speed of computation, energy consumption and cost are quantified. To illustrate the same proces, HDL implementation of a XOR function is shown below.

[[./assets/ecs-04.png][file:./assets/ecs-04.png]]

Using built-in libarary chips is similar to writing a regular program except the PARTS section is replaced with BUILTIN Xor. Some things to note are that internal pins are created automatically when they appear in an HDL program and that pins may have an unlimited fan-out. In HDL programs, the existence of forks is inferred from the code.

Chips are specified using the API style, for nand gate: \\
=Chip name: Nand= \\
=Input: a,b= \\
=Output: out= \\
=Function: if ((a==1) and (b==1)) then out = 0, else out = 1= \\

*Multiplexer*: Has two input(data) bits /a,b/ and one selection bit /sel/ which decides which input bit would be the output.

[[./assets/ecs-05.png][file:./assets/ecs-05.png]]

*Demultiplexer*: Takes in a single input and routes it to one of the possible outputs depeding on the selector bit.

[[./assets/ecs-06.png][file:./assets/ecs-06.png]]

HDL programs treat multi-bit values like single-bit values but they are indexed(from right to left, rightmost being 0$^{th}$ bit) to access individual bits. For example, a /m/-way /n/-bit mux would select one of its /m n/-bit inputs and output it to its /n/-bit output, where there would be /$k=log_{2}m$/ selection bits. A 4-way 16-bit mux API would look like: \\
=Chip name: Mux4Way16= \\
=Input: a[16],b[16],c[16],d[16],sel[2]= \\
=Output: out[16]= \\
=Function: if(sel==00,01,10, or 11) then out = a,b,c, or d= \\
=Comment: The assignment is a 16-bit operation. For example, "out = a" means "for i = 0..15 out[i] = a[i]"= \\

*** Boolean Arithmetic
/Word size/ is a term used for specifying the number of bits that computers use for representing a basic chunk of values. For example, integer values are stored in 8-, 16-, 32- or 64-bit registers. Fixed word size implies the existence of a limit on number of values that the registers can represent. In general, using /n/ bits, one can represent values from 0 to 2$^{n}$ - 1.
**** Addition
[[./assets/ecs-07.png][file:./assets/ecs-07.png]]

If the most significant bitwise addition generates a carry of 1, /overflow/ occurs, if that is ignored, one achieves /n/ correct bits for adding two /n/-bit numbers.
**** Signed Numbers
The coding scheme was chosen so that hardware implementation of arithmetic operations would be as simple as possible, its result being /two's complement/ binary code that represents negative x as binary code that represents 2$^{n}$ - x. For example, in a 4-bit binary system =-5= would be represented as binary of =(16-5)= or =1011=. This leads us to the following properties:
+ For an n-bit system, 2$^{n}$ signed numbers are generated, from -(2$^{n-1}$) to 2$^{n-1}$ - 1. 
+ Code for positive numbers begins with 0 while for negatives with 1.
+ To get code of -x from x, flip all bits of x and add 1.
**** Adders
*Half adder*: Adds two numbers/bits and outputs in for of =sum= and =carry= which are LSB and MSB of the addition of the input bits.

*Full adder*: Adds three bits and outputs the result in form of =sum= and =carry= bits.

*Adder*: Adds two /n/-bit numbers and outputs as =out=, which is also /n/-bit. Here the overflow bit is ignored.

*Incrementer*: Adds 1 to a given number, enables fetching next instruction from memory after executing the current.

Note that this adder implementation is inefficient because of delays incurred by propogation of carry bits which can be acclerated by /carry lookahead/ heuristics.
**** ALU
Designed to compute a set of arithmetic and logic operations, exactly /which/ ones is a design decision derived from cost-effectiveness considerations. Shown below is the Hack ALU which computes 18 functions which are controlled by six 1-bit /control bits/. In addition to those there are =zr= and =ng= bits which flag whether the output is zero or negative. Note that the output overflow bit is ignored.

[[./assets/ecs-08.png][file:./assets/ecs-08.png]]

[[./assets/ecs-09.png][file:./assets/ecs-09.png]]

To illustrate the ALU logic, if one wishes to compute =x-1 for x=27=, then control bits would be =001110=. These six directives are to be performed in order: first, we either set the x and y inputs to 0, or not; next, we either negate the resulting values, or not; then we compute either =+= or =&= on the preprocessed values; and finally, we either negate the resulting value, or not. The first two =00= show that we neither zero nor negate x, then =11= shows that we zero y and then negate it, leading to 16-bit value =1111111111111111= which is -1 in two's complement. The next control bit =1= dictates addition operation and the last =0= bit shows that output shouldn't be negated hence we get the desired function =x-1=.
*** Memory
All the chips considered so far are time independent, they're called /combinational chips/. Now /sequential chips/ are introduced whose output depends not only on current time inputs but past inputs and outputs too. The notion of time is introduced by a /clock/ that generates a /cycle/ of binary singals that are called a /tick/ and a /tock/.

By viewing the time in the form of cycles instead of a continuous progression, the changes in the system are made atomic i.e. they'll only be reflected during cycle transitions and within a cycle, no change would be shown. This discrete view helps in neutralizing randomness associated with time delays and can be used for synchronizing the operations of different chips. For that, the cycle should be long enough to contain the possible time delay but short enough so that the resulting system's fast because the changes would only be shown during transitions.

Also, to ensure output validity duration of the clock cycle has to be slightly longer than the time it takes a bit to travel the longest distance from one chip to another, plus the time it takes to complete the most time-consuming within-chip calculation.

[[./assets/ecs-11.png][file:./assets/ecs-11.png]]

**** Flip Flops
A Data flip-flip (DFF) is a memory chip which has a single-bit data input and output and an addition clock input(marked by small triangle on gate icon) that feeds from master clock's signal. It models the behavior =out(t)=in(t-1)= that is, in every subsequent time unit, DFF outputs input from previous time unit. At all other times, DFFs are /latched/, meaning that changes in their inputs have no immediate effect on the output.

[[./assets/ecs-10.png][file:./assets/ecs-10.png]]

Note that the concept of feedback loops doesn't make sense in combinational chips, but there is no difficulty in feeding outputs back into inputs if they're passed through a DFF gate so that the output at time t does not depend on itself but rather on the output at time t-1.

FFs are generally constructed from nand gates connected in feedback loops. The standard construction begins by building a non-clocked flip-flop which is bi-stable, that is, can be set to be in one of two states (storing 0, and storing 1). Then a clocked flip-flop is obtained by cascading two such non-clocked flip-flops, the first
being set when the clock ticks and the second when the clock tocks.
**** Registers
A register is a DFF with a /load/ input. It stores and outputs its current value until /load/ instructs it to update to a new value.

[[./assets/ecs-14.png][file:./assets/ecs-14.png]]

[[./assets/ecs-12.png][file:./assets/ecs-12.png]]

A 16-bit register behaves the same as a /bit/ chip, just that it deals with 16-bit values. A register fulfils the classical function of a memory device, remember and emit the last written value, until its /set/ to another.
**** RAM
A RAM chip consists of /n/ 16-bit register chips which can be selected and manipulated separately. By specifying a particular address (0 to n-1), a particular register can be selected for read/write operations. Note that the access time of any randomly selected register is instantaneous.

[[./assets/ecs-13.png][file:./assets/ecs-13.png]]

RAM implementations can be modular, for example one could implement RAM64 i.e. having 64 registers by using eight RAM8 chips, to select a particular register one uses a 6-bit address /xxxyyy/ where /xxx/ bits can be used to select one of the RAM8 chips and /yyy/ bits can be used to select the register within the RAM8 chip.
**** Counter
A chip that's implementation of an incrementer used to keep track of program instructions (PC: Program Counter). It has three functional inputs alongwith =in=, namely =inc=, which increases counter state every cycle(PC++) and =reset= which sets it to 0 and =load= sets its value to whatever values is fed by =in=.

[[./assets/ecs-15.png][file:./assets/ecs-15.png]]

[[./assets/ecs-16.png][file:./assets/ecs-16.png]]

*** Machine Language
#+begin_quote
"Form follows function."
#+end_quote
A machine language can be viewed as an agreed-upon formalism designed to manipulate a /memory/ using a /processor/ and a set of /registers/. Unlike high-level languages, whose design goals are cross-platform compatibility and power of expression, machine languages are designed to effect direct execution in a specific hardware platform and its control. For this project, Hack machine language is used.

Machine language programs can be written in either /binary/ or /symbolic/ instructions. Programs writtern in symbolic(/assembly/) languages are translated into executable binary code by a translator program called /assembler/.
**** Hardware
*Memory*: Functionally speaking, a memory is a continuous sequence of cells, also referred to as locations or memory registers, each having a unique address.

*Processor*: A device capable of performing a fixed set of primitve operations which include arithmetic, logical, memory access and control/branching operations.

*Registers*: Inside the processor's chip, they serve as high-speed local memory, allowing it to manipulate data and instructions without having to be dependent on slow memory chips. They're categorized into /data/ and /address/ registers.
**** Instructions
*Arithmetic and logical operations*: Assuming that the processor has a set of registers denoted =R0, R1, R2,...=. Basic operations like addition, subtraction, and(bitwise), or, not. \\
=load R1,true= \\
=load R2,false= \\
=and R1,R2,R2= (R1 \leftarrow R1 And R2) \\

*Memory access*: Using address register =A=, let us set memory location 17 to value 1. \\
=load A,17= \\
=load M,1= \\
Here by convention, M stands for memory register selected by A.

*Flow control*: For facilitating branching actions, labelled /goto/ statements exist.

[[./assets/ecs-17.png][file:./assets/ecs-17.png]]

*Symbols*: Make it easier to not rely on rigid physical addresses which makes the code /relocatable/.
**** Hack
*Memory*: Hack uses two distinct memory units, /data/ and /instruction/ memory. Both
memories are 16-bit wide, and each has a 15-bit address space, hence maximum addressable size of each memory unit is 2$^{15}$ or 32K 16-bit words.

[[./assets/ecs-18.png][file:./assets/ecs-18.png]]

Data memory's =address= input always contains some value so there's always one selected register called M. Instruction memory's =address= input also always contains some value that is, there's always one selected instruction memory register whose value is referred to as /current instruction/.

*Registers*: Hack Instructions work on three 16-bit registers, /data/ (D: stores value), /address/ (A: both address and data) and selected /data memory/ register M. To store a constant in A register, instruction is =@17=, to set D register to a value, two instructions are needed: =@17= then =D=A=.

*Addressing*: The =@xxx= instruction does two things, first it makes the RAM register whose address is =xxx= the selected memory register(M) and then it makes the value of the ROM register whose address is =xxx= the selected instruction. For example, to set value of =RAM[100]= to =17=, instructions are =@17, D=A, @100, M=D=. Initially A acts as a data register then as an address register. To set value of RAM[100] to value of =RAM[200]=, do =@200, D=M, @100, M-D=.

*Branching*:
** IUSACO
CLOSED: [2022-06-05 Sun]
:PROPERTIES:
:EXPORT_FILE_NAME: iusaco-notes
:END:
*** Input and Output
  #+begin_src cpp
  #include <cstdio>
  using namespace std;
  int main() {
      freopen("template.in", "r", stdin);
      freopen("template.out", "w", stdout);
  }
  #+end_src
+ When using C++, arrays should be declared globally, or initialized to zeros if declared locally to avoid garbage values.
+ 32bit int: $\pm 2\times10^{9}$ v/s 64bit int: $\pm 9\times 10^{18}$
*** Complexity and algorithm analysis
+ Elementary mathematical calculation: O(1)
+ Unordered set/map: O(1) per operation
+ Binary Search: O(log n)
+ Ordered set/map or Priority Queue: O(log n) per operation
+ Prime factorization or primality check for int: $O(\sqrt{n})$
+ Reading n inputs: O(n)
+ Iterating through n element array: O(n)
+ Sorting: Usually O(n log n) for =std::sort()=
+ Iterating through all subsets of size k of input elements: O($n^{k}$ ), for triplets O($n^{3}$)
+ Iterating through all subsets: O($2^{n}$)
+ Iterating through all permutations: O(n!)
*** Built-in Data Structures
Data Structure determines how data is stored, each supports some operations efficiently. In following discussion, desired data type is put between =<>=. Most std structures support =size()= and =empty()= methods.
**** Iterators
Allows for traversal of a container with the help of a pointer.
#+begin_src cpp
  for (vector<int>::iterator it = myvector.begin(); it != myvector.end(); ++it) {
    cout << *it; //prints the values in the vector using the pointer
  }
#+end_src

Alternate way to achieve the same with a for-each loop and =auto=.
#+begin_src cpp
  for(auto element : v){
    cout << element; // prints values in vector
  }
#+end_src

**** Dynamic Arrays
Addition and deletion at the end in O(1) time and in the middle in O(n) time.
#+begin_src cpp
  vector<int> v;
  for(int i = 1; i <= 10; i++){
    v.push_back(i); // stores 1 to 10 in a dynamic array
  }
#+end_src

Vectors can be made static sized by initializing it with a size, =vector<int> v(30);=. They also support an =v.erase()= operation. A dynamic array can be sorted (default ascending) by =sort(v.begin(), v.end())=.
**** Stacks and Queues
*Stacks*: LIFO with operations =push= (add at end), =pop= (remove at end) and =top= (show end) all of which are O(1). Declared as =stack<int> s=.

*Queues*: FIFO with operations =push= (add in front), =pop= (remove at end) and =front= (show end) in O(1).

*Deques*: Combination of a stack and a queue supporting insertion and deletion from both front and end. Operations are aptly named as =push_back=, =push_font=, =pop_back= and =pop_front=.

*Priority Queues*: Supports insertion of elements and deletion and retrieval of element /with highest priority/ in O(log n) where priority is based on a comparator function (highest element in front). Has =push= (add at end), =pop= (remove at end) and =top= (show end) operations and is declared as =priority_queue<int> pq;=.
**** Sets
A /set/ is a collection of objects having no duplicates.

*Unordered Sets*: Work by hashing that is, assigning a unique code to every object allowing for =insert=, =erase= and =count= (set contains element then 1 else 0) in O(1). Traversal is pointless. Declared as =unordered_set<int> s=.

#+begin_src cpp
  for(int element : s){
    cout << element << " "; // iterating through a set, arbitrary order
  }
#+end_src

*Ordered Sets*: Insertion, deletion and search needs O(log n) time. Has additional operations =begin()= (iterator to lowest element), =end()=, =lower_bound()= (iterator to least element \geq some k) and =upper_bound()=.

*Multisets*: A sorted set allowing multiple copies of same element, whose =count= operation returns the number of times an element is present in set. Time complexity of this operation is O(log n + f) where /log n/ factor searches for element and /f/ factor iterates through sorted set to get count. Declared as =multiset<int> ms=.
**** Maps
A /map/ is a set of /ordered pairs/ called key and value where keys must be unique but values can be repeated. Supported operations are addition and removal of key-value pair and /retrieval/ of values for a given key. Unordered maps perform aforementioned methods in O(1) whereas for ordered maps it's O(log n), sorted in order of key.

*Unordered Maps*: In map =m=, =m[key] = value= operator assigns value to a key and places the pair on the map, =m[key]= returns value associated with the key, =count(key)= checks for existence of key in the map and =erase(it)= removes pair associated with a key or iterator. Declared as =unordered_map<int, int> m=.

*Ordered Maps*: Supports additional operations =lower_bound= and =upper_bound= which return iterators pointing to lowest entry not less than/ strictly greater than a specified key.

#+begin_src cpp
  map<int, int> m; // [(3,5); (11,4)]
  m[10] = 491; // [(3,5); (10,491); (11,4)]
  cout << m.lower_bound(10)->first << " " << m.lower_bound(10)->second << "\n";
  // 10 491
  cout << m.upper_bound(10)->first << " " << m.upper_bound(10)->second << "\n";
  // 11 4
  m.erase(11); // [(3,5); (10,491)]
#+end_src
*** Simulation

** Meditations
CLOSED: [2022-03-14 Mon]
:PROPERTIES:
:EXPORT_FILE_NAME: meditations
:END:
*** Book 2: On the River Gran, Among the Quadi
And so none of them can hurt me. No one can implicate me in ugliness. Nor can I feel angry at my relative, or hate him. We were born to work together like feet, hands, and eyes, like the two rows of teeth, upper and lower. To obstruct each other is unnatural. To feel anger at someone, to turn your back on him: these are obstructions.

Concentrate every minute like a Roman- like a man- on doing what's in front of you with precise and genuine seriousness, tenderly, willingly, with justice. And on freeing yourself from all other distractions. Yes, you can- if you do everything as if it were the last thing you were doing in your life, and stop being aimless, stop letting emotions override what your mind tells you, stop being hypocritical, self-centered and irritable.

Do external things distract you? Then make time for yourself to learn something worthwhile; stop letting yourself be pulled in all directions. But make sure you guard against the other kind of confusion. People who labor all their lives but have no purpose to direct every thought and impulse toward are wasting their time- even when hard at work.

You could leave life right now. Let that determine what you do and say and think.

Nothing is more pathetic than people who run around in circles, "delving into things that lie beneath" and conducting investigations into the souls of the people around them, never realizing that all you have to do is to be attentive to the power inside you and worship it sincerely.

What is divine deserves our respect because it is good; what is human deserves our affection because it is like us. And our pity too, sometimes, for its inability to tell good from bad- as terrible a blindness as the kind that can't tell white from black.

The present is all that they can give up, since that is all you have, and what you do not have, you cannot lose.

The human soul degrades itself.

Above all, when it does its best to become an abscess, a kind of detached growth on the world.

When it allows its action and impulse to be without a purpose, to be random and disconnected: even the smallest things ought to be directed toward a goal.

The body and its parts are a river, the soul a dream and mist, life is warfare and a journey far from home, lasting reputation is oblivion.
Then what can guide us?
Only philosophy.

And nothing natural is evil.
*** Book 3: In Carnuntum
But getting the most out of ourselves, calculating where our duty lies, analyzing what we hear and see, deciding whether it's time to call it quits- all the things you need a healthy mind for... all those are gone.
So we need to hurry.
Not just because we move daily closer to death but also because our understanding- our grasp of the world- may be gone before we get there.

We should remember that even Nature's inadvertence has its own charm, its own attractiveness.

You boarded, you set sail, you've made the passage. Time to disembark. If it's for another life, well, there's nowhere without gods on that side either. If to nothingness, then you no longer have to put up with pain or pleasure, or go on dancing attendance on this battered crate, your body- so much inferior to that which serves it.
One is mind and spirit, the other earth and garbage.

Don't waste your the rest of your time here worrying about other people- unless it affects the common good. It will keep you from doing anything useful. You'll be too preoccupied with what so-and-so is doing, and why, and what they're saying, and what they're thinking, and what they're up to, and all the other things that throw you off and keep you from focusing on your own mind.
You need to avoid certain things in your train of thought: everything random, everything irrelevant. And certainly everything self-important or malicious.

He does only what is his to do, and considers constantly what the world has in store for him- doing his best, and trusting that all is *for* the best. For we carry our fate with us- and it carries us.

And he cares nothing for their praise- men who can't even meet their own standards.

... then don't make room for anything but it- for anything that might lead you astray, tempt you off the road, and leave you unable to devote yourself completely to achieving the goodness that is uniquely yours.

Choose what's best.
-Best is what benefits *me*.
As a rational being? Then follow through. Or just as an animal? Then say so and stand your ground without making a show of it.

Never regard something as doing you good if it makes you betray a trust, or lose your sense of shame, or makes you show hatred, suspicion, ill will, or hypocrisy, or a desire for things best done behind closed doors.

Your ability to control your thoughts- treat it with respect. It's all that protects your mind from false perceptions- false to your nature, and that of all rational beings. It's what makes thoughtfulness possible, and affection for other people, and submission to the divine.

It you do the job in a principled way, with diligence, energy and patience, if you keep yourself free of distractions, and keep the spirit inside you undamaged, as if you might have to give it back at any moment-
If you can embrace this without fear or expectation- can find fulfillment in what you're doing now, as Nature intended, and in superhuman truthfulness (every word, every utterance)- then your life will be happy.
No one can prevent that.

To make your mind your guide to what seems best: even people who deny the gods do that. Even people who betray their country. Even people who do sex behind closed doors.
If all rest is common coin, then what is unique to the good man?
To welcome with affection what is sent by fate. Not to stain or disturb the spirit within him with a mess of false beliefs.
*** Book 4
People try to get away from it all- to the country, to the beach, to the mountains. You always wish that you could too. Which is idiotic: you can get away from it anytime you like.
By going within.

An instant's recollection and there it is: complete tranquility. And by tranquility I mean a kind of harmony.

The things that have no hold on the soul. They stand there unmoving, outside it. Disturbance comes only from within- from our own perceptions.
That everything you see will soon alter and cease to exist. Think of how many changes you've already seen.
"The world is nothing but change. Our life is only perception."

If thought is something we share, then so is reason- what makes us reasoning beings.
If so, then the reason that tells us what to do and what not to do is also shared.
And if so, we share a common law.
And thus, are fellow citizens.
And fellow citizens of something.
And in that case, our state must be the world. What other entity could all of humanity belong to? And from it- from this state that we share- come thought and reason and law.

Choose not to be harmed, and you won't feel harmed. Don't feel harmed- and you haven't been.

It can ruin your life only if it ruins your character[fn:1]. Otherwise it cannot harm you- inside or not.

Two kinds of readiness are constantly needed:
1. to do only what the *logos* of authority and law directs, with the good of human beings in mind;
2. to reconsider your position, when someone can set you straight or convert you to his. But your conversion should always rest on a conviction that it's right, or benefits others- nothing else. Not because it's more appealing or more popular.

You have a mind?
-Yes.
Well, why not use it? Isn't that all you want- for it to do its job?

Not to live as if you had endless years ahead of you. Death overshadows you. While you're alive and able- be good.

The tranquility that comes when you stop caring what they say. Or think, or do. Only what *you* do. (Is this fair? Is this the right thing to do?)

But suppose those who remembered you were immortal and your memory undying. What good would it do you? And I don't just mean when you're dead, but in your own lifetime. What use is praise, except to make your lifestyle a little more comfortable?
"You're out of step- neglecting the gifts of nature to hand on someone's words in the future."

Beautiful things of any kind are beautiful in themselves and sufficient to themselves. Praise is extraneous. The object of praise remains what it was- no better and no worse. This applies, I think, even to "beautiful" things in ordinary life- physical objects, artworks.

Because most of what we say and do is not essential. If you can eliminate it, you'll have more time, and more tranquility. Ask yourself at any moment, "Is this necessary?"

Love the discipline you know, and let it support you. Entrust everything willingly to the gods, and then make your way through life- no one's master and no one's slave.

A key point to bear in mind: The value of attentiveness varies in propotion to its object. You're better off not giving the small things more time than they deserve.

Then what should we work for?
Only this: proper understanding; unselfish action; truthful speech. A resolve to accept whatever happens as necessary and familiar, flowing like water from that same source and spring.

On the verge of dying and still weighed down, still turbulent, still convinced external things can harm you, still rude to other people, still not acknowledging the truth: that wisdom is justice.

"A little wisp of soul carrying a corpse." - Epictetus.

What follows coheres with what went before. Not like a random catalogue whose order is imposed upon it arbitrarily, but logically connected. And just as what exists is ordered and harmonious. what comes into being betrays an order too. Not a mere sequence, but an astonishing concordance.

"Our words and actions should not be like those of sleepers" (for we act and speak in dreams as well) "or of children copying their parents" -doing and saying only what we have been told.

Suppose that a god announced that you were going to die tomorrow "or the day after". Unless you were a complete coward you wouldn't kick up a fuss about which day it was- what difference could it make? Now recognize that the difference between years from now and tomorrow is just as small.

Our lifetime is so brief. And to live it out in these cirumstances, among these people, in this body? Nothing to get excited about. Consider the abyss of time past, the infinite future. Three days of life or three generations: what's the difference?

Take the shortest route, the one that nature planned- to speak and act in the healthiest way. Do that, and be free of pain and stress, free of all calculation and pretension.

[fn:1] Character, as in the upholding of your innate morals, not as what is percieved by others.
*** Book 5
At dawn, when you have trouble getting out of bed, tell yourself: "I have to go to work- as a human being. What do I have to complain of, if I'm going to do what I was born for- the things I was brought into the world to do? Or is *this* what I was created for? To huddle under the blankets and stay warm?"
-But it's nicer here...
So you were born to feel "nice"? Instead of doing things and experiencing them? Don't you see the plants, the birds, the ants and spiders and bees going about their individual tasks, putting the world in order, as best as they can? And you're not willing to do your job as a human being? Why aren't you running to do what your nature demands?

But nature set a limit on that- as it did on eating and drinking. And you're over the limit. You've had more than enough of that. But not of working. There you're still below your quota.

Is helping others less valuable to you? Not worth your effort?

If an action or utterance is appropriate, then it's appropriate for you. Don't be put off by other people's comments and criticism. If it's right to say or do it, then it's the right thing for you to do or say.

Practice the virtues you *can* show: honesty, gravity, endurance, austerity, resignation, abstinence, patience, sincerity, moderation, seriousness, high-mindedness. Don't you see how much you have to offer- beyond excuses like "can't"? And yet you still settle for less.

Prayer for the Athenians:
/Zeus, rain down, rain down
On the lands and fields of Athens./
Either no prayers at all- or one as straightforward as that.

Look at the accomplishment of nature's plans in that light- the way you look at your own health- and accept what happens (even if it seems hard to accept). Accept it because of what it leads to: the good health of the world, and the well-being and prosperity of Zeus himself, who would not have brought this on anyone unless it brought benefit to the world as a whole. No nature would do that- bring something about that wasn't beneficial to what it governed.

[To be continued...]
** Computation :noexport:
CLOSED: [2022-06-02 Thu]
:PROPERTIES:
:EXPORT_FILE_NAME: computation-das
:END:
*** Simplicity
**** Turing Machine
**** Lambda Calculus
*** Limits - The Halting Problem
*** Structure
**** Turing Equivalence
**** Finite State Machines
**** Chomsky Hierarchy
1. Regex
2. Recognize Python
3. Recognize C++, JS etc
4. Turing-equivalent
** Differential Geometry :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: dg-notes
:END:
*** Theory of Space Curves
**** Representation of space curves
+ Level Curve: f(x,y,z) = C
+ From level curves to parametrized curves:
  $y=x^{2} <-----> \gamma(t)=(\gamma_{1}(t),\gamma_{2}(t))$ Taking $\gamma_{1}(t)=t$, we get $\gamma_{2}(t)=t^{2}$ hence the parametrization is $\gamma(t)=(t,t^{2})$
+ *NOTE:* Check if domain of /x/ satisfies domain of /t/ or not. That is, the same parametrisation can be represented as $(t^{2}.t^{4})$ or $(t^{3},t^{6})$ but only the latter is a correct representation.
+ From parametrized curves to level curves:
  $\gamma(t)=(cos^{3}t,sin^{3}t)$ <------> F(x,y)=C; Using $sin^{2}t+cos^{2}t=1$ we get, $x^{2/3}+y^{2/3}=1$ as the level curve.
**** Unique Parametric representation
+ Class 'm' \rightarrow /f/ is m-differentiable
+ A curve is /smooth/ if $\frac{d^{n}f}{dt^{n}}$ exists for all n \geq 1 and t \in (\alpha,\beta)
+ A function /f/ is /analytic/ if it is single valued and of class \infty
+ A function is /regular/ if it is differentiable and derivative is non-zero (f dot \neq 0)
+ A /regular f/ of class /m/ can also be called a /*path*/ of class /m/.
+ *NOTE:* A point of a parametrized curve can have multiple tangents.
**** Arc-length
+ Arc-length of a curve \gamma is given by the function $s(t)=\int_{t_{0}}^{t}|| \dot{\gamma}(u)|| du$
+ Speed: \(|| \dot{\gamma}(t) ||_{t}\) and a curve is unit-speed curve if its magnitude is 1 for all /t/.
+ For \gamma being a unit speed curve, $\ddot{\gamma}$ is zero or perpendicular to $\dot{\gamma}$ i.e. $\ddot{\gamma}.\dot{\gamma}=0$
+ If \gamma is a regular curve, then its arclength S at any point of \gamma is a smooth function of t.
+ Reparametrization: $\overline{\gamma}:(\overline{\alpha},\overline{\beta}) \rightarrow R^{n}$ <=> $\gamma: (\alpha,\beta) \rightarrow R^{n}$  exists iff \exists a smooth function \phi: $(\overline{\alpha},\overline{\beta}) \rightarrow (\alpha,\beta)$ such that its inverse \phi^{-1} is also smooth.
+ A /unit speed reparametrization/ exists for a curve iff it is /regular/.
**** Tangent and Osculating Plane
+ Assuming \gamma is a class \geq 1 i.e. it has a power series expansion,
\[ \gamma(u)=\gamma(u_{0}+h)=\gamma(u_{0})+\frac{h}{1!}\dot{\gamma}(u_{0})+\frac{h^{2}}{2!}\ddot{\gamma}(u_{0})+ ... + \frac{h^{n}}{n!}\gamma^{n}(u_{0})+O(h^{n})
\]
  where $h = u-u_0$
+ Let \gamma be class m \geq 2 and (P,Q) be points limiting position of a plane that contains tangential line at P and passes through Q as Q \rightarrow P is defined as the /osculating plane/.
+ *Tangent line:* $\vec{R}(t)=\vec{r}(u_{0})+t \vec{r'}(u_{0})$ at $u_{0}$
+ *Osculating Plane:* $[\vec{R}-\vec{r(0)}, \vec{r'(0)}, \vec{r''(0)}]=0$ where $\vec{R}=(X,Y,Z)$ gives the equation of the OP (here $\vec{r''}(0)\neq0$). The product inside the box is /scalar triple product/. Also, the OP passes through the unit vector of the curve and is perpendicular to the unit binormal vector.
+ Note that for smallest k \geq 2 such that $\vec{r^{(k)}}=0$, the last term in the box is replaced by $\vec{r'}^{(k)}(0)$
**** Principal normal and binormal
+ *Normal Plane:* $\vec{t}(0).(\vec{R}-\vec{r}(0)) = 0$
  It is perpendicular to the tangent line and is spanned by /n,b/
+ *Principal Normal Vector:* For m \geq 1, $\vec{n}=\frac{\vec{r''}(0)}{||\vec{r''}(0)||}$
+ *Unit Binormal Vector:* $\vec{b}=\vec{t}\times\vec{n}$
+ OP: b.(R-r)
+ NP: t.(R-r)
+ RP: n.(R-r)
**** Curvature and Torsion
+ For a /unit speed curve/ or /arc length parametrized/ curve \gamma(t), the curvature \kappa(t) is defined as $||\ddot{\gamma}(t)||$ (1)
+ For a /regular/ curve \gamma(t) *in* $R^{3}$, $\kappa = \frac{||\ddot{\gamma}\times\dot{\gamma}||}{||\dot{\gamma}^{3}||}$
+ For a unit speed curve \gamma, /unit tangent vector/ $\hat{t}=\dot{\gamma}$ and for \kappa \neq 0, /unit normal vector/ is given by  $\hat{n}(s)=\frac{\dot{\hat{\gamma}}(s)}{\kappa(s)}$ since (1). And /unit binormal vector/ can be given by $\hat{b}=\hat{t}\times\hat{n}$
+ *Orthonormal Basis* of a curve is given by {$\hat{t},\hat{n},\hat{b}$}
+ Now b is given by t \times n , hence $\dot{b}=\dot{t}\times n+t\times\dot{n}$ , since $\dot{b}$ has to be perpendicular to t and b, $\implies \ddot{b}||n$, therefore $\boxed{\dot{b}=-\tau n}$ *iff* \kappa \neq 0.
+ Torsion measures the arc rate of turning of osculating plane.
+ For a regular curve \gamma in $R^{3}$ with \kappa \neq 0, the /torsion/ is given by
  \[
  \tau = \frac{(\dot{\gamma}\times\ddot{\gamma}).\dddot{\gamma}}{||\dot{\gamma}\times\ddot{\gamma}||^{2}}
  \]
+ Also, /radius of curvature/ \rho is inverse of curvature.
+ Finally, tying it all together is the /Serret-Frenet formula/ (arc length parameter):
  $\begin{bmatrix} \dot{t} \\
   \dot{n} \\
   \dot{b}  \end{bmatrix} = \begin{bmatrix} 0 & \kappa & 0 \\
    -\kappa & 0 & \tau \\
    0 & -\tau & 0 \end{bmatrix} \begin{bmatrix} t \\
    n \\
    b \end{bmatrix}$
**** Behaviour of a curve near one of its points
+ For a regular curve of class m \geq 2 with nonvanishing curvature, the curve is /planar/ iff \tau=0 everywhere.
+ For an analytic curve with arc length parameter, as s \rightarrow 0, a new parametrization for small s can be defined as:
  \[
    X = s - \frac{\kappa^{2}s^{3}}{6} - \frac{\kappa\kappa' s^{4}}{8} + o(s^{4})
  \]
  \[
    Y = \frac{\kappa s^{2}}{2} + \frac{\kappa' s^{3}}{6} + \frac{\kappa''-\kappa\tau-\kappa^{3}}{24} s^{4} + o(s^{4})
  \]
  \[
   Z = \frac{\kappa\tau}{6}s^{3} + \frac{2\kappa'\tau+\kappa\tau'}{24}s^{4} + o(s^{4})
  \]
+ Here the o notation represents that for f = o(g), as s \rightarrow 0, $lim \frac{f(s)}{g(s)}=0$
+ From previous theorem:
  1. $\kappa(0) = \lim_{s \to 0} \frac{2Y}{X^{2}}$
  2. $\tau(0) = \lim_{s \to 0} \frac{3Z}{XY}$
  3. For $P=\vec{r}(0), Q=\vec{r}(s)$, the length of chord
     \[
      PQ = s(1-\frac{\kappa^{2}s^{2}}{24}) + o(s^{3}) \~ s(1-\frac{\kappa^{2}s^{2}}{24})o(s^{3})
    \]
    If f(t)=g(t)+o(t), then as t \rightarrow 0, it can be written as f(t)~g(t)o(t)
+ The length of common perpendicular between tangents at two nearby points of $\vec{r}(s)$ at arcual distance /s/ is approximately $d=\frac{\kappa\tau s^{3}}{12}$. This is the shortest distance between tangents at nearby points of r(s).
**** Contact between curves and surface
+ For a surface S: F(x,y,z)=0 and a parametrized curve C: $\vec{r}(u)$ = (f(u),g(u),h(u)), let P be a point on C. P lies on S iff F(f(P),g(P),h(P))=0.
+ Let \phi(u) = F(f(u),g(u),h(u)) for any parameter value u. Then P lies on S iff \phi(u_{0})=0.
+ Assuming F and $\vec{r}$ are of class m for sufficiently large m, then \phi(u) has a taylor expansion where $\frac{O(h^{n+1})}{h^{n+1}}$ is bounded as h \rightarrow 0.
+ Definition: Surface S and a parametrized curve C has an /n-point contact/ (or contact of order n) at P if $\phi(u_{0}) = \phi'(u_{0}) = ... = \phi^{(n-1)}(u_{0}) = 0$ and $\phi^{(n)}(u_{0})\neq 0$
+ If S and C have a contact of order 1 at P then it is called a /simple intersection/ of S and C.
+ If P is in n-point contact of S and C, then S and C intersect at P in /n/ coincidental points.
+ Condition for /n-point contact/ at P is invariant under a change of parameter.
+ Osculating Plane at P of $\vec{r}$ has atleast a 3-point contact with $\vec{r}$ at P.
**** Osculating circle (circle of curvature)
+ For a regular curve $\vec{r}(s)$ of class m \geq 2, let $P=\vec{r}(0)$ and $P_{i}=\vec{r}(s_{i}), i=1,2,3$ be 3 non collinear points near P on the curve. Then there is a unique circle through all $P_{i}$. The limiting circle, if existent, for all $P_{i} \rightarrow P$ is called /osculating circle/ of r(s) at P.
+ Center of OC (c) is called /centre of curvature/ of r(s) at P while its radius \rho(0) is called radius of curvature. Also, the OC lies in the OP.
+ Theorem: $\rho(0)=\frac{1}{\kappa(0)}$, $\vec{c}(o)= \vec{r}(0)+\rho(0)\vec{n}(0)$
+ OC does not exist at points where curvature vanishes and OC of a circle is the same circle itself.
**** Osculating Sphere
+ Definition: For a regular path r(s) of class m \geq 2, assuming P = r(0) and \kappa(0)\tau(0) \neq 0, a sphere which has atleast a 4-point contact with r(s) at P is called /osculating sphere/ at P on r.
+ \rho(s)= $\frac{1}{\kappa(s)}$ is called radius of curvature and \sigma(s)= $\frac{1}{\tau(s)}$ is called radius of torsion of r(s)
+ Theorem: OS at P on r is given by $|\vec{c}-\vec{R}|^{2} = R^{2}$ where $R = \sqrt{\rho(0)^{2}+\sigma(0)^{2}\rho'(0)^{2}}$ and $\vec{c}=\vec{r}(0)+\rho(0)\vec{n}(0)+\sigma(0)\rho'(0)\vec{b}(0)$ where c and R are COSC and ROSC to r(s) at r(0)
+ Centre of OS lies in the normal plane of r(s) as $c-r(0)$ is a linear combination of n(0) and b(0)
+ If \kappa is constant then ROC=ROSC and COC=COSC. In particular, if r is a circle, then its its own OC and is a great circle of the OS.
**** Locus of centres of spherical curvature
+ Since COSC at r(s) is $c(s) =r(s)+\rho(s)n(s)+\sigma(s)\rho'(s)b(s)$, it moves along a path as /s/ varies. For this path, SFF, \kappa, \tau can be calculated and will be denoted with subscript c.
+ Assuming \tau(s)>0,
  1. $c'(s) = (\frac{\rho(s)}{\sigma(s)}+ \frac{d (\sigma(s)\rho'(s))}{ds})b(s)$
  2. For a regular c(s), unit tangent vector is $t_{c}(s) = eb(s)$
  3. $\frac{ds_{c}}{ds}=|\frac{\rho(s)}{\sigma(s)}+\frac{d(\sigma(s)\rho'(s))}{ds}|$
  Here e is 1 if ds_{c}/ds > 0, -1 ow. Also $e = t_{c}(s).b(s)$
+ Also on differentiating,
  1. $\kappa_{c}(s) = \frac{\tau(s)}{\frac{ds_{c}}{ds}}$ or \kappa(s)= $-\tau_{c}(s)e \frac{ds_{c}}{ds}$
  2. Which gives $\tau(s)\tau_{c}(s)=\kappa(s)\kappa_{c}(s)$
+ Theorem: ROC of center of curvatures (i.e. center of OCs) is given by
  \[
  \rho_{1} = [( \frac{\rho^{2}\sigma}{R^{3}}\frac{d}{ds}(\frac{\sigma\rho'}{\rho})-\frac{1}{R} )^{2} + \frac{\rho'^{2}\sigma^{4}}{\rho^{2}R^{4}}]^{-1/2}
  \]
**** Tangent surfaces, involutes and evolutes
+ Definition: Tangent surface to a curve r is union of all tangent lines to r at all its points.
+ Tangent line to r at r(s) is R(u,s) = r(s)+ur'(s)
+ For both varying r and u, one gets the tangent surface.
+ Image of the curve u=u(s) in us-plane gives a curve $r_{1}(s)=r(s)+u(s)r'(s)$
+ Definition: Involute of r is a curve on the tangent surface of r which meets all generating lines orthogonally at corresponding points.
+ If $r_{1}(s)$ denotes the pos vector on the involute C_1 of a curve C corresponding to its points r(s) then r_{1}(s)=r(s)+(c-s)t(s) for a constant c.
+ For an involute c(s) of a regular path r(s) of class m \geq 2.
  \[
    \kappa_{c}^2 = \frac{\tau^{2}+\kappa^{2}}{\kappa^{2}(c-s)^{2}}, \tau_c = \frac{\kappa\tau'-\kappa'\tau}{\kappa(c-s)(\tau^{2}+\kappa^{2})}
  \]

+ Definition: If $\overline{C}$ is an involute of C then C is called an evolute of $\overline{C}$.
+ For a regular curve r(s), evolute is given by $r_{1}(s)=r(s)+\rho(s)n(s)+\rho(s)cot(\psi(s)+c)b(s)$ where c is a constant and \psi(s) = $\int \tau(s)ds$
+ r(s) has infinitely many evolutes, as c is random constant. For a plane curve, \tau = 0.
+ Tangents to two different evolutes corresponding to two constans A and B drawn from the same point of the given curve are inclined to each other at a constant angle A-B.
  \[
    r_{1} = r+\rho\textbf{n}-\rho tan(\psi+a)\textbf{b}
  \]
  Further $\psi = \int \tau ds$ so that \psi'=\tau...
*** First Fundamental Form and Local Intrinsic Properties of a Surface
**** Introduction
+ The surfaces are defined similar to curves by an equation of the type F(/x,y,z/) = 0 or parametrically by expressing /x,y,z/ in terms of two parameters /u,v/ varying over a domain.
+ After defining the surface locally, its points are classified as ordinary or singular.
+ Then using tangent plane at a point and the surface normal at it, a coordinate system *\((r_1, r_2, N)\)* at every point of the surface is introduced.
+ After that, a certain quadratic differential form known as /first fundamental form/ on a surface and direction coefficients are introduced.
**** Definition of a Surface
*Definition 1:* Locus of a point P(/x,y,z/) in $E_{3}$ satisfying some restrictions on /x,y,z/ which is expressed by a relation of the type F(/x,y,z/) = 0.

This equation is called the /implicit/ or the /constraint/ equation of the surface which allows for a global study of the surface.

*Definition 2:* For parameters /u, v/ taking real values and varying over a domain D, a surface is defined /parametrically/ as
  \[
      x = f(u,v), y = g(u,v), z = h(u,v)
  \]
  where /f, g/ and /h/ are single valued continuous functions possessing continuous derivatives of /r/-th order. Such surfaces are called surfaces of class /r/.

Parametric representation is useful for local study of surfaces i.e. in the neighbourhood of a point which is a small region *but* it is not unique for a surface. Also, the parameters /u/ and /v/ are called /curvilinear coordinates/.

*Definition 3:* For two parametric representations /u, v/ and /u', v'/ of the same surface, any transformation of the form $u'=\phi(u,v)$ and $v'=\psi(u,v)$ relating the two representations is called a /parametric transformation/.

*Definition 4:* A parametric transformation is /proper/ if:
  1. \phi and \psi are single valued functions.
  2. The Jacobian $\frac{\delta (\phi,\psi)}{\delta (u,v)}\neq0$ in some domain D.
These conditions are necessary and sufficient for existence of inverse in the neighbourhood of any point in D' which is the domain of /u', v'/ corresponding to the domain D of the /u, v/ plane.
**** Nature of Points on a Surface
*Notation:* For *r* being the position vector of a point on the surface, *r* = (x,y,z), we can take r = r(u,v) as the parametric form of the surface and use $r_1 = \frac{\delta r}{\delta u} = (x_{1},y_{1},z_{1})$ and $r_2 = \frac{\delta r}{\delta v} = (x_{2},y_{2},z_{2})$, similarly we can denote second order derivatives using $r_{11}, r_{21}$ etc.

*Definition 1:* If $r_{1}\times r_{2}\neq0$ at a point on a surface, then the point is called an /ordinary/ point. A point which is not an ordinary point is called a /singularity/.

Remarks:
+ Considering M = $\begin{bmatrix} x_{1} & y_{1} & z_{1}\\
  x_{2} & y_{2} & z_{2}\end{bmatrix}$
  For $r_{1} \times r_{2} \neq 0$ at an ordinary point, i.e. rank of M is two at that point.
+ If the rank of M is either zero or one, the point on the surface is a singular point.
+ If $r_{1} \times r_{2}\neq0$ or equivalently rank of M is two, then /x,y,z/ uniquely determine the parameters /u,v/ in the neighbourhood of an ordinary point.
+ When only one determinant minor of M is zero, one cannot conclude that the point is a singular point.
+ A /proper/ parametric transformation transforms an ordinary point into an ordinary point.
+ Due to geometrical nature of the surface, some singularities continue to be singularities, regardless of the parametric representations, these are called /essential singularities/.
+ There are other singularities depending on the choice of parametric representation which are called /artificial singularities/.
*Example:* Consider the circular cone represented by /x = u sin\alpha cosv, y = u sin\alpha sinv, z = u cos\alpha/ where \alpha is the semivertical angle of cone with O as origin and OP = /u/, where P is any point on the cone.
Computing M, then at /u/ = 0, the determinant of every second order minor is zero, hence it is an essential singularity.

*Example:* Taking any point 0 as origin in the plane, /x = u cosv, y = u sinv, z = 0/, we get $r_{1} \times r_{2} = u\textbf{k}$. Hence it is zero only when /u/ = 0 i.e. it is an artificial singularity /since/ it arises due to the choice of the parametric coordinates and not due to the nature of the surface.
**** Representation of a Surface
For our study of surfaces, we consider only ordinary points. And we consider the entire surface as a collection of parts, each part being given a particular parametrisation and the adjacent parts being related by a /proper/ parametric transformation.

*Definition 1:* A representation R of a surface S of class /r/ in $E_{3}$ is a collection of points in $E_{3}$ covered by a system of overlapping parts ${S_{j}}$ where each part {{$S_{j}$} is given by a parametric equation of class /r/. Each point lying in the common portion of two parts $S_{i}, S_{j}$ is such that the change of parameters from one part to is adjacent is given by a /proper/ parametric transformation of class /r/.

*/Note:/* Since one cannot parameterise the whole surface without introducing artificial singularities, one has to resort to a surface composed of many overlapping parts.

It is possible to have many representations of the same surface by considering different systems of overlapping parts ($S_{j}$), each part is given by a parametric equation of class /r/.

*Definition 2:* For R and R' being two representations of class /r/ of the surface S, they are /equivalent/ if the composite family of parts {$S_{j},S'_{j}$} satisfies the condition that for each point P lying in the place of overlap, the change of parameter from $S_{j}$  to $S'_{j}$ at P is given by a proper parametric transformation of class r.

*Theorem:* The notion of /r/-equivalence of representations of a surface is an equivalence relation.

This equivalence relation introduces a partition into the family of surfaces of class /r/ splitting them into mutually disjoint equivalence classes, each class containing the surface equivalent to one another in the above equivalence relation.

*Definition 3:* A surface S of class /r/ in $E_{3}$ is an /r/-equivalence class of representations.

Thus a surface consists of different overlapping portions related to one another by proper parametric transformations and all other surfaces related to the given one by the equivalence relation of class /r/.
**** Curves on Surfaces
For a surface *r* = r(/u,v/), let /u = u(t)/ and /v = v(t)/ be a curve of class /s/ lying in the domain D of the /uv/-plane. Considering *r* = r[u(t), v(t)] which gives the position vector of a point in terms of a single parameter /t/ such that it is a curve lying on a surface with class equal to the smaller of /r/ and /s/. The equation /u = u(t)/ and /v = v(t)/ are called /curvilinear equations/ of the curve on the surface.

*Definition 1:* For *r*, a given surface of class /r/, let /v = c/, then position vector *r* = r(u,c) is a function of a single parameter /t/ and hence *r* = r(u,c) represents a curve lying on the surface *r* = r(u,v). This curve is called the /parametric curve/ v = constant.

By varying the values of /c/, a system of parametric curves /v/ = constant is generated and similarly another system is generated by keeping /u/ constant and varying /v/.

Properties that are a consequence of assuming only ordinary points on the surface:
1. Through every point of the surface, there passes one and only one parametric curve of each system.
2. No two curves of the same system intersect.
3. The curves of the system $u=u_{o}$ and $v=v_{o}$ intersect once but not more than once if $(u_{o},v_{o}) \in D$.
4. The parametric curves of the system u = $c_{1}$ and v = $c_{2}$ cannot touch each other.

*Definition 2:* Let u = $c_{1}$ and v = $c_{2}$, when the constants vary, the whole surface is covered with a net of parametric curves, two of which pass through each point.

*Definition 3:* Two parametric curves through a point P are /othogonal/ if $\textbf{r}_{1}.\textbf{r}_{2}= 0$ at P.
**** Tangent Plane and Surface Normal
Let *r* = r[u(t), v(t)] be a general curve lying on the surface passing through [u(t), v(t)], then the tangent to the curve at any point P on the surface is
\[
\frac{dr}{dt} = r_{1}\frac{du}{dt}+r_{2}\frac{dv}{dt}
\]
*Definition 1:* Tangent to any curve drawn on a surface is called a tangent line to the surface. The tangents to different curves through P on a surface lie in a plane containing two independent vectors $r_{1}$ and $r_{2}$ at P called the /tangent plane/ at P.

*Theorem 1:* The equation of a tangent plane at P on a surface with position vector *r* = r(u,v) is either \(R = r+ar_{1}+br_{2}\) or \((R-r).(r_{1}\times r_{2}) = 0\) where a and b are parameters.

*Definition 2:* The normal to the surface P is a line through P and perpendicular to the tangent plane at P.

*Theorem 2:* The equation of the normal *N* at a point P on the surface r = r(u,v) is \(R=r+a(r_{1}\times r_{2})\).

*Theorem 3:* A proper parametric transformation either leaves every normal unchanged or reverses the direction of the normal.
**** General Surface of Revolution
*Definiton 1:* A surface generated by the rotation of a plane curve about an axis in its plane is called a /surface of revolution/.

*Theorem 1:* The position vector of any point on the surface of revolution generated by the curve [g(u),o,f(u)] in the XOZ plane is
  $\textbf{r} = [g(u)cosv, g(u)sinv, f(u)]$
where /v/ is the angle of roatation about the /z/-axis.
** Analog and Digital VLSI Design :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: advd-notes
:END:
*** Radio Spectrum
+ Used for communication initially
+ Wireless communication
+ Radio Spectrum is divided into frequency bands which are allocated to certain services.
+ The band is subdivided into channels that are used for particular transmission.
+ The wider the frequency bands and the channel, the more information that can be passed through them.
  | Frequency             | Use                        |
  |-----------------------+----------------------------|
  | VLF                   | Maritime Navigation        |
  | LF                    | Maritime Navigation        |
  | MF                    | AM Radio                   |
  | HF                    | Shortwave Radio            |
  | VHF (30-300 MHz)      | TV, FM Radio               |
  | UHF (300 MHz - 3 GHz) | TV, Mobile, GPS, Wi-FI, 4G |
  | SHF                   | Satellite                  |
  | EHF                   | Radio Astronomy            |
+ LF Bands provide wider coverage due to *high penetration power* but they have *poor capacity* (carry less information).
+ HF bands have greater capacity but less wider coverage.
+ Cell phones are multi-band device, when one's closer to a radio tower/station, it uses HF bands, but at poor reception they fall back to LF bands (GSM: 900-1800 MHz).
+ Wireless networks cover large amounts of area via a number of low-power radio stations laid out in hexagonal, cell-like grids.
+ Cellular commuication works by transmitting analog voice/data after amplification and conversion to digital bits into the environment and then received by selecting the corresponding frequency (highly selective network), processing the data (noise removal etc) and then converting back to analog audio. This process is know as modulation-demodulation.
[[./assets/advd-rf-tran.png][file:./assets/advd-rf-tran.png]]
+ Elements of a transceiver: /Oscillators, phase-clocked loops, frequency synthesizers, converters, filters, power circuits/ having *high data rate, resolution, less cost and energy per conversion*.
*** FIXME VLSI Design - An Overview
+ *Moore's Law:* Number of components (transistors) in ICs would double every two years. This was possible because of /scaling/.
[[./assets/advd-moore-law.png][file:./assets/advd-moore-law.png]]
+ Learn how to convert schematic into a layout and vice-versa.
+ First microprocessor from intel - 4004, 8 bit
+ FPGA: Customizable pre-fabricated design
+ VLSI Design Styles

*** Fabrication
+ Sequence of steps that are followed to get a silicon chip with different patterns
+ Clean room: Class 1 = 1 dust particle in 1 ft^{3}
+ VLSI Design flow:
  Functional Description (Verilog) \rightarrow Circuit Design \rightarrow Layout \rightarrow Masks (Patterns)
+ Twin-tub process: For p-mos, there's an n-well and vice versa.
+ [Simplified-CMOS-Process.jpg]
+ CVD: Growing Field Oxide and gate oxide
+ Lithography: Process of patterning the silicon
+ Why Si over Ge? Band gap Si>Ge, Ge can't be used in mass production due to lack of raw material also SiO_2 is highly stable whereas GeO is soluble in water.
+ Getting that wafer:
  Sand \rightarrow SiO_2 \rightarrow Metallurgical Grade Si (99.9% Pure) \rightarrow CZ Chamber (1000^\odot C) \rightarrow Seed Crystal + Molten Si \rightarrow Si crystal ingot \rightarrow Diamond saw \rightarrow Polishing \rightarrow Silican Wafer
+ Dopants are introduced in the CZ chamber via /diffusion/ion implantation/, n-type: B (Pentavalent), p-type: P(Trivalent)
+ Diffusion:
  Temperature is around 650 C, Carrier made of quartz, Dopant in either crystal or powdered form, preheating temperature slightly lower than furnace, carrier gas carries the dopant vapours onto the silicon wafer by getting into the vacant sites of lattice defects and when they move from interstitional locations to lattice positions, doping is complete.
+ Fick's Law: Determines the amount of dopant required, diffusion temperature and the duration of the diffusion.
+ Ion Implantation:
  Source of the dopants are in ionic (charged) form, so an ion source releases a beam of ions which is columated by lenses to a small spot size called aperture, this accelerated beam of ions hits the silicon surface and the bombardment results in dislodging of Si atoms from the lattice, and the broken bonds are healed and dopant settling is done via /annealing/ (heating of wafer post-implantation).
+ Deposition:
  Used to deposit different materials from SiO_2 to metals, it can be achieved either chemically or physically. CVD is similar to diffusion whereas PVD is akin to ion implantation.
+ For metal deposition, generally MCl_2 are used since on reaction with hydrogen (carrier) it forms HCl which is a volatile by-product that can be easily disposed of.
+ One of the simplest PVD methods called sputtering in which a sputtering target block made out of the metal to be deposited is held and a highly non-reactive Ar^+ ionic sputtering gas is directed onto the target by creating a potential difference, this causes bombardment of the ions onto the target and results in dislodging of parts of target material which are deposited onto the substrate.
*** Lithography
+ Stone + Write: Process of creating patterns on the Si wafer, analogous to stenciling. The ink is /light of a particular wavelength/, the stencil is a mask (quartz plate) and a resist (polymer that reacts with light).
+ The *mask* has opaque and transparent regions which are created by coating it with Chromium. In the transparent regions, the light falls over the Si substrate and interacts with the resist.
+ *Resist* can be of two kinds, the positive resist softens on interaction with light and the softened material can be removed by a particular solvent and the area unexposed to light stays intact whereas the negative resist hardens on interaction so the uninteracted material can be removed by the solvent.
+ After the pattern is created on the resist, it can be transferred over to the Si substrate either by additive or subtractive process and acetone removes the posres and all that's left is the deposited material (Al) in case of additive process whereas in the subtractive process a chemical etchent (KOH) is used to etch out the area not protected by the posres and acetone removes the resist.
+ Negative resist better for etching since hardening makes for stronger withold over removal process.
+ Diffraction Limit (Fresnel diffraction) limits the minimum feature size that can be achieved by lithography, Rayleigh limit. For smaller wavelength lights, the limit is smaller and vice-versa. (Why are 7nm gate sizes common?)
+ Epitaxy: Growing highly pure Si by using underlying Si crystal as substrate which reduces the large number of defects thereby improving mobility. When the underlying substrate and the material to be grown is the same (matching lattice structure), homoepitaxy is under play and for heteroepitaxy (HBT) the lattice structure aren't same (GeAs etc).
+ Through epitaxy, one can have a lightly doped layer over highly doped layer which is not possible with diffusion/ion implantation. It is achieved by MOCVD (Metal Organic CVD).
+ nMOS fabrication: Pure Si Crystal + (Si+Dopant) Melt \rightarrow Thick $SiO_2$ deposited over surface (FO) \rightarrow Deposit Photoresist (for pattern creation) \rightarrow Photoresist exposed to UV through mask \rightarrow Remove unpolymerised photoresist \rightarrow Etch $SiO_2$ via HF acid, then remove unpolymerised photoresist \rightarrow Add gate oxide then polysilicon via CVD \rightarrow Again coat with resist and and pass UV, then etch out unexposed area \rightarrow Remove resist and polysilicon gate is created \rightarrow Diffusion/Ion-implantation to form source and drain (Self-aligned process) \rightarrow Grow a thick layer of $SiO_{2}$ again for creating metal contact \rightarrow Photoresist  and masking, exposing, etching, photoresist removal \rightarrow Metal deposition \rightarrow Photoresist deposition (Removal of excess metal), mask-4, removal.
*** nMOS Inverter Fabrication
+ Wafer diameter: 200-300mm
+ Inverter:
  Start with wafer, p-type \rightarrow Grow $SiO_{2}$ via CVD (Thermal Oxidation) \rightarrow Create n-well (Masking, HF Etching, PR Removal via Piranah [$H_{2}O_{2}+H_{2}SO_{4}$], Diffusion/Ion-Implantation, Oxide Removal) \rightarrow Polysilicon Deposition and Gate formation (Self-align mask) \rightarrow Oxide patterning in active area(S, D, PS) \rightarrow  n-diffusion/implantation (also forms n+ region in the well for body contact) \rightarrow Oxide stripping \rightarrow Oxide deposition and patterning (for p-mos) \rightarrow p-diffusion/implantation \rightarrow MOS insulation (oxide deposition) \rightarrow Opening creation (Removal) \rightarrow Metal deposition
+ Shallow trench isolation : Etching and thick oxide deposition to prevent MOS interaction
+ In place of $SiO_2$, high-k dielectrics are being used for their high-epsilon values.
+ Layout Design (VLSI Design Flow): Functionality (VHDL) \rightarrow Transform functional description into circuit \rightarrow Take area and time constraints into account to estimate parasitics \rightarrow Stick Diagram Layout \rightarrow Mask layout Design \rightarrow DRC Check (Design rules) \rightarrow Extract parasitics from circuit \rightarrow Simulation \rightarrow Fabrication
+ Device Parasitics: $C_{DB}$, $C_{GD}$
+ Extrinsic Parasitics: Due to interconnects
+ Design rules: Lambda based for scaling portability. Min. contact: 2\lambda, Contact to active spacing: \lambda, Contact to poly-spacing: 2\lambda, n-well to active n-mos area: 9\lambda, n-well to active overlap: 5\lambda.
+ Stick Diagram: Combination of edges and nodes. Needed for sharing S&D to reduce area via Euler's theorem.
+ Segregation coefficient:  Concentration of dopants in ingot / Concentration of dopants in liquid form; Useful in determining concentration of final wafer. $k_{d}=\frac{C_{s}}{C_{l}}$
*** Fabrication Layout Design
+ Micron rules: Specify absolute value of parameters, since not all dimensions scale linearly below 1um.
+ Stick Diagram: Combination of edges (transistor) and nodes (interconnection). Needed for orientation by defining sharing of S&D to reduce area and parasitic capacitances via Euler's theorem.
+ Design Rules:
  1. Minimum Width: Lithography, diffraction limit
  2. Minimum Spacing: To prevent problems due to misalignment
  3. Minimum Enclosure: To prevent problems due to misalignment
  4. Minimum Extension: To prevent polysilicon misalignment problems
+ Euler's graph:
  1. Generate p-net and n-net.
  2. Find eulerian path, where a node can be traversed atmost twice but an edge only once.
  3. Check if the polysilicon path generated can be used on the n-net.
+ In mos, source and drain are interchangable, which is not possible in bjt, hence mos allows for smaller footprint.
+ Analog layout techniques: The aim is to minimize offset and have high CMRR, (i.e. mos M_1 and M_2 are matched so low noise) which determine the minimum input signal that can be detected.
  1. $R_{g}<<\frac{1}{g_{m}}$
  2. To reduce this resistance, folder topology was introduced, two poly lines connected together represented a large L even though it wasn't actually large, hence W/L decreases.
  3. Sometimes folder topology can result in some skewed layouts, so multi-fingered topology was introduced, where /n/ poly lines are connected together instead of just two.
  4. By splitting the poly, it's resistance decreases but the capacitance associated with S/D perimeter increases.
  5. For odd fingers, S/D perimeter capacitance,  \(C_{p}=\frac{N+1}{2}(2E+\frac{2W}{N})C_{jsw}\) (Side-wall/Fringe Capacitance)
  6. Matching: Since fabrication is not isotropic, orientation of polysilicon needs to be the same throughout, even interconnects need to be of the same length.
  7. Gate shadowing effect: Diffusion is not done vertically, there's an tilt of 7 degrees to avoid channeling (dopants penetrating deeper than needed through lattice spacing). This tilt causes asymmetry in source and drain diffusion extensions.
  8. Dummy transistors: To avoid neighbour asymmetries (coupling) but since it causes an increase in area, it's not advised.
*** Layout Techniques
+ Interdigitated: Linear technique, alternate fingers of the two transistors but it still has mismatched envvironments. Useful when a treshold of mismatch is allowed.
+ Common Centroid: Place transistors such that transistors can either be placed in 1 or 2 directions.
+ Takes care of processing and surrounding errors.
+ Parasitics tells us about the speed of the propogating signal.
+ Device Parasitics: $C_{sb}$, depletion region; Can be reduced by junction sharing (Euler's graph, S/D Sharing). But as we increase the number of fingers, the overlap capacitance increases.
*** Parasitics
+ Major reasons for delay:
  1. Internal parasitic
  2. Interconnect Parasitic
  3. Input capacitance of fan-out gates
+ Interconnect Capacitance: Model each interconnect as a Resistance and Capacitance combination.
+ Lumped RC Model: Model as a single RC combination
+ Distributed RC: Model as a combination of multiple RCs.
+ Transmission line model: Inductance is also introduced to account for magnetic coupling for long interconnects.
+ If $\tau_{rise}>t$, one can use lumped RC although even then distributed is preferred, but for $t>\tau_{rise}$ transmission model is preferred.
+ Due to scaling, gate delays are reducing but interconnect delays are increasing (chip size, and shrinking distance, fringing)
+ Inter module signals: Power ($V_{dd}$), ground, clock.
+ Intra module connections: Since they run over small distances, they can be modeled via lumped or distributed.
+ Yuan and Trick Interconnect Capacitance Estimation: Accounts for all fringing etc.
+ Interconnect resistance estimation:
+ Calculation of Interconnect delay: For simple lumped RC: \tau = 0.69RC, for distributed systems, we use elmore delay formula
+ Necessary conditions for elmore delay:
  1. One input node
  2. No loops
  3. All capacitors connected to the ground
+
*** Scaling and its effects
+ Process Issues:
  1. Shallow Trench: Signal coupling between transistors which can be avoided by increasing distance between them and adding $SiO_2$ between them. TCE (Temp. Coeff of Expansion) of Si and SiO_2 is different and on different expansion, due to stress mobility and therefore I-V characteristics changes, Can be avoided by using dummy fingers.
  2. Well Proximity: Can be avoided using dummy.
  3. Latchup:

** Combinatorial Mathematics :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: combi-math
:END:
*** General Counting Methods for Selection and Arrangement
1. Addition Principle:
   If there are $r_1$ different objects in the first set, $r_2$ different objects in the second set, . . . , and $r_m$ different objects in the m^th set, and if the different sets are disjoint, then the number of ways to select an object from one of the m sets is $r_1 +r_2 + · · · +r_m$.
2. Multiplication Principle:
   Suppose a procedure can be broken into m successive (ordered) stages, with $r_1$ different outcomes in the first stage, $r_2$ different outcomes in the second stage,. . . ,and $r_m$ different outcomes in the mth stage. If the number of outcomes at each stage is independent of the choices in previous stages and if the composite outcomes are all distinct, then the total procedure has $r_{1} ×r_{2} × · · · ×r_{m}$ different composite outcomes.
3. Remember that the addition principle requires disjoint sets of objects and the multiplication principle requires that the procedure break into ordered stages and that the composite outcomes be distinct.
4. A permutation of n distinct objects is an arrangement, or ordering, of the n objects. An r-permutation of n distinct objects is an arrangement using r of the n objects.
5. An r-combination of n distinct objects is an unordered selection, or subset, of r out of the n objects.
6. *Theorem 1:* If there are n objects, with $r_1$ of type 1, $r_2$ of type 2, . . . , and $r_m$ of type m, where $r_1 +r_2 + · · · +r_m = n$, then the number of arrangements of these n objects, denoted $P(n; r_{1}, r_{2}, . . . , r_{m})$, is
   \[
        P(n;r_{1},r_{2}, . . . ,r_{m}) = \frac{n!}{r_{1}!r_{2}! . . .r_{m}!}
   \]
7. *Theorem 2:* The number of selections with repetition of r objects chosen from n types of objects is C(r + n − 1,r).
8. Distributions of /distinct objects/ are equivalent to *arrangements* and Distributions of /identical objects/ are equivalent to *selections*.
9. Ways to arrange, select, distribute /r/ objects from /n/ items or into /n/ boxes:
   | Repition   | Arrangement                   | Combination |
   |------------+-------------------------------+-------------|
   | No         | P(n,r)                        | C(n,r)      |
   | Unlimited  | n^r                           | C(n+r-1, r) |
   | Restricted | P(n; r_{1}, r_{2}, .., r_{m}) | -           |
10. Equations with integer-valued variables are called /diophantine/ equations.
11. Equivalent forms of selection with repetition:
    + Number of ways to select /r/ objects with repetition from /n/ different types of objects.
    + Number of ways to distribute /r/ identical objects into /n/ distinct boxes.
    + Number of non-negative integer solutions to $x_1 + x_2 + ... + x_n = r$
12. *Binomial Theorem*:
    \[
        (1+x)^n = C(n,0) + C(n,1)x + C(n,2)x^2 + ... + C(n,k)x^k + C(n,n)x^n
     \]
13. Committee Selection Model: Represent C(n,k) committees of /k/ people chosen from a set of /n/ people.
14. Block Walking Model: Using Pascal's triangle, label each street corner in the network with the pair (n,k) where /n/ is the number of blocks traversed from (0,0) and /k/ is the number of times the person chose the right branch at intersections.

*** Generating Functions
1. Assuming /a_{r}/ denotes the /number of ways to select r objects/ in a certain procedure, g(x) is a generating function for a_{r} if g(x) has the polynomial expansion:
   \[
        g(x) = a_0 + a_1 x + a_2 x^2 + ... + a_r x^r + a_n x^n
   \]
2. \[
        \frac{1-x^{m+1}}{1-x} = 1+x+x^{2}+...+x^{m}
   \]
3. \[
        \frac{1}{1-x} = 1+x+x^{2}+...
   \]
4. \[
        (1+x)^n = 1 + {n \choose 1} x + {n \choose 2} x^2 + ... + {n \choose r} x^r + ... + {n \choose n} x^n
   \]
5. \[
        (1-x^{m})^n = 1 - {n \choose 1} x^m + {n \choose 2} x^{2m} + ... + (-1)^k {n \choose k} x^{km} + ... + (-1)^r {n \choose n} x^{nm}
   \]
6. \[
        \frac{1}{(1-x)^n} = 1 + {1+n-1 \choose 1} x + {2+n-1 \choose 2} x^2 + ... + {r+n-1 \choose r} x^r + ...
   \]
7. If h(x) = f(x)g(x) where \(f(x) = a_0 + a_1 x + a_2 x^2 + ...\) and \(g(x) = b_0 + b_1 x + b_2 x^2 + ...\) then:
   \[
        h(x) = a_{0}b_{0}+(a_{1}b_{0}+a_{0}b_{1})x + ... + (a_{r}b_{0}+a_{r-1}b_{1}+a_{r-2}b_{2}+ ... +a_{0}b_{r}) x^r + ...
   \]
8. Partition
9. Exp Gen Fn
10. Summation Method
