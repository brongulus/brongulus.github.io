:CONFIG:
#+hugo_base_dir: ../
#+seq_todo: TODO DRAFT DONE
#+options: creator:t
#+macro: updatetime {{{time(%B %e\, %Y)}}}
#+startup: inlineimages
#+startup: logdone
#+startup: latexpreview
:END:

#+title: Blog Source File
#+author: Prashant Tak

* TODO
1. Fix Time
2. Add Sections
3. Link to RSS feed
4. Add dark mode toggle
5. +Look into whether latex can be used and generated properly for displaying notes+ Improve mathjax performance
6. Create Tufte Theme for hugo that ACTUALLY works based on tufte css and the current tale theme
7. +Replace with material scroll bar+
8. Open external links in new tab by default, unless it's meta-links.
9. Improve Image filter sigh
* Doubts
1. What is r?
2. dev-corpus refers to?
3. F1 score? Measure of accuracy
4. xpos?

* About
:PROPERTIES:
  :EXPORT_FILE_NAME: about
:EXPORT_HUGO_SECTION: about
:END:

Hi! I'm Prashant.

* Blog
:PROPERTIES:
:EXPORT_HUGO_SECTION: blog
:END:

** Morphosyntactic Tagging with a Meta-BiLSTM Model - An Overview
:PROPERTIES:
:EXPORT_FILE_NAME: nnfl-paper
:EXPORT_AUTHOR: Bernd Bohnet, et al
:END:
(Subtitle: /I had shingles, which is a painful disease./)
[[file:assets/machine_learning.png]]

This post contains a complete overview of the titled paper and provides a basic outline of related concepts. This paper aims to investigate to what extent having initial sub-word and word context insensitive representations affect performance.

*** Abstract
1. RNN leads to advances in speech tagging accuracy [[https://www.aclweb.org/anthology/K18-2001.pdf][Zeman et al]]
2. Common thing among models, /rich initial word encodings/.
3. Encodings are composed of recurrent character-based representation with learned and pre-trained word embeddings[fn:10].
4. Problem with the encodings, context restriced to a single word hence only via subsequent recurrent layers the word information is processed.
5. The paper deals with models that use RNN with sentence-level context.
6. This provides results via synchronized training with a meta-model that learns to combine their states.
7. Results are provided on part-of-speech and morphological tagging[fn:1] with great performance on a number of languages.
*** Terms
1. Morphosyntactic = Morphology + Syntax and Morphology is study of words, how they are formed, and their relationship to other words in the same language.
2. [[https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577][RNN]]: [[https://arxiv.org/pdf/1211.5063.pdf][On difficulty of training RNNs]]
3. [[http://colah.github.io/posts/2015-08-Understanding-LSTMs/][LSTM]]: Long Short-Term Memory is a type of RNN that addresses the vanishing gradient problem through additional cells, input and output gates.
4. BiLSTM: It is a sequence processing model that consists of two LSTMs. They effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).
*** [[https://www.kdnuggets.com/2018/06/getting-started-natural-language-processing.html][Basics of NLP]] / Pre-requisites
**** Key Terms
1. *NLP*: Natural Language Processing concerns itself with interaction of technology with human languages.
2. *Tokenization*: An early step in the NLP process which splits longer strings of text into smaller pieces, or /tokens/.
3. *Normalization*: A series of tasks meant to put all text on a level playing field i.e. converting it to lowercase, removing punctuation, expanding contractions, converting numbers to their word equivalents, stripping white space, removing stop words and so on.
   + *Stemming*: Process of eliminating affixes (suffixes, prefixes, infixes, circumfixes) from a word to obtain its stem. For example, /running/ becomes /run/.
   + *Lemmatization*: It's related to stemming but is able to capture canonical forms based on the word's lemma (root form). For example, /better/ would turn into /good/.
4. *Corpus*: The latin word for /body/ refers to a collection of texts which may be formed of a single language of texts, or multiple. They are generally used for statistical linguistic analysis and hypothesis testing.
5. *Stop words*: Filter words which contribute little to the overall meaning of text since they are the very common words of the language. For example: /the/, /a/ etc.
6. *Parts-of-speech (POS) Tagging*: It consists of assigning a category tag to the tokenized parts of a sentence such as nouns, verbs, adjectives etc. The category of words is distinguished since they share similar grammatical properties.
7. *Statistical Language Modeling*: It's the process of building a model which takes /words/ as input and assign probabilities to the various sequences that can be formed using them.
8. *Bag of words*: It's a representation model used to simplify the contents of a selection of text by just reducing the words to their frequency.
9. *n-gram*: It focuses on preserving contagious sequences of N items from the text selection.
**** A framework for NLP
1. *Data Collection or Assembly*: Building the corpus
2. *Data Preprocessing*: Perform operations on the collected corpus which consists of tokenization, normalization, substitution (noise removal).
3. *Data Exploration & Visualization*: Includes visualizing word counts and distributions, generating wordclouds, performing distance measures.
4. *Model Building*: Choosing the language models (FSM, MM), classifiers and sequence models (RNNs, LSTMs).
5. *Model Evaluation*
**** Data Representation
1. We need to encode text in a way that can be controlled by us using a statistical classifier.
2. We go from a set of categorical features in text: words, letters, POS tags, word arrangement, order etc to a series of /vectors/.
3. *One-hot Encoding* (Sparse Vectors) :
   + Each word, or token corresponds to a vector element.
   + Result of one-hot encoding is a sparse matrix, that is, for a corpus containing a lot of tokens, representing a small subset of them would lead to a lot of zero vectors which would consume a large amount of memory.
   + One more drawback is that while it contains the information regarding the presence of a certain word, it lacks positional information so making sense of the tokens is not an option. For example, /Kate hates Alex/ is the same as /Alex hates Kate/.
   + Variants of one-hot encoding are /bag-of-words/, /n-gram/ and /TF-IDF/ representations.
4. *Dense Embedding Vectors*:
   + The information of the semantic relationship between tokens can be conveyed using manual or learned POS tagging that determines which tokens in a text perform what type of function. (noun, verb, adverb, etc)
   + This is useful for /named entity recognition/, i.e. our search is restricted to just the nouns.
   + But if one represents /features/[fn:2] as dense vectors i.e. with core features embedded into an embedding space of size /d/ dimensions, we can compress the number of dimensions used to represent a large corpus into a manageable amount.
   + Here, each feature no longer has its own dimension but is rather mapped to a vector.
**** [[http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf][Word Representation]]
**** [[https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79#:~:text=Subword%2Dmodels%3A%20Byte%20Pair%20Encodings%20and%20friends,-2.1%20Byte%20pair&text=Byte%20pair%20encoding%20(BPE)%20is,pairs%20into%20a%20new%20byte.&text=BPE%20is%20a%20word%20segmentation,(Unicode)%20characters%20in%20data.][Subword models]]
1. *Purely Character-level models*: In character-level modes, word embeddings[fn:3] can be composed of character embeddings which have several advantages. /Character-level/ models are needed because:
   + Languages like Chinese don't have /word segmentations/.
   + For languages that do have, they segment in different ways.
   + To handle large, open, informal vocabulary.
   + Character level model can generate embeddings for /unknown/ words.
   + Similar spellings share similar embeddings
2. *Subword-models*: TBD???
*** Morphology
It is a section of grammar whose main objects are *words* of languages, their /significant parts/ and /morphological signs/. Morphology studies:
+ Inflection
+ Derivation
+ POS
+ Grammatical values
**** Grammatical Value

*** Introduction
Morphosyntactic tagging accuracy has improved due to using BiLSTMs to create /sentence-level context sensitive encodings/[fn:4] of words which is done by creating an initial context insensitive word representation[fn:5] having three parts:
1. A dynamically trained word embedding
2. A fixed pre-trained word-embedding, induced from a large corpus
3. A sub-word character model, which is the final state of a RNN model that ingests one character at a time.
In such a model, sub-word character-based representations only interact via subsequent recurrent layers. To elaborate, context insensitive representations would normalize words that shouldn't be, but due to the subsequent BiLSTM layer, this would be overridden. This behaviour differs from traditional linear models.[fn:6]

This paper aims to investigate to what extent having initial subword and word context insensitive representations affect performance. It proposes a hybrid model based on three models- context sensitive initial character and word models and a meta-BiLSTM model which are all trained synchronously.

On testing this system on 2017 CoNLL data sets, largest gains were found for morphologically rich languages, such as in the Slavic family group. It was also benchmarked on English PTB(?) data, where it performed extremely well compared to the previous best system.
*** Related Work
1. An excellent example of an accurate linear model that uses both word and sub-word features.[fn:6] It uses context sensitive n-gram affix features.
2. First Modern NN for tagging which initially used only word embeddings[fn:7], was later extended to include suffix embeddings.[fn:8]
3. TBD TBD
4. This is the jumping point for current architectures for tagging models with RNNs.[fn:5]
5. Then [fn:4] showed that subword/word combination representation leads to state-of-the-art morphosyntactic tagging accuracy.
*** Models
**** Sentence-based Character Model
In this model, a BiLSTM is applied to all characters of a sentence to induce fully context sensitive initial word encodings. It uses sentences split into UTF8 characters as input, the spaces between the tokens are included and each character is mapped to a dynamically learned embedding. A forward LSTM reads the characters from left to right and a backward LSTM reads sentences from right to left.

#+CAPTION: Sentence-based Character Model: The representation for the token /shingles/ is the concatenation of the four shaded boxes.
[[file:assets/nnfl1a.png]]

For an /n/-character sentence, for each character embedding \((e_{1}^{char},...,e_{n}^{char})\), a BiLSTM is applied:
\[
f_{c,i}^{0},b_{c,i}^{0} = BiLSTM(r_{0},(e_{1}^{char},...,e_{n}^{char}))_{i}
\]
For multiple layers(/l/) that feed into each other through the concatenation of previous layer encodings, the last layer has both forward \((f_{c,l}^{l},...,f_{c,n}^{l})\) and backward \((b_{c,l}^{l},...,b_{c,n}^{l})\) output vectors for each character.

To create word encodings, relevant subsets of these context sensitive character encodings are combined which can then be used in a model that assigns morphosyntactic tags to each word directly or via subsequent layers. To accomplish this, the model concatenates upto four character output vectors: the {/forward, backward/} output of the {/first, last/} character in the token /T/ = \((F_{1st}(w), F_{last}(w), B_{1st}(w), B_{last}(w))\) which are represented by the four shaded box in /Fig. 1/.

Thus, the proposed model concatenates all four of these and passes it as input to an multilayer perceptron (MLP):
\[
g_{i} = concat(T)
\]
\[
m_{i}^{chars} = MLP(g_{i})
\]
A tag can then be predicted with a /linear classifier/ that takes as input \(m_{i}^{chars}\), applies a /softmax/ function and chooses for each word the tag with highest probability.
**** Word-based Character Model
To investigate whether a sentence sensitive character model (/Fig.1/) is better than a model where the context is restricted to the characters of a word, (/Fig.2/) which uses the final state of a unidirectional LSTM, combined with the attention mechanism of (ADD REF: cao rei) over all characters.

#+CAPTION: Word-based Character Model: The token is represented by concatenation of attention over the lightly shaded boxes with the final cell (dark box).
[[file:assets/nnfl1b.png]]

#+CAPTION: BiLSTM variant of Character-level word representation
[[file:assets/nnfl1.png]]

**** Sentence-based Word Model
The inputs are the words of the sentence and for each of the words, we use pre-trained word embeddings \((p_{1}^{word},...,p_{n}^{word})\) summed with a dynamically learned word embedding for each word in the corpus \((e_{1}^{word},...,e_{n}^{word})\):
\[
in_{i}^{word} = e_{i}^{word}+p_{i}^{word}
\]
The summed embeddings \(in_{i}\) are passed as input to one or more BiLSTM layers whose output \(f_{w,i}^{l}, b_{w,i}^{l}\) is concatenated and used as the final encoding, which is then passed to an MLP:
\[
o_{i}^{word} = concat(f_{w,i}^{l}, b_{w,i}^{l})
\]
\[
m_{i}^{word} = MLP(o_{i}^{word})
\]
The output of this BiLSTM is essentially the Word-based Character Model before tag prediction, with the exception that the word-based character encodings are excluded.

#+CAPTION: Tagging Architecture of Word-based Character Model and Sentence-based Word Model
[[file:assets/nnfl2a.png]]

**** Meta-BiLSTM: Model Combination
If each of the character or word-based encodings are trained with their own loss and are combined using an additional meta-BiLSTM model, optimal performance is obtained. The meta-biLSTM model concatenates the output of context sensitive character and word-based encoding for each word and puts this through another BiLSTM to create an /additional/ combined context sensitive encoding. This is followed by a final MLP whose output is passed to a linear layer for tag prediction.
\[
cw_{i} = concat(m_{i}^{char}, m_{i}^{word})
\]
\[
f_{m,i}^{l}, b_{m,i}^{l} = BiLSTM(r_{0},(cw_{0},...,cw_{n}))_{i}
\]
\[
m_{i}^{comb} = MLP(concat(f_{m,i}^{l}, b_{m,i}^{l}))
\]

#+CAPTION: Tagging Architecture of Meta-BiLSTM. Data flows along the arrows and the optimizers minimize the loss of the classifiers independently and backpropogate along the bold arrows.
[[file:assets/nnfl2b.png]]
**** Training Schema
Loss of each model is minimized independently by separate optimizers with their own hyperparameters which makes this a multi-task learning model and hence a schedule must be defined in which individual models are updated. In the proposed algorithm, during each epoch, each of the models are updated in sequence using the entire training data.

[[file:assets/nnflAlg.png]]

In terms of model selection, after each epoch, the algorithm evaluates the tagging accuracy of the development set and keeps the parameters of the best model. Accuracy is measured using the meta-BiLSTM tagging layer, which requires a forward pass through all three models. Only the meta-BiLSTM layer is used for model selection and test-time prediction.

The training is synchronous as the meta-BiLSTM model is trained in tandem with the two encoding models, and not after they have converged. When the meta-BiLSTM was allowed to back-propagate through the whole network, performance degraded regardless of the number of loss functions used. Each language could in theory used separate hyperparameters but identical settings for each language works well for large corpora.
*** Experiments and Results
**** Experimental Setup
The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training. The dropout[fn:9] used on the embeddings is achieved by a single dropout mask and dropout is used on the input and the states of the LSTM.

#+NAME: Architecture
| Model | Parameter                     | Value |
|-------+-------------------------------+-------|
| C,W   | BiLSTM Layers                 |     3 |
| M     | BiLSTM Layers                 |     1 |
| CWM   | BiLSTM size                   |   400 |
| CWM   | Dropout LSTM                  |  0.33 |
| CWM   | Dropout MLP                   |  0.33 |
| W     | Dropout Embeddings            |  0.33 |
| C     | Dropout Embedding             |   0.5 |
| CWM   | Nonlinear Activation Fn (MLP) |   ELU |

TODO Add two remaining tables
**** Data Sets
**** POS Tagging Results
**** POS Tagging on WSJ
**** Morphological Tagging Results
*** Ablation Study (Takeaways)
+ *Impact of the training schema*: Separate optimization better than Joint optimization
+ *Impact of the Sentence-based Character Model*: Higher accuracy than word-based character context
+ *Impact of the Meta-BiLSTM Model Combination*: Combined model has significantly higher accuracy than individual models
+ *Concatenation Strategies for the Context-Sensitive Character Encodings*: Model bases a token encoding on both forward and backward character representations of both first and last character in token. (/Fig. 1/) ....
+ *Sensitivity to Hyperparameter Search*: With larger network sizes, capacity of the network increases, but it becomes prone to overfitting. Future variants of this model might benefit from higer regularization.
+ *Discussion*: TODO Proposed modifications
*** Conclusions
*** Readings and Resources
1. Pytorch: [[https://pytorch.org/tutorials/beginner/nn_tutorial.html][Beginner Guide]], [[https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG][Detailed Guides]], [[https://www.cs.toronto.edu//~lczhang/360/][Notebook form]]
2. Math: [[https://explained.ai/matrix-calculus/index.html][Matrix Calculus]], [[https://mml-book.com/][Book]]
3. Basics:
   + [[https://www.kaggle.com/learn/python][Python]]
   + [[https://realpython.com/jupyter-notebook-introduction/#getting-up-and-running-with-jupyter-notebook][Jupyter]]
   + [[http://cs231n.github.io/python-numpy-tutorial/#numpy][Numpy]], [[https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb][Numpy 2]]
   + [[https://mlcourse.ai/articles/topic1-exploratory-data-analysis-with-pandas/][Pandas]], [[https://www.kaggle.com/learn/pandas][Pandas 2]]
   + [[https://mlcourse.ai/articles/topic2-visual-data-analysis-in-python/][Matplotlib]], [[https://matplotlib.org/matplotblog/posts/an-inquiry-into-matplotlib-figures/][Matplotlib 2]]
   + [[https://mlcourse.ai/articles/topic2-part2-seaborn-plotly/][Seaborn]]
   + [[http://scipy-lectures.org/][Overview]]
4. Interactive Tutorials on [[https://www.deeplearning.ai/ai-notes/initialization/][Weight Initialization]], [[https://www.deeplearning.ai/ai-notes/optimization/][Different Optimizers]]
5. Rougier's Bits
   + [[https://github.com/rougier/matplotlib-tutorial][Matplotlib Tutorial]], [[https://github.com/matplotlib/cheatsheets][Matplotlib Cheatsheets]]
   + [[https://github.com/rougier/numpy-tutorial][Numpy Tutorial]], [[https://www.labri.fr/perso/nrougier/from-python-to-numpy/][From Python to Numpy]], [[https://github.com/rougier/numpy-100][100 Numpy Exercises]]
   + [[https://www.labri.fr/perso/nrougier/python-opengl/][Python & OpenGL for Scientific Visualization]], [[https://github.com/rougier/scientific-visualization-book][Scientific Visualization]]
6. NLP: [[https://github.com/microsoft/nlp-recipes][Best Practices]], [[https://nlpoverview.com/][DL Techniques for NLP]]
7. BiLSTM: [[https://arxiv.org/pdf/1807.00818v1.pdf][Improving POS tagging]]
8. [[https://github.com/google/meta_tagger][Implementation]] of the paper
*** Specific to Paper
1. [[https://universaldependencies.org/guidelines.html][Universal Dependencies]]
2. [[https://lena-voita.github.io/nlp_course.html][Great Tutorial for NLP]]
3. [[https://github.com/Sdernal/Morphology/blob/master/README.md][Morphology]]
*** Footnotes
[fn:1] Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically. As compared to Part-of-speech tagging, morphological tagging also considers morphological features, such as case, gender or the tense of verbs.
[fn:2] They are the different categorical characteristic of the given data. For example, it could be /grammatical/ classes or some /physical/ features. It is context and result dependent. Then for each token, a weight is assigned to it with respect to each feature.
[fn:3] A word embedding is a learned representation for text where words that have the same meaning have a similar representation.
[fn:4] [[https://www.aclweb.org/anthology/K17-3002.pdf][Graph based Neural Dependency Parser]]
[fn:5] [[https://arxiv.org/pdf/1604.05529.pdf][POS Tagging with BiLSTM]]
[fn:6] [[http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=40AFFD632AC50016FE3B435B5C3FD50F?doi=10.1.1.4.7273&rep=rep1&type=pdf][*Fast POS Tagging: SVM Approach]]
[fn:7] [[http://machinelearning.org/archive/icml2008/papers/391.pdf][Unified architecture for NLP]]
[fn:8] [[https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf][NLP(almost) from Scratch]]
[fn:9] Dropping out units (hidden and visible) in a neural network, helps prevent the network from overfitting.
[fn:10] [[https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe][Everything about Embeddings]] Embedding converts symbolic representations into meaningful
** An introduction to the nix package manager :noexport:
:PROPERTIES:
:EXPORT_FILE_NAME: nix-intro
:END:
*** Why?
*** Benefits
*** Flakes
*** Overlays
*** Profiles
*** Multi User
*** Nix Shell
*** Home-manager
** My Pathway to learning Japanese
   :PROPERTIES:
   :EXPORT_FILE_NAME: japanese-guide
   :END:
*** JPod101
    Add a nice collection of their videos and cheatsheet materials since most sources don't really tell much about their actual content.
*** Hiragana
    + Why? All other basic textbooks have as requirement, romaji as a crutch that hurts later on
    + Tofugu Guide
    + Tae Kim Quiz
*** Katakana
    + Important thing to not delay learning it even when other sources say so.
    + TODO Add sources and more info
*** Kanji + Grammar + Vocabulary
**** Intro
     + Important step because after crossing the initial simple hurdle you're presented with a mammoth choice.
     + Genki - Mention ToKini
     + Grammar - Use genki before Tae Kim??
     + Kanji - Can I promote WaniKani Anki Deck???
     + Vocabulary
     + Readings - Tofugu article
**** Tango N5 deck
     | 彼   | Kare   | He   |
     | 彼女 | Kanojo | She  |
     | 名前 | Namae  | Name |
     |      |        |      |

*** Grammar Points
**** Verb Conjugation
     Start with misa's video then watch Andy's.
**** Particles (Do after conjugation!)
     + =は= (pronounced as wa): TOPIC MARKER- It marks the topic of a clause and creates focus. Translation: /as for, speaking of/. It can also be use as a CONTRASTING MARKER where =demo= acts as /but/ For example, I eat bread, *but* I *don't* eat butter. Here the topics bread and butter would be followed by the topic marker. =kore= means /this/ can can be just as a pointer for distinguishing an object
     + =ga=: SUBJECT PARTICLE- Since it denotes existence, it is generally used with =arimasu= and =imasu= (for animate things) meaning (/to have/ or /to exist/) There existence copulas can be negated by turning =su= to =sen=. It can be used as a DESIRE MARKER: `S + O + =ga= + Adjective of desire` for example =hoshii desu= which is /(I) want/ or `Stem of verb + =tai desu= ` which is /[I] want to [verb]/.
*** Pitch Accent
- Playlist
- My notes
- kotu.io

*** Bunpro
**** N5
***** Level 1
      1. da:
         + to be/is
         + Do not use with i-adj. (Cure Dolly video)
         + Noun+da
         + na-adj+da
         + Casual
      2. desu:
         + to be/is
         + Can be used with both na/i-adj
         + Noun/Adj + desu
         + Polite version of da
      3. wa(ha):
         + denotes the .... (use notes from text)
         + Sentence Topic + wa
         + Emphasizes what comes after it unlike ga.
         + ....
      4. mo:
         + also, too as well
         + Noun + mo
         + atashi mo. (me too)
         + Kore mo sensei desu. (He is also a teacher)
      5. kore:
         + this
         + kore wa penn desu.
         + kore mo hitsyoo desu. (This is also necessary)
      6. no:
         + indicates possession
         + Noun 1 + no + Noun 2
         + これも私のペンです。 (This is also my pen)
      7. ii:
         + adjective meaning "good"
         |          | Present | Past        |
         | Postive  | ii      | yokatta     |
         | Negative | yokunai | yokunakatta |
         + yoku also means "frequently/often" be careful
         + テストは、よくなかった。(Test was not good)
      8. ka:
         + question particle
         + Phrase + ka
         + 明日もいいですか。(ashita) (Is tomorrow also good?)
***** Level 2
      1. deshoo:
         + right? probably
         + Asking for confirmation
         + Noun/Verb/Adj + deshyoo
         + 明日も雨でしょう。 (ashita, ame) (It will also probably rain tomorrow)
      2. ga:
         + Subject marker identifier
         + Subject + ga
         + Denotes who or what performed the action, emphasizes what came before it
      3. sore:
         + that
         + それもいいです。(That is also good.)
      4. ru-verbs/Ichidan verbs:
         | Conjugation | Casual      | Polite           |
         | Present     | taberu      | tabemasu         |
         | Past        | tabeta      | tabmashita       |
         | Negative    | tabenai     | tabemasen        |
         | Neg. Past   | tabenakatta | tabemasendeshita |
      5. gaaru:
         + To be/ There is
         + Polite form: gaarimasu
         + Noun + gaaru
         + For non-living things
         + Ga is often omitted in casual speech
      6. koko:
         + here/ this place
         + (Near the speaker)
      7. to:
         + and/ with
         + Noun + to + Noun/Verb
         + When listing nouns, to: exhaustive, ya: non-exhaustive
      8. ~ ndesu.nodesu
         +

*** Anki Deck Sequence:
1. Hiragana Deck
2. Katakana Deck
3. Katakana Sentences
4. Kanji Deck: Either RTK order Allinone or WaniKani? or Kanji deck by [[https://sites.google.com/view/jo-mako/home][Jo Mako]]
5. Grammar Deck: Jo Mako
6. Vocabulary: Tango decks + JP1K + Kanji in context
** Creating a blog using ox-hugo, org mode and github pages
:PROPERTIES:
:EXPORT_FILE_NAME: blog-creation
:END:

I was going to make a post explaining how I made this blog but it was rendered pretty useless by [[https://dev.to/usamasubhani/setup-a-blog-with-hugo-and-github-pages-562n][this.]] So yeah, I might archive this later.

1. Install hugo from your package manager.
2. Create a new site:
   #+begin_src sh
hugo new site blog
   #+end_src
3. Add a theme:
   #+begin_src sh
cd blog
git init
git submodule add <theme_url> themes/<name>
   #+end_src
4. Install ox-hugo in emacs
   #+begin_src emacs-lisp
;; goes in packages.el
(package! ox-hugo)

;; goes in config.el
(use-package ox-hugo
  :after ox)
   #+end_src
5. TODO Explain the process of content and properties, tags etc.
6. Export
7. Config.toml (theme, title, url, publishdir, etc)
8. Run server, check localhost.
9. Push
10. Go to GitHub repository Settings > GitHub pages. Select /docs in Source.
11. Voila!
* Readings
* Resources
* Notes
:PROPERTIES:
:EXPORT_HUGO_SECTION: notes
:END:
** Differential Geometry
:PROPERTIES:
:EXPORT_FILE_NAME: dg-notes
:END:
*** Theory of Space Curves
**** Representation of space curves
+ Level Curve: f(x,y,z) = C
+ From level curves to parametrized curves:
  $y=x^{2} <-----> \gamma(t)=(\gamma_{1}(t),\gamma_{2}(t))$ Taking $\gamma_{1}(t)=t$, we get $\gamma_{2}(t)=t^{2}$ hence the parametrization is $\gamma(t)=(t,t^{2})$
+ *NOTE:* Check if domain of /x/ satisfies domain of /t/ or not. That is, the same parametrisation can be represented as $(t^{2}.t^{4})$ or $(t^{3},t^{6})$ but only the latter is a correct representation.
+ From parametrized curves to level curves:
  $\gamma(t)=(cos^{3}t,sin^{3}t)$ <------> F(x,y)=C; Using $sin^{2}t+cos^{2}t=1$ we get, $x^{2/3}+y^{2/3}=1$ as the level curve.
**** Unique Parametric representation
+ Class 'm' \rightarrow /f/ is m-differentiable
+ A curve is /smooth/ if $\frac{d^{n}f}{dt^{n}}$ exists for all n \geq 1 and t \in (\alpha,\beta)
+ A function /f/ is /analytic/ if it is single valued and of class \infty
+ A function is /regular/ if it is differentiable and derivative is non-zero (f dot \neq 0)
+ A /regular f/ of class /m/ can also be called a /*path*/ of class /m/.
+ *NOTE:* A point of a parametrized curve can have multiple tangents.
**** Arc-length
+ Arc-length of a curve \gamma is given by the function $s(t)=\int_{t_{0}}^{t}|| \dot{\gamma}(u)|| du$
+ Speed: \(|| \dot{\gamma}(t) ||_{t}\) and a curve is unit-speed curve if its magnitude is 1 for all /t/.
+ For \gamma being a unit speed curve, $\ddot{\gamma}$ is zero or perpendicular to $\dot{\gamma}$ i.e. $\ddot{\gamma}.\dot{\gamma}=0$
+ If \gamma is a regular curve, then its arclength S at any point of \gamma is a smooth function of t.
+ Reparametrization: $\overline{\gamma}:(\overline{\alpha},\overline{\beta}) \rightarrow R^{n}$ <=> $\gamma: (\alpha,\beta) \rightarrow R^{n}$  exists iff \exists a smooth function \phi: $(\overline{\alpha},\overline{\beta}) \rightarrow (\alpha,\beta)$ such that its inverse \phi^{-1} is also smooth.
+ A /unit speed reparametrization/ exists for a curve iff it is /regular/.
**** Tangent and Osculating Plane
+ Assuming \gamma is a class \geq 1 i.e. it has a power series expansion,
\[ \gamma(u)=\gamma(u_{0}+h)=\gamma(u_{0})+\frac{h}{1!}\dot{\gamma}(u_{0})+\frac{h^{2}}{2!}\ddot{\gamma}(u_{0})+ ... + \frac{h^{n}}{n!}\gamma^{n}(u_{0})+O(h^{n})
\]
  where $h = u-u_0$
+ Let \gamma be class m \geq 2 and (P,Q) be points limiting position of a plane that contains tangential line at P and passes through Q as Q \rightarrow P is defined as the /osculating plane/.
+ *Tangent line:* $\vec{R}(t)=\vec{r}(u_{0})+t \vec{r'}(u_{0})$ at $u_{0}$
+ *Osculating Plane:* $[\vec{R}-\vec{r(0)}, \vec{r'(0)}, \vec{r''(0)}]=0$ where $\vec{R}=(X,Y,Z)$ gives the equation of the OP (here $\vec{r''}(0)\neq0$). The product inside the box is /scalar triple product/. Also, the OP passes through the unit vector of the curve and is perpendicular to the unit binormal vector.
+ Note that for smallest k \geq 2 such that $\vec{r^{(k)}}=0$, the last term in the box is replaced by $\vec{r'}^{(k)}(0)$
**** Principal normal and binormal
+ *Normal Plane:* $\vec{t}(0).(\vec{R}-\vec{r}(0)) = 0$
  It is perpendicular to the tangent line and is spanned by /n,b/
+ *Principal Normal Vector:* For m \geq 1, $\vec{n}=\frac{\vec{r''}(0)}{||\vec{r''}(0)||}$
+ *Unit Binormal Vector:* $\vec{b}=\vec{t}\times\vec{n}$
+ OP: b.(R-r)
+ NP: t.(R-r)
+ RP: n.(R-r)
**** Curvature and Torsion
+ For a /unit speed curve/ or /arc length parametrized/ curve \gamma(t), the curvature \kappa(t) is defined as $||\ddot{\gamma}(t)||$ (1)
+ For a /regular/ curve \gamma(t) *in* $R^{3}$, $\kappa = \frac{||\ddot{\gamma}\times\dot{\gamma}||}{||\dot{\gamma}^{3}||}$
+ For a unit speed curve \gamma, /unit tangent vector/ $\hat{t}=\dot{\gamma}$ and for \kappa \neq 0, /unit normal vector/ is given by  $\hat{n}(s)=\frac{\dot{\hat{\gamma}}(s)}{\kappa(s)}$ since (1). And /unit binormal vector/ can be given by $\hat{b}=\hat{t}\times\hat{n}$
+ *Orthonormal Basis* of a curve is given by {$\hat{t},\hat{n},\hat{b}$}
+ Now b is given by t \times n , hence $\dot{b}=\dot{t}\times n+t\times\dot{n}$ , since $\dot{b}$ has to be perpendicular to t and b, $\implies \ddot{b}||n$, therefore $\boxed{\dot{b}=-\tau n}$ *iff* \kappa \neq 0.
+ Torsion measures the arc rate of turning of osculating plane.
+ For a regular curve \gamma in $R^{3}$ with \kappa \neq 0, the /torsion/ is given by
  \[
  \tau = \frac{(\dot{\gamma}\times\ddot{\gamma}).\dddot{\gamma}}{||\dot{\gamma}\times\ddot{\gamma}||^{2}}
  \]
+ Also, /radius of curvature/ \rho is inverse of curvature.
+ Finally, tying it all together is the /Serret-Frenet formula/ (arc length parameter):
  $\begin{bmatrix} \dot{t} \\
   \dot{n} \\
   \dot{b}  \end{bmatrix} = \begin{bmatrix} 0 & \kappa & 0 \\
    -\kappa & 0 & \tau \\
    0 & -\tau & 0 \end{bmatrix} \begin{bmatrix} t \\
    n \\
    b \end{bmatrix}$
**** Behaviour of a curve near one of its points
+ For a regular curve of class m \geq 2 with nonvanishing curvature, the curve is /planar/ iff \tau=0 everywhere.
+ For an analytic curve with arc length parameter, as s \rightarrow 0, a new parametrization for small s can be defined as:
  \[
    X = s - \frac{\kappa^{2}s^{3}}{6} - \frac{\kappa\kappa' s^{4}}{8} + o(s^{4})
  \]
  \[
    Y = \frac{\kappa s^{2}}{2} + \frac{\kappa' s^{3}}{6} + \frac{\kappa''-\kappa\tau-\kappa^{3}}{24} s^{4} + o(s^{4})
  \]
  \[
   Z = \frac{\kappa\tau}{6}s^{3} + \frac{2\kappa'\tau+\kappa\tau'}{24}s^{4} + o(s^{4})
  \]
+ Here the o notation represents that for f = o(g), as s \rightarrow 0, $lim \frac{f(s)}{g(s)}=0$
+ From previous theorem:
  1. $\kappa(0) = \lim_{s \to 0} \frac{2Y}{X^{2}}$
  2. $\tau(0) = \lim_{s \to 0} \frac{3Z}{XY}$
  3. For $P=\vec{r}(0), Q=\vec{r}(s)$, the length of chord
     \[
      PQ = s(1-\frac{\kappa^{2}s^{2}}{24}) + o(s^{3}) \~ s(1-\frac{\kappa^{2}s^{2}}{24})o(s^{3})
    \]
    If f(t)=g(t)+o(t), then as t \rightarrow 0, it can be written as f(t)~g(t)o(t)
+ The length of common perpendicular between tangents at two nearby points of $\vec{r}(s)$ at arcual distance /s/ is approximately $d=\frac{\kappa\tau s^{3}}{12}$. This is the shortest distance between tangents at nearby points of r(s).
**** Contact between curves and surface
+ For a surface S: F(x,y,z)=0 and a parametrized curve C: $\vec{r}(u)$ = (f(u),g(u),h(u)), let P be a point on C. P lies on S iff F(f(P),g(P),h(P))=0.
+ Let \phi(u) = F(f(u),g(u),h(u)) for any parameter value u. Then P lies on S iff \phi(u_{0})=0.
+ Assuming F and $\vec{r}$ are of class m for sufficiently large m, then \phi(u) has a taylor expansion where $\frac{O(h^{n+1})}{h^{n+1}}$ is bounded as h \rightarrow 0.
+ Definition: Surface S and a parametrized curve C has an /n-point contact/ (or contact of order n) at P if $\phi(u_{0}) = \phi'(u_{0}) = ... = \phi^{(n-1)}(u_{0}) = 0$ and $\phi^{(n)}(u_{0})\neq 0$
+ If S and C have a contact of order 1 at P then it is called a /simple intersection/ of S and C.
+ If P is in n-point contact of S and C, then S and C intersect at P in /n/ coincidental points.
+ Condition for /n-point contact/ at P is invariant under a change of parameter.
+ Osculating Plane at P of $\vec{r}$ has atleast a 3-point contact with $\vec{r}$ at P.
**** Osculating circle (circle of curvature)
+ For a regular curve $\vec{r}(s)$ of class m \geq 2, let $P=\vec{r}(0)$ and $P_{i}=\vec{r}(s_{i}), i=1,2,3$ be 3 non collinear points near P on the curve. Then there is a unique circle through all $P_{i}$. The limiting circle, if existent, for all $P_{i} \rightarrow P$ is called /osculating circle/ of r(s) at P.
+ Center of OC (c) is called /centre of curvature/ of r(s) at P while its radius \rho(0) is called radius of curvature. Also, the OC lies in the OP.
+ Theorem: $\rho(0)=\frac{1}{\kappa(0)}$, $\vec{c}(o)= \vec{r}(0)+\rho(0)\vec{n}(0)$
+ OC does not exist at points where curvature vanishes and OC of a circle is the same circle itself.
**** Osculating Sphere
+ Definition: For a regular path r(s) of class m \geq 2, assuming P = r(0) and \kappa(0)\tau(0) \neq 0, a sphere which has atleast a 4-point contact with r(s) at P is called /osculating sphere/ at P on r.
+ \rho(s)= $\frac{1}{\kappa(s)}$ is called radius of curvature and \sigma(s)= $\frac{1}{\tau(s)}$ is called radius of torsion of r(s)
+ Theorem: OS at P on r is given by $|\vec{c}-\vec{R}|^{2} = R^{2}$ where $R = \sqrt{\rho(0)^{2}+\sigma(0)^{2}\rho'(0)^{2}}$ and $\vec{c}=\vec{r}(0)+\rho(0)\vec{n}(0)+\sigma(0)\rho'(0)\vec{b}(0)$ where c and R are COSC and ROSC to r(s) at r(0)
+ Centre of OS lies in the normal plane of r(s) as $c-r(0)$ is a linear combination of n(0) and b(0)
+ If \kappa is constant then ROC=ROSC and COC=COSC. In particular, if r is a circle, then its its own OC and is a great circle of the OS.
**** Locus of centres of spherical curvature
+ Since COSC at r(s) is $c(s) =r(s)+\rho(s)n(s)+\sigma(s)\rho'(s)b(s)$, it moves along a path as /s/ varies. For this path, SFF, \kappa, \tau can be calculated and will be denoted with subscript c.
+ Assuming \tau(s)>0,
  1. $c'(s) = (\frac{\rho(s)}{\sigma(s)}+ \frac{d (\sigma(s)\rho'(s))}{ds})b(s)$
  2. For a regular c(s), unit tangent vector is $t_{c}(s) = eb(s)$
  3. $\frac{ds_{c}}{ds}=|\frac{\rho(s)}{\sigma(s)}+\frac{d(\sigma(s)\rho'(s))}{ds}|$
  Here e is 1 if ds_{c}/ds > 0, -1 ow. Also $e = t_{c}(s).b(s)$
+ Also on differentiating,
  1. $\kappa_{c}(s) = \frac{\tau(s)}{\frac{ds_{c}}{ds}}$ or \kappa(s)= $-\tau_{c}(s)e \frac{ds_{c}}{ds}$
  2. Which gives $\tau(s)\tau_{c}(s)=\kappa(s)\kappa_{c}(s)$
+ Theorem: ROC of center of curvatures (i.e. center of OCs) is given by
  \[
  \rho_{1} = [( \frac{\rho^{2}\sigma}{R^{3}}\frac{d}{ds}(\frac{\sigma\rho'}{\rho})-\frac{1}{R} )^{2} + \frac{\rho'^{2}\sigma^{4}}{\rho^{2}R^{4}}]^{-1/2}
  \]
**** Tangent surfaces, involutes and evolutes
+ Definition: Tangent surface to a curve r is union of all tangent lines to r at all its points.
+ Tangent line to r at r(s) is R(u,s) = r(s)+ur'(s)
+ For both varying r and u, one gets the tangent surface.
+ Image of the curve u=u(s) in us-plane gives a curve $r_{1}(s)=r(s)+u(s)r'(s)$
+ Definition: Involute of r is a curve on the tangent surface of r which meets all generating lines orthogonally at corresponding points.
+ If $r_{1}(s)$ denotes the pos vector on the involute C_1 of a curve C corresponding to its points r(s) then r_{1}(s)=r(s)+(c-s)t(s) for a constant c.
+ For an involute c(s) of a regular path r(s) of class m \geq 2.
  \[
    \kappa_{c}^2 = \frac{\tau^{2}+\kappa^{2}}{\kappa^{2}(c-s)^{2}}, \tau_c = \frac{\kappa\tau'-\kappa'\tau}{\kappa(c-s)(\tau^{2}+\kappa^{2})}
  \]

+ Definition: If $\overline{C}$ is an involute of C then C is called an evolute of $\overline{C}$.
+ For a regular curve r(s), evolute is given by $r_{1}(s)=r(s)+\rho(s)n(s)+\rho(s)cot(\psi(s)+c)b(s)$ where c is a constant and \psi(s) = $\int \tau(s)ds$
+ r(s) has infinitely many evolutes, as c is random constant. For a plane curve, \tau = 0.
+ Tangents to two different evolutes corresponding to two constans A and B drawn from the same point of the given curve are inclined to each other at a constant angle A-B.
  \[
    r_{1} = r+\rho\textbf{n}-\rho tan(\psi+a)\textbf{b}
  \]
  Further $\psi = \int \tau ds$ so that \psi'=\tau...
*** First Fundamental Form and Local Intrinsic Properties of a Surface
**** Introduction
+ The surfaces are defined similar to curves by an equation of the type F(/x,y,z/) = 0 or parametrically by expressing /x,y,z/ in terms of two parameters /u,v/ varying over a domain.
+ After defining the surface locally, its points are classified as ordinary or singular.
+ Then using tangent plane at a point and the surface normal at it, a coordinate system *\((r_1, r_2, N)\)* at every point of the surface is introduced.
+ After that, a certain quadratic differential form known as /first fundamental form/ on a surface and direction coefficients are introduced.
**** Definition of a Surface
*Definition 1:* Locus of a point P(/x,y,z/) in $E_{3}$ satisfying some restrictions on /x,y,z/ which is expressed by a relation of the type F(/x,y,z/) = 0.

This equation is called the /implicit/ or the /constraint/ equation of the surface which allows for a global study of the surface.

*Definition 2:* For parameters /u, v/ taking real values and varying over a domain D, a surface is defined /parametrically/ as
  \[
      x = f(u,v), y = g(u,v), z = h(u,v)
  \]
  where /f, g/ and /h/ are single valued continuous functions possessing continuous derivatives of /r/-th order. Such surfaces are called surfaces of class /r/.

Parametric representation is useful for local study of surfaces i.e. in the neighbourhood of a point which is a small region *but* it is not unique for a surface. Also, the parameters /u/ and /v/ are called /curvilinear coordinates/.

*Definition 3:* For two parametric representations /u, v/ and /u', v'/ of the same surface, any transformation of the form $u'=\phi(u,v)$ and $v'=\psi(u,v)$ relating the two representations is called a /parametric transformation/.

*Definition 4:* A parametric transformation is /proper/ if:
  1. \phi and \psi are single valued functions.
  2. The Jacobian $\frac{\delta (\phi,\psi)}{\delta (u,v)}\neq0$ in some domain D.
These conditions are necessary and sufficient for existence of inverse in the neighbourhood of any point in D' which is the domain of /u', v'/ corresponding to the domain D of the /u, v/ plane.
**** Nature of Points on a Surface
*Notation:* For *r* being the position vector of a point on the surface, *r* = (x,y,z), we can take r = r(u,v) as the parametric form of the surface and use $r_1 = \frac{\delta r}{\delta u} = (x_{1},y_{1},z_{1})$ and $r_2 = \frac{\delta r}{\delta v} = (x_{2},y_{2},z_{2})$, similarly we can denote second order derivatives using $r_{11}, r_{21}$ etc.

*Definition 1:* If $r_{1}\times r_{2}\neq0$ at a point on a surface, then the point is called an /ordinary/ point. A point which is not an ordinary point is called a /singularity/.

Remarks:
+ Considering M = $\begin{bmatrix} x_{1} & y_{1} & z_{1}\\
  x_{2} & y_{2} & z_{2}\end{bmatrix}$
  For $r_{1} \times r_{2} \neq 0$ at an ordinary point, i.e. rank of M is two at that point.
+ If the rank of M is either zero or one, the point on the surface is a singular point.
+ If $r_{1} \times r_{2}\neq0$ or equivalently rank of M is two, then /x,y,z/ uniquely determine the parameters /u,v/ in the neighbourhood of an ordinary point.
+ When only one determinant minor of M is zero, one cannot conclude that the point is a singular point.
+ A /proper/ parametric transformation transforms an ordinary point into an ordinary point.
+ Due to geometrical nature of the surface, some singularities continue to be singularities, regardless of the parametric representations, these are called /essential singularities/.
+ There are other singularities depending on the choice of parametric representation which are called /artificial singularities/.
*Example:* Consider the circular cone represented by /x = u sin\alpha cosv, y = u sin\alpha sinv, z = u cos\alpha/ where \alpha is the semivertical angle of cone with O as origin and OP = /u/, where P is any point on the cone.
Computing M, then at /u/ = 0, the determinant of every second order minor is zero, hence it is an essential singularity.

*Example:* Taking any point 0 as origin in the plane, /x = u cosv, y = u sinv, z = 0/, we get $r_{1} \times r_{2} = u\textbf{k}$. Hence it is zero only when /u/ = 0 i.e. it is an artificial singularity /since/ it arises due to the choice of the parametric coordinates and not due to the nature of the surface.
**** Representation of a Surface
For our study of surfaces, we consider only ordinary points. And we consider the entire surface as a collection of parts, each part being given a particular parametrisation and the adjacent parts being related by a /proper/ parametric transformation.

*Definition 1:* A representation R of a surface S of class /r/ in $E_{3}$ is a collection of points in $E_{3}$ covered by a system of overlapping parts ${S_{j}}$ where each part {{$S_{j}$} is given by a parametric equation of class /r/. Each point lying in the common portion of two parts $S_{i}, S_{j}$ is such that the change of parameters from one part to is adjacent is given by a /proper/ parametric transformation of class /r/.

*/Note:/* Since one cannot parameterise the whole surface without introducing artificial singularities, one has to resort to a surface composed of many overlapping parts.

It is possible to have many representations of the same surface by considering different systems of overlapping parts ($S_{j}$), each part is given by a parametric equation of class /r/.

*Definition 2:* For R and R' being two representations of class /r/ of the surface S, they are /equivalent/ if the composite family of parts {$S_{j},S'_{j}$} satisfies the condition that for each point P lying in the place of overlap, the change of parameter from $S_{j}$  to $S'_{j}$ at P is given by a proper parametric transformation of class r.

*Theorem:* The notion of /r/-equivalence of representations of a surface is an equivalence relation.

This equivalence relation introduces a partition into the family of surfaces of class /r/ splitting them into mutually disjoint equivalence classes, each class containing the surface equivalent to one another in the above equivalence relation.

*Definition 3:* A surface S of class /r/ in $E_{3}$ is an /r/-equivalence class of representations.

Thus a surface consists of different overlapping portions related to one another by proper parametric transformations and all other surfaces related to the given one by the equivalence relation of class /r/.
**** Curves on Surfaces
For a surface *r* = r(/u,v/), let /u = u(t)/ and /v = v(t)/ be a curve of class /s/ lying in the domain D of the /uv/-plane. Considering *r* = r[u(t), v(t)] which gives the position vector of a point in terms of a single parameter /t/ such that it is a curve lying on a surface with class equal to the smaller of /r/ and /s/. The equation /u = u(t)/ and /v = v(t)/ are called /curvilinear equations/ of the curve on the surface.

*Definition 1:* For *r*, a given surface of class /r/, let /v = c/, then position vector *r* = r(u,c) is a function of a single parameter /t/ and hence *r* = r(u,c) represents a curve lying on the surface *r* = r(u,v). This curve is called the /parametric curve/ v = constant.

By varying the values of /c/, a system of parametric curves /v/ = constant is generated and similarly another system is generated by keeping /u/ constant and varying /v/.

Properties that are a consequence of assuming only ordinary points on the surface:
1. Through every point of the surface, there passes one and only one parametric curve of each system.
2. No two curves of the same system intersect.
3. The curves of the system $u=u_{o}$ and $v=v_{o}$ intersect once but not more than once if $(u_{o},v_{o}) \in D$.
4. The parametric curves of the system u = $c_{1}$ and v = $c_{2}$ cannot touch each other.

*Definition 2:* Let u = $c_{1}$ and v = $c_{2}$, when the constants vary, the whole surface is covered with a net of parametric curves, two of which pass through each point.

*Definition 3:* Two parametric curves through a point P are /othogonal/ if $\textbf{r}_{1}.\textbf{r}_{2}= 0$ at P.
**** Tangent Plane and Surface Normal
Let *r* = r[u(t), v(t)] be a general curve lying on the surface passing through [u(t), v(t)], then the tangent to the curve at any point P on the surface is
\[
\frac{dr}{dt} = r_{1}\frac{du}{dt}+r_{2}\frac{dv}{dt}
\]
*Definition 1:* Tangent to any curve drawn on a surface is called a tangent line to the surface. The tangents to different curves through P on a surface lie in a plane containing two independent vectors $r_{1}$ and $r_{2}$ at P called the /tangent plane/ at P.

*Theorem 1:* The equation of a tangent plane at P on a surface with position vector *r* = r(u,v) is either \(R = r+ar_{1}+br_{2}\) or \((R-r).(r_{1}\times r_{2}) = 0\) where a and b are parameters.

*Definition 2:* The normal to the surface P is a line through P and perpendicular to the tangent plane at P.

*Theorem 2:* The equation of the normal *N* at a point P on the surface r = r(u,v) is \(R=r+a(r_{1}\times r_{2})\).

*Theorem 3:* A proper parametric transformation either leaves every normal unchanged or reverses the direction of the normal.
**** General Surface of Revolution
*Definiton 1:* A surface generated by the rotation of a plane curve about an axis in its plane is called a /surface of revolution/.

*Theorem 1:* The position vector of any point on the surface of revolution generated by the curve [g(u),o,f(u)] in the XOZ plane is
\[
\textbf{r} = [g(u)cosv, g(u)sinv, f(u)]
\]
where /v/ is the angle of roatation about the /z/-axis.

** Analog and Digital VLSI Design
:PROPERTIES:
:EXPORT_FILE_NAME: advd-notes
:END:
*** Radio Spectrum
+ Used for communication initially
+ Wireless communication
+ Radio Spectrum is divided into frequency bands which are allocated to certain services.
+ The band is subdivided into channels that are used for particular transmission.
+ The wider the frequency bands and the channel, the more information that can be passed through them.
  | Frequency             | Use                        |
  |-----------------------+----------------------------|
  | VLF                   | Maritime Navigation        |
  | LF                    | Maritime Navigation        |
  | MF                    | AM Radio                   |
  | HF                    | Shortwave Radio            |
  | VHF (30-300 MHz)      | TV, FM Radio               |
  | UHF (300 MHz - 3 GHz) | TV, Mobile, GPS, Wi-FI, 4G |
  | SHF                   | Satellite                  |
  | EHF                   | Radio Astronomy            |
+ LF Bands provide wider coverage due to *high penetration power* but they have *poor capacity* (carry less information).
+ HF bands have greater capacity but less wider coverage.
+ Cell phones are multi-band device, when one's closer to a radio tower/station, it uses HF bands, but at poor reception they fall back to LF bands (GSM: 900-1800 MHz).
+ Wireless networks cover large amounts of area via a number of low-power radio stations laid out in hexagonal, cell-like grids.
+ Cellular commuication works by transmitting analog voice/data after amplification and conversion to digital bits into the environment and then received by selecting the corresponding frequency (highly selective network), processing the data (noise removal etc) and then converting back to analog audio. This process is know as modulation-demodulation.
[[file:assets/advd-rf-tran.png]]
+ Elements of a transceiver: /Oscillators, phase-clocked loops, frequency synthesizers, converters, filters, power circuits/ having *high data rate, resolution, less cost and energy per conversion*.
*** FIXME VLSI Design - An Overview
+ *Moore's Law:* Number of components (transistors) in ICs would double every two years. This was possible because of /scaling/.
[[file:assets/advd-moore-law.png]]
+ Learn how to convert schematic into a layout and vice-versa.
+ First microprocessor from intel - 4004, 8 bit
+ FPGA: Customizable pre-fabricated design
+ VLSI Design Styles

*** Fabrication
+ Sequence of steps that are followed to get a silicon chip with different patterns
+ Clean room: Class 1 = 1 dust particle in 1 ft^{3}
+ VLSI Design flow:
  Functional Description (Verilog) \rightarrow Circuit Design \rightarrow Layout \rightarrow Masks (Patterns)
+ Twin-tub process: For p-mos, there's an n-well and vice versa.
+ [Simplified-CMOS-Process.jpg]
+ CVD: Growing Field Oxide and gate oxide
+ Lithography: Process of patterning the silicon
+ Why Si over Ge? Band gap Si>Ge, Ge can't be used in mass production due to lack of raw material also SiO_2 is highly stable whereas GeO is soluble in water.
+ Getting that wafer:
  Sand \rightarrow SiO_2 \rightarrow Metallurgical Grade Si (99.9% Pure) \rightarrow CZ Chamber (1000^\odot C) \rightarrow Seed Crystal + Molten Si \rightarrow Si crystal ingot \rightarrow Diamond saw \rightarrow Polishing \rightarrow Silican Wafer
+ Dopants are introduced in the CZ chamber via /diffusion/ion implantation/, n-type: B (Pentavalent), p-type: P(Trivalent)
+ Diffusion:
  Temperature is around 650 C, Carrier made of quartz, Dopant in either crystal or powdered form, preheating temperature slightly lower than furnace, carrier gas carries the dopant vapours onto the silicon wafer by getting into the vacant sites of lattice defects and when they move from interstitional locations to lattice positions, doping is complete.
+ Fick's Law: Determines the amount of dopant required, diffusion temperature and the duration of the diffusion.
+ Ion Implantation:
  Source of the dopants are in ionic (charged) form, so an ion source releases a beam of ions which is columated by lenses to a small spot size called aperture, this accelerated beam of ions hits the silicon surface and the bombardment results in dislodging of Si atoms from the lattice, and the broken bonds are healed via /annealing/ (heating of wafer post-implantation).
+ Deposition:
  Used to deposit different materials from SiO_2 to metals, it can be achieved either chemically or physically. CVD is similar to diffusion whereas PVD is akin to ion implantation.
+ For metal deposition, generally MCl_2 are used since on reaction with hydrogen (carrier) it forms HCl which is a volatile by-product that can be easily disposed of.
+ One of the simplest PVD methods called sputtering in which a sputtering target block made out of the metal to be deposited is held and a highly non-reactive Ar^+ ionic sputtering gas is directed onto the target by creating a potential difference, this causes bombardment of the ions onto the target and results in dislodging of parts of target material which are deposited onto the substrate.
*** Lithography
+ Stone + Write: Process of creating patterns on the Si wafer, analogous to stenciling. The ink is /light of a particular wavelength/, the stencil is a mask (quartz plate) and a resist (polymer that reacts with light).
+ The *mask* has opaque and transparent regions which are created by coating it with Chromium. In the transparent regions, the light falls over the Si substrate and interacts with the resist.
+ *Resist* can be of two kinds, the positive resist softens on interaction with light and the softened material can be removed by a particular solvent and the area unexposed to light stays intact whereas the negative resist hardens on interaction so the uninteracted material can be removed by the solvent.
+ After the pattern is created on the resist, it can be transferred over to the Si substrate either by additive or subtractive process and acetone removes the posres and all that's left is the deposited material (Al) in case of additive process whereas in the subtractive process a chemical etchent (KOH) is used to etch out the area not protected by the posres and acetone removes the resist.
+ Diffraction Limit (Fresnel diffraction) limits the minimum feature size that can be achieved by lithography, Rayleigh limit. For smaller wavelength lights, the limit is smaller and vice-versa. (Why are 7nm gate sizes common?)
+ Epitaxy: Growing highly pure Si by using underlying Si crystal as substrate which reduces the large number of defects thereby improving mobility. When the underlying substrate and the material to be grown is the same (matching lattice structure), homoepitaxy is under play and for heteroepitaxy (HBT) the lattice structure aren't same (GeAs etc).
+ Through epitaxy, one can have a lightly doped layer over highly doped layer which is not possible with diffusion/ion implantation. It is achieved by MOCVD (Metal Oxide CVD).
+
