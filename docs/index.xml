<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bacchanalian madness</title>
    <link>https://brongulus.github.io/</link>
    <description>Recent content on bacchanalian madness</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://brongulus.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://brongulus.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://brongulus.github.io/about/about/</guid>
      <description>Hi! Take 1</description>
    </item>
    
    <item>
      <title>Creating a blog using ox-hugo, org mode and github pages</title>
      <link>https://brongulus.github.io/blog/blog-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://brongulus.github.io/blog/blog-creation/</guid>
      <description>Install hugo from your package manager. Create a new site:  hugo new site blog  Add a theme:  cd blog git init git submodule add &amp;lt;theme_url&amp;gt; themes/&amp;lt;name&amp;gt;  Install ox-hugo in emacs  ;; goes in packages.el (package! ox-hugo) ;; goes in config.el (use-package ox-hugo :after ox)  TODO Explain the process of content and properties, tags etc. Export Config.toml (theme, title, url, etc) Run server, check localhost.</description>
    </item>
    
    <item>
      <title>Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings - An Overview</title>
      <link>https://brongulus.github.io/blog/nnfl-paper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://brongulus.github.io/blog/nnfl-paper/</guid>
      <description>Abstract  RNN leads to advances in speech tagging accuracy Zeman et al Common thing among models, rich initial word encodings. (What does encoding, maybe related to RNN) Encodings are composed of recurrent character-based representation with learned and pre-trained word embeddings. (What are embeddings here) Problem with the encodings, context restriced to a single word hence only via subsequent recurrent layers the word information is processed. The paper deals with models that use RNN with sentence-level context.</description>
    </item>
    
  </channel>
</rss>
