<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2026 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Morphosyntactic Tagging with BiLSTM Model</title>
<meta name="author" content="Bernd Bohnet, et al" />
<meta name="generator" content="Org Mode" />
<script src='/css/head.js'></script> <link rel='stylesheet' type='text/css' href='/css/stylesheet.css'>

<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css' integrity='sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC' crossorigin='anonymous'>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js' integrity='sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja' crossorigin='anonymous'></script>
<script defer src='https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js' integrity='sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR' crossorigin='anonymous'></script>
<script defer>
document.addEventListener('DOMContentLoaded', function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
        ],
        throwOnError: false
    });
});
</script>
</head>
<body>
<div id="nav" class="status">
<a href='/'>bacchanalian madness</a><a href='/rss.xml'>
    <button id='rss-button' aria-label='Get RSS feed' type='button'>
      <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 448 512' fill='currentColor'>
        <path d='M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z'></path>
      </svg>
    </button>
  </a><button id='dark-mode-button' aria-label='Toggle Dark Mode' type='button'>
    <svg id='moon-icon' xmlns='http://www.w3.org/2000/svg' viewBox='0 0 20 20' fill='currentColor'>
      <path d='M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z'></path>
    </svg>
    <svg id='sun-icon' xmlns='http://www.w3.org/2000/svg' viewBox='0 0 20 20' fill='currentColor' style='display: none;'>
      <path d='M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z' fill-rule='evenodd' clip-rule='evenodd'></path>
    </svg>
  </button>
</div>
<div id="content" class="content">
<header>
<h1 class="title">Morphosyntactic Tagging with BiLSTM Model</h1>
</header><nav id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgf74a79b">Abstract</a></li>
<li><a href="#orge7d8f36">Terms</a></li>
<li><a href="#org6d76454">Basics of NLP</a></li>
<li><a href="#orgf9456ff">Morphology</a></li>
<li><a href="#orgdf31c0a">Introduction</a></li>
<li><a href="#org3647166">Related Work</a></li>
<li><a href="#org4a2d55b">Models</a></li>
<li><a href="#org5ee477b">Experiments and Results</a></li>
<li><a href="#orgaed8b2b">Ablation Study (Takeaways)</a></li>
<li><a href="#orga13a714">Conclusions</a></li>
<li><a href="#org1f7a6e6">Readings and Resources</a></li>
<li><a href="#org164f30d">Specific to Paper</a></li>
</ul>
</div>
</nav>
<blockquote>
<p>&ldquo;I had shingles, which is a painful disease.&rdquo;
</p>
</blockquote>


<figure id="org9d30536">
<img src="/assets/machine_learning.png" alt="machine_learning.png">

</figure>

<p>This post contains a complete overview of the titled paper and provides a basic outline of related concepts. This paper aims to investigate to what extent having initial sub-word and word context insensitive representations affect performance.
</p>
<div id="outline-container-orgf74a79b" class="outline-2">
<h2 id="orgf74a79b">Abstract</h2>
<div class="outline-text-2" id="text-orgf74a79b">
<ol class="org-ol">
<li>RNN leads to advances in speech tagging accuracy <a href="https://www.aclweb.org/anthology/K18-2001.pdf">Zeman et al</a></li>
<li>Common thing among models, <i>rich initial word encodings</i>.</li>
<li>Encodings are composed of recurrent character-based representation with learned and pre-trained word embeddings<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.</li>
<li>Problem with the encodings, context restriced to a single word hence only via subsequent recurrent layers the word information is processed.</li>
<li>The paper deals with models that use RNN with sentence-level context.</li>
<li>This provides results via synchronized training with a meta-model that learns to combine their states.</li>
<li>Results are provided on part-of-speech and morphological tagging<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> with great performance on a number of languages.</li>
</ol>
</div>
</div>
<div id="outline-container-orge7d8f36" class="outline-2">
<h2 id="orge7d8f36">Terms</h2>
<div class="outline-text-2" id="text-orge7d8f36">
<ol class="org-ol">
<li>Morphosyntactic = Morphology + Syntax and Morphology is study of words, how they are formed, and their relationship to other words in the same language.</li>
<li><a href="https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577">RNN</a>: <a href="https://arxiv.org/pdf/1211.5063.pdf">On difficulty of training RNNs</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a>: Long Short-Term Memory is a type of RNN that addresses the vanishing gradient problem through additional cells, input and output gates.</li>
<li>BiLSTM: It is a sequence processing model that consists of two LSTMs. They effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).</li>
</ol>
</div>
</div>
<div id="outline-container-org6d76454" class="outline-2">
<h2 id="org6d76454"><a href="https://www.kdnuggets.com/2018/06/getting-started-natural-language-processing.html">Basics of NLP</a></h2>
<div class="outline-text-2" id="text-org6d76454">
</div>
<div id="outline-container-org8df85ed" class="outline-3">
<h3 id="org8df85ed">Key Terms</h3>
<div class="outline-text-3" id="text-org8df85ed">
<ol class="org-ol">
<li><b>NLP</b>: Natural Language Processing concerns itself with interaction of technology with human languages.</li>
<li><b>Tokenization</b>: An early step in the NLP process which splits longer strings of text into smaller pieces, or <i>tokens</i>.</li>
<li><b>Normalization</b>: A series of tasks meant to put all text on a level playing field i.e. converting it to lowercase, removing punctuation, expanding contractions, converting numbers to their word equivalents, stripping white space, removing stop words and so on.
<ul class="org-ul">
<li><b>Stemming</b>: Process of eliminating affixes (suffixes, prefixes, infixes, circumfixes) from a word to obtain its stem. For example, <i>running</i> becomes <i>run</i>.</li>
<li><b>Lemmatization</b>: It&rsquo;s related to stemming but is able to capture canonical forms based on the word&rsquo;s lemma (root form). For example, <i>better</i> would turn into <i>good</i>.</li>
</ul></li>
<li><b>Corpus</b>: The latin word for <i>body</i> refers to a collection of texts which may be formed of a single language of texts, or multiple. They are generally used for statistical linguistic analysis and hypothesis testing.</li>
<li><b>Stop words</b>: Filter words which contribute little to the overall meaning of text since they are the very common words of the language. For example: <i>the</i>, <i>a</i> etc.</li>
<li><b>Parts-of-speech (POS) Tagging</b>: It consists of assigning a category tag to the tokenized parts of a sentence such as nouns, verbs, adjectives etc. The category of words is distinguished since they share similar grammatical properties.</li>
<li><b>Statistical Language Modeling</b>: It&rsquo;s the process of building a model which takes <i>words</i> as input and assign probabilities to the various sequences that can be formed using them.</li>
<li><b>Bag of words</b>: It&rsquo;s a representation model used to simplify the contents of a selection of text by just reducing the words to their frequency.</li>
<li><b>n-gram</b>: It focuses on preserving contagious sequences of N items from the text selection.</li>
</ol>
</div>
</div>
<div id="outline-container-org788b82b" class="outline-3">
<h3 id="org788b82b">A framework for NLP</h3>
<div class="outline-text-3" id="text-org788b82b">
<ol class="org-ol">
<li><b>Data Collection or Assembly</b>: Building the corpus</li>
<li><b>Data Preprocessing</b>: Perform operations on the collected corpus which consists of tokenization, normalization, substitution (noise removal).</li>
<li><b>Data Exploration &amp; Visualization</b>: Includes visualizing word counts and distributions, generating wordclouds, performing distance measures.</li>
<li><b>Model Building</b>: Choosing the language models (FSM, MM), classifiers and sequence models (RNNs, LSTMs).</li>
<li><b>Model Evaluation</b></li>
</ol>
</div>
</div>
<div id="outline-container-org1ee935b" class="outline-3">
<h3 id="org1ee935b">Data Representation</h3>
<div class="outline-text-3" id="text-org1ee935b">
<ol class="org-ol">
<li>We need to encode text in a way that can be controlled by us using a statistical classifier.</li>
<li>We go from a set of categorical features in text: words, letters, POS tags, word arrangement, order etc to a series of <i>vectors</i>.</li>
<li><b>One-hot Encoding</b> (Sparse Vectors) :
<ul class="org-ul">
<li>Each word, or token corresponds to a vector element.</li>
<li>Result of one-hot encoding is a sparse matrix, that is, for a corpus containing a lot of tokens, representing a small subset of them would lead to a lot of zero vectors which would consume a large amount of memory.</li>
<li>One more drawback is that while it contains the information regarding the presence of a certain word, it lacks positional information so making sense of the tokens is not an option. For example, <i>Kate hates Alex</i> is the same as <i>Alex hates Kate</i>.</li>
<li>Variants of one-hot encoding are <i>bag-of-words</i>, <i>n-gram</i> and <i>TF-IDF</i> representations.</li>
</ul></li>
<li><b>Dense Embedding Vectors</b>:
<ul class="org-ul">
<li>The information of the semantic relationship between tokens can be conveyed using manual or learned POS tagging that determines which tokens in a text perform what type of function. (noun, verb, adverb, etc)</li>
<li>This is useful for <i>named entity recognition</i>, i.e. our search is restricted to just the nouns.</li>
<li>But if one represents <i>features</i><sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup> as dense vectors i.e. with core features embedded into an embedding space of size <i>d</i> dimensions, we can compress the number of dimensions used to represent a large corpus into a manageable amount.</li>
<li>Here, each feature no longer has its own dimension but is rather mapped to a vector.</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgcebf75c" class="outline-3">
<h3 id="orgcebf75c"><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">Word Representation</a></h3>
</div>
<div id="outline-container-org62db422" class="outline-3">
<h3 id="org62db422"><a href="https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79#:~:text=Subword%2Dmodels%3A%20Byte%20Pair%20Encodings%20and%20friends,-2.1%20Byte%20pair&amp;text=Byte%20pair%20encoding%20(BPE)%20is,pairs%20into%20a%20new%20byte.&amp;text=BPE%20is%20a%20word%20segmentation,(Unicode)%20characters%20in%20data.">Subword models</a></h3>
<div class="outline-text-3" id="text-org62db422">
<ol class="org-ol">
<li><b>Purely Character-level models</b>: In character-level modes, word embeddings<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup> can be composed of character embeddings which have several advantages. <i>Character-level</i> models are needed because:
<ul class="org-ul">
<li>Languages like Chinese don&rsquo;t have <i>word segmentations</i>.</li>
<li>For languages that do have, they segment in different ways.</li>
<li>To handle large, open, informal vocabulary.</li>
<li>Character level model can generate embeddings for <i>unknown</i> words.</li>
<li>Similar spellings share similar embeddings</li>
</ul></li>
<li><b>Subword-models</b>: TBD???</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgf9456ff" class="outline-2">
<h2 id="orgf9456ff">Morphology</h2>
<div class="outline-text-2" id="text-orgf9456ff">
<p>It is a section of grammar whose main objects are <b>words</b> of languages, their <i>significant parts</i> and <i>morphological signs</i>. Morphology studies:
</p>
<ul class="org-ul">
<li>Inflection</li>
<li>Derivation</li>
<li>POS</li>
<li>Grammatical values</li>
</ul>
</div>
<div id="outline-container-orgb417dee" class="outline-3">
<h3 id="orgb417dee">Grammatical Value</h3>
</div>
</div>
<div id="outline-container-orgdf31c0a" class="outline-2">
<h2 id="orgdf31c0a">Introduction</h2>
<div class="outline-text-2" id="text-orgdf31c0a">
<p>Morphosyntactic tagging accuracy has improved due to using BiLSTMs to create <i>sentence-level context sensitive encodings</i><sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup> of words which is done by creating an initial context insensitive word representation<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup> having three parts:
</p>
<ol class="org-ol">
<li>A dynamically trained word embedding</li>
<li>A fixed pre-trained word-embedding, induced from a large corpus</li>
<li>A sub-word character model, which is the final state of a RNN model that ingests one character at a time.</li>
</ol>
<p>In such a model, sub-word character-based representations only interact via subsequent recurrent layers. To elaborate, context insensitive representations would normalize words that shouldn&rsquo;t be, but due to the subsequent BiLSTM layer, this would be overridden. This behaviour differs from traditional linear models.<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>
</p>

<p>This paper aims to investigate to what extent having initial subword and word context insensitive representations affect performance. It proposes a hybrid model based on three models- context sensitive initial character and word models and a meta-BiLSTM model which are all trained synchronously.
</p>

<p>On testing this system on 2017 CoNLL data sets, largest gains were found for morphologically rich languages, such as in the Slavic family group. It was also benchmarked on English PTB(?) data, where it performed extremely well compared to the previous best system.
</p>
</div>
</div>
<div id="outline-container-org3647166" class="outline-2">
<h2 id="org3647166">Related Work</h2>
<div class="outline-text-2" id="text-org3647166">
<ol class="org-ol">
<li>An excellent example of an accurate linear model that uses both word and sub-word features.<sup><a id="fnr.7.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup> It uses context sensitive n-gram affix features.</li>
<li>First Modern NN for tagging which initially used only word embeddings<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>, was later extended to include suffix embeddings.<sup><a id="fnr.9" class="footref" href="#fn.9" role="doc-backlink">9</a></sup></li>
<li>TBD TBD</li>
<li>This is the jumping point for current architectures for tagging models with RNNs.<sup><a id="fnr.6.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup></li>
<li>Then <sup><a id="fnr.5.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup> showed that subword/word combination representation leads to state-of-the-art morphosyntactic tagging accuracy.</li>
</ol>
</div>
</div>
<div id="outline-container-org4a2d55b" class="outline-2">
<h2 id="org4a2d55b">Models</h2>
<div class="outline-text-2" id="text-org4a2d55b">
</div>
<div id="outline-container-org527cf21" class="outline-3">
<h3 id="org527cf21">Sentence-based Character Model</h3>
<div class="outline-text-3" id="text-org527cf21">
<p>In this model, a BiLSTM is applied to all characters of a sentence to induce fully context sensitive initial word encodings. It uses sentences split into UTF8 characters as input, the spaces between the tokens are included and each character is mapped to a dynamically learned embedding. A forward LSTM reads the characters from left to right and a backward LSTM reads sentences from right to left.
</p>


<figure id="org12fbe56">
<img src="/assets/nnfl1a.png" alt="nnfl1a.png">

<figcaption><span class="figure-number">Figure 1: </span>Sentence-based Character Model: The representation for the token <i>shingles</i> is the concatenation of the four shaded boxes.</figcaption>
</figure>

<p>For an <i>n</i>-character sentence, for each character embedding \((e_{1}^{char},...,e_{n}^{char})\), a BiLSTM is applied:
\[
f_{c,i}^{0},b_{c,i}^{0} = BiLSTM(r_{0},(e_{1}^{char},...,e_{n}^{char}))_{i}
\]
For multiple layers(<i>l</i>) that feed into each other through the concatenation of previous layer encodings, the last layer has both forward \((f_{c,l}^{l},...,f_{c,n}^{l})\) and backward \((b_{c,l}^{l},...,b_{c,n}^{l})\) output vectors for each character.
</p>

<p>To create word encodings, relevant subsets of these context sensitive character encodings are combined which can then be used in a model that assigns morphosyntactic tags to each word directly or via subsequent layers. To accomplish this, the model concatenates upto four character output vectors: the {<i>forward, backward</i>} output of the {<i>first, last</i>} character in the token <i>T</i> = \((F_{1st}(w), F_{last}(w), B_{1st}(w), B_{last}(w))\) which are represented by the four shaded box in <i>Fig. 1</i>.
</p>

<p>Thus, the proposed model concatenates all four of these and passes it as input to an multilayer perceptron (MLP):
\[
g_{i} = concat(T)
\]
\[
m_{i}^{chars} = MLP(g_{i})
\]
A tag can then be predicted with a <i>linear classifier</i> that takes as input \(m_{i}^{chars}\), applies a <i>softmax</i> function and chooses for each word the tag with highest probability.
</p>
</div>
</div>
<div id="outline-container-org37bbbb0" class="outline-3">
<h3 id="org37bbbb0">Word-based Character Model</h3>
<div class="outline-text-3" id="text-org37bbbb0">
<p>To investigate whether a sentence sensitive character model (<i>Fig.1</i>) is better than a model where the context is restricted to the characters of a word, (<i>Fig.2</i>) which uses the final state of a unidirectional LSTM, combined with the attention mechanism of (ADD REF: cao rei) over all characters.
</p>


<figure id="orgb5c7f08">
<img src="/assets/nnfl1b.png" alt="nnfl1b.png">

<figcaption><span class="figure-number">Figure 2: </span>Word-based Character Model: The token is represented by concatenation of attention over the lightly shaded boxes with the final cell (dark box).</figcaption>
</figure>


<figure id="orgc8f5d5b">
<img src="/assets/nnfl1.png" alt="nnfl1.png">

<figcaption><span class="figure-number">Figure 3: </span>BiLSTM variant of Character-level word representation</figcaption>
</figure>
</div>
</div>
<div id="outline-container-orgee3b27d" class="outline-3">
<h3 id="orgee3b27d">Sentence-based Word Model</h3>
<div class="outline-text-3" id="text-orgee3b27d">
<p>The inputs are the words of the sentence and for each of the words, we use pre-trained word embeddings \((p_{1}^{word},...,p_{n}^{word})\) summed with a dynamically learned word embedding for each word in the corpus \((e_{1}^{word},...,e_{n}^{word})\):
\[
in_{i}^{word} = e_{i}^{word}+p_{i}^{word}
\]
The summed embeddings \(in_{i}\) are passed as input to one or more BiLSTM layers whose output \(f_{w,i}^{l}, b_{w,i}^{l}\) is concatenated and used as the final encoding, which is then passed to an MLP:
\[
o_{i}^{word} = concat(f_{w,i}^{l}, b_{w,i}^{l})
\]
\[
m_{i}^{word} = MLP(o_{i}^{word})
\]
The output of this BiLSTM is essentially the Word-based Character Model before tag prediction, with the exception that the word-based character encodings are excluded.
</p>


<figure id="orga803d0a">
<img src="/assets/nnfl2a.png" alt="nnfl2a.png">

<figcaption><span class="figure-number">Figure 4: </span>Tagging Architecture of Word-based Character Model and Sentence-based Word Model</figcaption>
</figure>
</div>
</div>
<div id="outline-container-orgb1d5eab" class="outline-3">
<h3 id="orgb1d5eab">Meta-BiLSTM: Model Combination</h3>
<div class="outline-text-3" id="text-orgb1d5eab">
<p>If each of the character or word-based encodings are trained with their own loss and are combined using an additional meta-BiLSTM model, optimal performance is obtained. The meta-biLSTM model concatenates the output of context sensitive character and word-based encoding for each word and puts this through another BiLSTM to create an <i>additional</i> combined context sensitive encoding. This is followed by a final MLP whose output is passed to a linear layer for tag prediction.
\[
cw_{i} = concat(m_{i}^{char}, m_{i}^{word})
\]
\[
f_{m,i}^{l}, b_{m,i}^{l} = BiLSTM(r_{0},(cw_{0},...,cw_{n}))_{i}
\]
\[
m_{i}^{comb} = MLP(concat(f_{m,i}^{l}, b_{m,i}^{l}))
\]
</p>


<figure id="orge6307dd">
<img src="/assets/nnfl2b.png" alt="nnfl2b.png">

<figcaption><span class="figure-number">Figure 5: </span>Tagging Architecture of Meta-BiLSTM. Data flows along the arrows and the optimizers minimize the loss of the classifiers independently and backpropogate along the bold arrows.</figcaption>
</figure>
</div>
</div>
<div id="outline-container-org7ea180e" class="outline-3">
<h3 id="org7ea180e">Training Schema</h3>
<div class="outline-text-3" id="text-org7ea180e">
<p>Loss of each model is minimized independently by separate optimizers with their own hyperparameters which makes this a multi-task learning model and hence a schedule must be defined in which individual models are updated. In the proposed algorithm, during each epoch, each of the models are updated in sequence using the entire training data.
</p>


<figure id="org4e7ce61">
<img src="/assets/nnflAlg.png" alt="nnflAlg.png">

</figure>

<p>In terms of model selection, after each epoch, the algorithm evaluates the tagging accuracy of the development set and keeps the parameters of the best model. Accuracy is measured using the meta-BiLSTM tagging layer, which requires a forward pass through all three models. Only the meta-BiLSTM layer is used for model selection and test-time prediction.
</p>

<p>The training is synchronous as the meta-BiLSTM model is trained in tandem with the two encoding models, and not after they have converged. When the meta-BiLSTM was allowed to back-propagate through the whole network, performance degraded regardless of the number of loss functions used. Each language could in theory used separate hyperparameters but identical settings for each language works well for large corpora.
</p>
</div>
</div>
</div>
<div id="outline-container-org5ee477b" class="outline-2">
<h2 id="org5ee477b">Experiments and Results</h2>
<div class="outline-text-2" id="text-org5ee477b">
</div>
<div id="outline-container-orgf456eea" class="outline-3">
<h3 id="orgf456eea">Experimental Setup</h3>
<div class="outline-text-3" id="text-orgf456eea">
<p>The word embeddings are initialized with zero values and the pre-trained embeddings are not updated during training. The dropout<sup><a id="fnr.10" class="footref" href="#fn.10" role="doc-backlink">10</a></sup> used on the embeddings is achieved by a single dropout mask and dropout is used on the input and the states of the LSTM.
</p>

<table id="org8c69f22">


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-right">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Model</th>
<th scope="col" class="org-left">Parameter</th>
<th scope="col" class="org-right">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">C,W</td>
<td class="org-left">BiLSTM Layers</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-left">M</td>
<td class="org-left">BiLSTM Layers</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">CWM</td>
<td class="org-left">BiLSTM size</td>
<td class="org-right">400</td>
</tr>

<tr>
<td class="org-left">CWM</td>
<td class="org-left">Dropout LSTM</td>
<td class="org-right">0.33</td>
</tr>

<tr>
<td class="org-left">CWM</td>
<td class="org-left">Dropout MLP</td>
<td class="org-right">0.33</td>
</tr>

<tr>
<td class="org-left">W</td>
<td class="org-left">Dropout Embeddings</td>
<td class="org-right">0.33</td>
</tr>

<tr>
<td class="org-left">C</td>
<td class="org-left">Dropout Embedding</td>
<td class="org-right">0.5</td>
</tr>

<tr>
<td class="org-left">CWM</td>
<td class="org-left">Nonlinear Activation Fn (MLP)</td>
<td class="org-right">ELU</td>
</tr>
</tbody>
</table>

<p>TODO Add two remaining tables
</p>
</div>
</div>
<div id="outline-container-org4966291" class="outline-3">
<h3 id="org4966291">Data Sets</h3>
</div>
<div id="outline-container-orgd643028" class="outline-3">
<h3 id="orgd643028">POS Tagging Results</h3>
</div>
<div id="outline-container-orgc19957c" class="outline-3">
<h3 id="orgc19957c">POS Tagging on WSJ</h3>
</div>
<div id="outline-container-org57353d3" class="outline-3">
<h3 id="org57353d3">Morphological Tagging Results</h3>
</div>
</div>
<div id="outline-container-orgaed8b2b" class="outline-2">
<h2 id="orgaed8b2b">Ablation Study (Takeaways)</h2>
<div class="outline-text-2" id="text-orgaed8b2b">
<ul class="org-ul">
<li><b>Impact of the training schema</b>: Separate optimization better than Joint optimization</li>
<li><b>Impact of the Sentence-based Character Model</b>: Higher accuracy than word-based character context</li>
<li><b>Impact of the Meta-BiLSTM Model Combination</b>: Combined model has significantly higher accuracy than individual models</li>
<li><b>Concatenation Strategies for the Context-Sensitive Character Encodings</b>: Model bases a token encoding on both forward and backward character representations of both first and last character in token. (<i>Fig. 1</i>) &#x2026;.</li>
<li><b>Sensitivity to Hyperparameter Search</b>: With larger network sizes, capacity of the network increases, but it becomes prone to overfitting. Future variants of this model might benefit from higer regularization.</li>
<li><b>Discussion</b>: TODO Proposed modifications</li>
</ul>
</div>
</div>
<div id="outline-container-orga13a714" class="outline-2">
<h2 id="orga13a714">Conclusions</h2>
</div>
<div id="outline-container-org1f7a6e6" class="outline-2">
<h2 id="org1f7a6e6">Readings and Resources</h2>
<div class="outline-text-2" id="text-org1f7a6e6">
<ol class="org-ol">
<li>Pytorch: <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">Beginner Guide</a>, <a href="https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG">Detailed Guides</a>, <a href="https://www.cs.toronto.edu//~lczhang/360/">Notebook form</a></li>
<li>Math: <a href="https://explained.ai/matrix-calculus/index.html">Matrix Calculus</a>, <a href="https://mml-book.com/">Book</a></li>
<li>Basics:
<ul class="org-ul">
<li><a href="https://www.kaggle.com/learn/python">Python</a></li>
<li><a href="https://realpython.com/jupyter-notebook-introduction/#getting-up-and-running-with-jupyter-notebook">Jupyter</a></li>
<li><a href="http://cs231n.github.io/python-numpy-tutorial/#numpy">Numpy</a>, <a href="https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-2-Numpy.ipynb">Numpy 2</a></li>
<li><a href="https://mlcourse.ai/articles/topic1-exploratory-data-analysis-with-pandas/">Pandas</a>, <a href="https://www.kaggle.com/learn/pandas">Pandas 2</a></li>
<li><a href="https://mlcourse.ai/articles/topic2-visual-data-analysis-in-python/">Matplotlib</a>, <a href="https://matplotlib.org/matplotblog/posts/an-inquiry-into-matplotlib-figures/">Matplotlib 2</a></li>
<li><a href="https://mlcourse.ai/articles/topic2-part2-seaborn-plotly/">Seaborn</a></li>
<li><a href="http://scipy-lectures.org/">Overview</a></li>
</ul></li>
<li>Interactive Tutorials on <a href="https://www.deeplearning.ai/ai-notes/initialization/">Weight Initialization</a>, <a href="https://www.deeplearning.ai/ai-notes/optimization/">Different Optimizers</a></li>
<li>Rougier&rsquo;s Bits
<ul class="org-ul">
<li><a href="https://github.com/rougier/matplotlib-tutorial">Matplotlib Tutorial</a>, <a href="https://github.com/matplotlib/cheatsheets">Matplotlib Cheatsheets</a></li>
<li><a href="https://github.com/rougier/numpy-tutorial">Numpy Tutorial</a>, <a href="https://www.labri.fr/perso/nrougier/from-python-to-numpy/">From Python to Numpy</a>, <a href="https://github.com/rougier/numpy-100">100 Numpy Exercises</a></li>
<li><a href="https://www.labri.fr/perso/nrougier/python-opengl/">Python &amp; OpenGL for Scientific Visualization</a>, <a href="https://github.com/rougier/scientific-visualization-book">Scientific Visualization</a></li>
</ul></li>
<li>NLP: <a href="https://github.com/microsoft/nlp-recipes">Best Practices</a>, <a href="https://nlpoverview.com/">DL Techniques for NLP</a></li>
<li>BiLSTM: <a href="https://arxiv.org/pdf/1807.00818v1.pdf">Improving POS tagging</a></li>
<li><a href="https://github.com/google/meta_tagger">Implementation</a> of the paper</li>
</ol>
</div>
</div>
<div id="outline-container-org164f30d" class="outline-2">
<h2 id="org164f30d">Specific to Paper</h2>
<div class="outline-text-2" id="text-org164f30d">
<ol class="org-ol">
<li><a href="https://universaldependencies.org/guidelines.html">Universal Dependencies</a></li>
<li><a href="https://lena-voita.github.io/nlp_course.html">Great Tutorial for NLP</a></li>
<li><a href="https://github.com/Sdernal/Morphology/blob/master/README.md">Morphology</a></li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe">Everything about Embeddings</a> Embedding converts symbolic representations into meaningful
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Morphological tagging is the task of assigning labels to a sequence of tokens that describe them morphologically. As compared to Part-of-speech tagging, morphological tagging also considers morphological features, such as case, gender or the tense of verbs.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">They are the different categorical characteristic of the given data. For example, it could be <i>grammatical</i> classes or some <i>physical</i> features. It is context and result dependent. Then for each token, a weight is assigned to it with respect to each feature.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">A word embedding is a learned representation for text where words that have the same meaning have a similar representation.
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="https://www.aclweb.org/anthology/K17-3002.pdf">Graph based Neural Dependency Parser</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="https://arxiv.org/pdf/1604.05529.pdf">POS Tagging with BiLSTM</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=40AFFD632AC50016FE3B435B5C3FD50F?doi=10.1.1.4.7273&amp;rep=rep1&amp;type=pdf">*Fast POS Tagging: SVM Approach</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="http://machinelearning.org/archive/icml2008/papers/391.pdf">Unified architecture for NLP</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara"><a href="https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">NLP(almost) from Scratch</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10" role="doc-backlink">10</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">Dropping out units (hidden and visible) in a neural network, helps prevent the network from overfitting.
</p></div></div>


</div>
</div></div>
<footer id="postamble" class="status">
&copy; 2026 . Made with <a href="https://www.gnu.org/software/emacs/">Emacs</a> 31.0.50 (<a href="https://orgmode.org">Org</a> mode 9.7.11).
</footer>
</body>
</html>
