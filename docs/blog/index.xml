<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on bacchanalian madness</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blog on bacchanalian madness</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Mar 2023 14:35:45 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>I R SSI</title>
      <link>http://localhost:1313/blog/irc/</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/irc/</guid>
      <description>&lt;figure&gt;&lt;a href=&#34;https://xkcd.com/1782&#34;&gt;&lt;img src=&#34;https://imgs.xkcd.com/comics/team_chat.png&#34;&gt;&lt;/a&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Umm. Guilty, though I&amp;rsquo;ve only been using it for two days, but I&amp;rsquo;ve been having a blast! So let&amp;rsquo;s get into it.&lt;/p&gt;&#xA;&lt;div class=&#34;note&#34;&gt;&#xA;&lt;p&gt;This is intended for users who are familiar with chat systems like discord and wish to have a similar visual experience while working with IRC. I don&amp;rsquo;t discuss all the aspects such as chat etiquette and other security related stuff for which I&amp;rsquo;d direct the readers to resources at the bottom.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Another nix post in the wall</title>
      <link>http://localhost:1313/blog/nix-intro/</link>
      <pubDate>Thu, 02 Jun 2022 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/nix-intro/</guid>
      <description>&lt;h2 id=&#34;starting&#34;&gt;Starting&lt;/h2&gt;&#xA;&lt;p&gt;Are you using Nixos? This is not for you.&#xA;Do you want multi-user installation? This is not for you.&#xA;This is only useful if you want to use both flakes and home-manager.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Single-user installation (no sudo needed, easier to remove, good for testing purposes)&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  sh &amp;lt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;curl -L https://nixos.org/nix/install&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; --no-daemon&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Source the new profile or login.&#xA;&lt;code&gt;. ~/.nix-profile/etc/profile.d/nix.sh&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Since most of the nix &amp;ldquo;guides&amp;rdquo; are outdated, check what your current version supports &lt;code&gt;nix --help&lt;/code&gt;, &lt;del&gt;at the time of writing this, there&amp;rsquo;s no need to enable experimental features for flakes :)&lt;/del&gt; spoke too soon.&lt;/li&gt;&#xA;&lt;li&gt;Upgrading nix:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  nix-channel --update&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; nix-env -iA nixpkgs.nix nixpkgs.cacert&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check if &lt;code&gt;nixpkgs-unstable&lt;/code&gt; channel (package sources basically) is installed or not by &lt;code&gt;nix-channel --list&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;You can look at user-installed packages by &lt;code&gt;nix-env --query&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;home-manager&#34;&gt;Home manager&lt;/h2&gt;&#xA;&lt;p&gt;Allows declarative configuration of user-specific (non global) packages and dotfiles.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Morphosyntactic Tagging with BiLSTM Model</title>
      <link>http://localhost:1313/blog/nnfl-paper/</link>
      <pubDate>Sun, 21 Mar 2021 03:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/nnfl-paper/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;I had shingles, which is a painful disease.&amp;rdquo;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;figure&gt;&lt;a href=&#34;http://localhost:1313/ox-hugo/machine_learning.png&#34;&gt;&lt;img src=&#34;http://localhost:1313/ox-hugo/machine_learning.png&#34;&gt;&lt;/a&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;This post contains a complete overview of the titled paper and provides a basic outline of related concepts. This paper aims to investigate to what extent having initial sub-word and word context insensitive representations affect performance.&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;RNN leads to advances in speech tagging accuracy &lt;a href=&#34;https://www.aclweb.org/anthology/K18-2001.pdf&#34;&gt;Zeman et al&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Common thing among models, &lt;em&gt;rich initial word encodings&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Encodings are composed of recurrent character-based representation with learned and pre-trained word embeddings&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Problem with the encodings, context restriced to a single word hence only via subsequent recurrent layers the word information is processed.&lt;/li&gt;&#xA;&lt;li&gt;The paper deals with models that use RNN with sentence-level context.&lt;/li&gt;&#xA;&lt;li&gt;This provides results via synchronized training with a meta-model that learns to combine their states.&lt;/li&gt;&#xA;&lt;li&gt;Results are provided on part-of-speech and morphological tagging&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; with great performance on a number of languages.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;terms&#34;&gt;Terms&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Morphosyntactic = Morphology + Syntax and Morphology is study of words, how they are formed, and their relationship to other words in the same language.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577&#34;&gt;RNN&lt;/a&gt;: &lt;a href=&#34;https://arxiv.org/pdf/1211.5063.pdf&#34;&gt;On difficulty of training RNNs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;LSTM&lt;/a&gt;: Long Short-Term Memory is a type of RNN that addresses the vanishing gradient problem through additional cells, input and output gates.&lt;/li&gt;&#xA;&lt;li&gt;BiLSTM: It is a sequence processing model that consists of two LSTMs. They effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;basics-of-nlp&#34;&gt;&lt;a href=&#34;https://www.kdnuggets.com/2018/06/getting-started-natural-language-processing.html&#34;&gt;Basics of NLP&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;key-terms&#34;&gt;Key Terms&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Natural Language Processing concerns itself with interaction of technology with human languages.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: An early step in the NLP process which splits longer strings of text into smaller pieces, or &lt;em&gt;tokens&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Normalization&lt;/strong&gt;: A series of tasks meant to put all text on a level playing field i.e. converting it to lowercase, removing punctuation, expanding contractions, converting numbers to their word equivalents, stripping white space, removing stop words and so on.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Stemming&lt;/strong&gt;: Process of eliminating affixes (suffixes, prefixes, infixes, circumfixes) from a word to obtain its stem. For example, &lt;em&gt;running&lt;/em&gt; becomes &lt;em&gt;run&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Lemmatization&lt;/strong&gt;: It&amp;rsquo;s related to stemming but is able to capture canonical forms based on the word&amp;rsquo;s lemma (root form). For example, &lt;em&gt;better&lt;/em&gt; would turn into &lt;em&gt;good&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Corpus&lt;/strong&gt;: The latin word for &lt;em&gt;body&lt;/em&gt; refers to a collection of texts which may be formed of a single language of texts, or multiple. They are generally used for statistical linguistic analysis and hypothesis testing.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Stop words&lt;/strong&gt;: Filter words which contribute little to the overall meaning of text since they are the very common words of the language. For example: &lt;em&gt;the&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt; etc.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Parts-of-speech (POS) Tagging&lt;/strong&gt;: It consists of assigning a category tag to the tokenized parts of a sentence such as nouns, verbs, adjectives etc. The category of words is distinguished since they share similar grammatical properties.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Statistical Language Modeling&lt;/strong&gt;: It&amp;rsquo;s the process of building a model which takes &lt;em&gt;words&lt;/em&gt; as input and assign probabilities to the various sequences that can be formed using them.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Bag of words&lt;/strong&gt;: It&amp;rsquo;s a representation model used to simplify the contents of a selection of text by just reducing the words to their frequency.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;n-gram&lt;/strong&gt;: It focuses on preserving contagious sequences of N items from the text selection.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;a-framework-for-nlp&#34;&gt;A framework for NLP&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Collection or Assembly&lt;/strong&gt;: Building the corpus&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Perform operations on the collected corpus which consists of tokenization, normalization, substitution (noise removal).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data Exploration &amp;amp; Visualization&lt;/strong&gt;: Includes visualizing word counts and distributions, generating wordclouds, performing distance measures.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Model Building&lt;/strong&gt;: Choosing the language models (FSM, MM), classifiers and sequence models (RNNs, LSTMs).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Model Evaluation&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;data-representation&#34;&gt;Data Representation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;We need to encode text in a way that can be controlled by us using a statistical classifier.&lt;/li&gt;&#xA;&lt;li&gt;We go from a set of categorical features in text: words, letters, POS tags, word arrangement, order etc to a series of &lt;em&gt;vectors&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;One-hot Encoding&lt;/strong&gt; (Sparse Vectors) :&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Each word, or token corresponds to a vector element.&lt;/li&gt;&#xA;&lt;li&gt;Result of one-hot encoding is a sparse matrix, that is, for a corpus containing a lot of tokens, representing a small subset of them would lead to a lot of zero vectors which would consume a large amount of memory.&lt;/li&gt;&#xA;&lt;li&gt;One more drawback is that while it contains the information regarding the presence of a certain word, it lacks positional information so making sense of the tokens is not an option. For example, &lt;em&gt;Kate hates Alex&lt;/em&gt; is the same as &lt;em&gt;Alex hates Kate&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Variants of one-hot encoding are &lt;em&gt;bag-of-words&lt;/em&gt;, &lt;em&gt;n-gram&lt;/em&gt; and &lt;em&gt;TF-IDF&lt;/em&gt; representations.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Dense Embedding Vectors&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The information of the semantic relationship between tokens can be conveyed using manual or learned POS tagging that determines which tokens in a text perform what type of function. (noun, verb, adverb, etc)&lt;/li&gt;&#xA;&lt;li&gt;This is useful for &lt;em&gt;named entity recognition&lt;/em&gt;, i.e. our search is restricted to just the nouns.&lt;/li&gt;&#xA;&lt;li&gt;But if one represents &lt;em&gt;features&lt;/em&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; as dense vectors i.e. with core features embedded into an embedding space of size &lt;em&gt;d&lt;/em&gt; dimensions, we can compress the number of dimensions used to represent a large corpus into a manageable amount.&lt;/li&gt;&#xA;&lt;li&gt;Here, each feature no longer has its own dimension but is rather mapped to a vector.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;word-representation&#34;&gt;&lt;a href=&#34;http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf&#34;&gt;Word Representation&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;subword-models-20is-pairs-20into-20a-20new-20byte-dot-and-text-bpe-20is-20a-20word-20segmentation--unicode--20characters-20in-20data-dot&#34;&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79#:~:text=Subword%2Dmodels%3A%20Byte%20Pair%20Encodings%20and%20friends,-2.1%20Byte%20pair&amp;amp;text=Byte%20pair%20encoding%20(BPE)%20is,pairs%20into%20a%20new%20byte.&amp;amp;text=BPE%20is%20a%20word%20segmentation,(Unicode)%20characters%20in%20data.&#34;&gt;Subword models&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Purely Character-level models&lt;/strong&gt;: In character-level modes, word embeddings&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; can be composed of character embeddings which have several advantages. &lt;em&gt;Character-level&lt;/em&gt; models are needed because:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Languages like Chinese don&amp;rsquo;t have &lt;em&gt;word segmentations&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;For languages that do have, they segment in different ways.&lt;/li&gt;&#xA;&lt;li&gt;To handle large, open, informal vocabulary.&lt;/li&gt;&#xA;&lt;li&gt;Character level model can generate embeddings for &lt;em&gt;unknown&lt;/em&gt; words.&lt;/li&gt;&#xA;&lt;li&gt;Similar spellings share similar embeddings&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Subword-models&lt;/strong&gt;: TBD???&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;morphology&#34;&gt;Morphology&lt;/h2&gt;&#xA;&lt;p&gt;It is a section of grammar whose main objects are &lt;strong&gt;words&lt;/strong&gt; of languages, their &lt;em&gt;significant parts&lt;/em&gt; and &lt;em&gt;morphological signs&lt;/em&gt;. Morphology studies:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creating a blog using ox-hugo</title>
      <link>http://localhost:1313/blog/blog-creation/</link>
      <pubDate>Sun, 21 Mar 2021 02:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/blog-creation/</guid>
      <description>&lt;p&gt;I was going to make a post explaining how I made this blog but it was rendered pretty useless by &lt;a href=&#34;https://dev.to/usamasubhani/setup-a-blog-with-hugo-and-github-pages-562n&#34;&gt;this.&lt;/a&gt; So yeah, I might archive this later.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install hugo from your package manager.&lt;/li&gt;&#xA;&lt;li&gt;Create a new site:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   hugo new site blog&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add a theme:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; blog&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   git init&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   git submodule add &amp;lt;theme_url&amp;gt; themes/&amp;lt;name&amp;gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install ox-hugo in emacs&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-emacs-lisp&#34; data-lang=&#34;emacs-lisp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;c1&#34;&gt;;; goes in packages.el&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;package!&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;ox-hugo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;c1&#34;&gt;;; goes in config.el&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;use-package&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;ox-hugo&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;nb&#34;&gt;:after&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;ox&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;TODO Explain the process of content and properties, tags etc.&lt;/li&gt;&#xA;&lt;li&gt;Export&lt;/li&gt;&#xA;&lt;li&gt;Config.toml (theme, title, url, publishdir, etc)&lt;/li&gt;&#xA;&lt;li&gt;Run server, check localhost.&lt;/li&gt;&#xA;&lt;li&gt;Push&lt;/li&gt;&#xA;&lt;li&gt;Go to GitHub repository Settings &amp;gt; GitHub pages. Select /docs in Source.&lt;/li&gt;&#xA;&lt;li&gt;Voila!&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
